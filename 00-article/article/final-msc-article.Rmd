---
title: "Department of Statistics 2019: Mapping Inequalities Online Using Data"
author: "Candidate Number: 10140"
date: ""
bibliography: tex/ref.bib       # Do not edit: Keep this naming convention and location
# When you want to include more references you will need to update your ref.bib file
csl: tex/harvard-cite-them-right.csl        # You can update this to suit the style of your referencing
toc: yes        # Table of contents
lot: yes        # List of tables
lof: yes        # List of figures
numbersections: yes             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.5                # Set distance between lines.
output:
  pdf_document:
    keep_tex: TRUE
    template: tex/tex-default.tex        # Default tex file where you can do some niggly work
    fig_width: 3.5        # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
documentclass: "article"
BottomCFooter: "\\footnotesize \\thepage\\"       # Add a '#' before this line to remove footer.
header-includes:        # Here you can include with extra packages
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
# Note: Include = false implies code is executed but not printed in pdf
```
\renewcommand{\vec}[1]{\mathbf{#1}}
\newgeometry{left=3.5cm, right=2cm, top=20mm,bottom=4cm,top=2.5cm}

<!-- SUMMARY: 300 WORDS -->

<!-- INTRODUCTION: Aims of the study and a brief outline of the main sections of the dissertation -->

# Introduction \label{Intro}

Aims: 

1. Literature review on natural language processing for GH/SE data and information retrieval

2. Exploratory and descriptive analysis of small dataset

3. Explore small dataset and employ ML methods to get predict quality of question/commit

4. 



<!-- MAIN SECTIONS: Review of the literature, description of methods used and the results of the study -->


\newpage

# Brief Literature Review \label{Lit}

## A Summary of the Research



@Brinton2014:

Studies behaviour in courses offered by MOOC provider during summer of 2013. State that social learning is a key element of scalable education on MOOC and transpires through online discussion forums, they want to understand forum activities. 
Two NB features: First is that there is a high decline rate - discussion begin with a lot of energy and then depletes over duration of course. Second is that discussion are *high-volume* and *noisy* (information overload), i.e. 30% or more of courses produced new discussion threads at rates making reading by students and teachers infeasible. Also, much discussion is off-topic.

@Brinton2014 explore reasons for decline of activity on MOOC forums and find effective ways of classifying threads to rank their relevance. They use linear regression models to analyze forum activity and observe that, for example, teachers getting involved is correlated with increase in discussion volume, but does not affect depletion.

They propose a unified generative model for discussin threads, allowing them to choose efficient thread classifiers as well as design an effective algorithm to rank relevance.

They want to address information overload (which actually falls into field of information retrieval) by forming a simple model and thus improving the online learning experience. Contrary to IR, they want to highlight the unique characteristics of MOOC dynamics when compared to Yahoo!, Q&A and StackExchange or social media sites.

Their methodology for addressing information overload:

* First few days see a lot of small-talk in forums which need to be classified and filtered out.

* Small talk then fades away, thus need to rank relevance of new threads over time.

Therefore need effective classifier for discussion-thread and algorithm for ranking relevance.

"We propose a unified generative model for thread discussions that simultaneously guides (i) the choice of classifiers, (ii) the design of algorithms for extracting important topics in each forum, and (iii) the design of a relevance ranking algorithm based on the resulting topic extraction algorithm."

"We crawled the forum content from Coursera’s server at a rate of 1 to 3 pages per second using Python and the Selenium library. Finally, we used Beautifulsoup to parse the html into text files. In total, our data set consists of approximately 830K posts (Section 3 presents more details)."

"Through our analysis, we presented a large-scale statistical analysis of a MOOC platform (Coursera), in which we made a number of interesting observations; for instance, that active participation of the teaching staff is associated with an increase in discussion volume but does not reduce the participation decline rate. We also presented two proof-of-concept algorithms for keyword extraction and relevance-ranking of discussion threads, each of which was demonstrated to be effective, through human evaluation when necessary."



@Stadtfeld2019:

"The findings underline the importance of understanding social network dynamics in educational settings. They call for the creation of university environments promoting the development of positive relationships in pursuit of academic success."



@Blei:

"describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics."


@Joulin:

"This paper explores a simple and efficient baseline for text classification."



@Conati:


@Kalliamvakou2014:

"If a researcher seeks to see trends of programming language use, type of tools built, number and size of contributions and so on, the publicly available data can give solid information about the descriptive characteristics of the GitHub environment."

"We recommend that researchers interested in performing stud- ies using GitHub data first assess its fit and then target the data that can really provide information towards answering their research questions."

"Perhaps the biggest threat to validity to any study that uses GitHub data indiscriminately is the bias towards per- sonal use. While many repositories are being actively devel- oped on GitHub, most of them are simply personal, inactive repositories. Therefore, one of the most important ques- tions to consider when using GitHub data is what type of repository one’s study needs and to then sample suitable repositories accordingly."

"While we believe there to be a need for research on the identification and automatic classification of GitHub projects according to their purpose, we suggest a rule of thumb. In our own experience, the best way to identify active software development projects is to consider projects that, during a recent time period, had a good balance of number of com- mits and pull requests, and have a number of committers and authors larger than 2."

"Based on our work, we believe a simple way to determine whether a repository actively works with another might be to identify if commits have flown from one to the other in both directions, but this strategy requires further validation."

@Bogdan2013:

"Our study shows that active GitHub committers ask fewer questions and provide more answers than others. Moreover, we observe that active StackOverflow askers distribute their work in a less uniform way than developers that do not ask questions. Finally, we show that despite the interruptions incurred, the StackOverflow activity rate correlates with the code changing activity in GitHub."


\newpage

# Datasets I Have Found \label{Data} 




\newpage

# Empirical Methodology \label{Meth} 

## Empirical Model \label{Model} 

This is an equation:

\begin{align} \label{eq:EP1}
A_{it}=f(T_i^{(t)},S_i^{(t)},P_i^{(t)},B_i^{(t)},I_i),
\end{align}



\footnotesize
\renewcommand{\thetable}{\arabic{table}}

\begin{longtable} {@{} l r r c r @{}} \caption{\textbf{Learner achievement (\%)}}
\label{tab:Dep}\\ \hline \hline
Subject & Mean & Q1 & Median & Q3 \\
\hline
Maths&      27& 15&   23&  35\\ \hline \hline
\end{longtable}
\begin{center} Source: Own calculations in Stata using 2004 Grade 6 Intermediate Phase Systemic Evaluation.\end{center}
\normalsize



\setcounter{figure}{0}
\renewcommand{\thefigure}{\arabic{figure}}

\begin{figure}
\caption{\textbf{\footnotesize Cumulative graph for subject scores}}
\label{fig:1}
```{r echo=FALSE, out.width='40%', fig.align='center'}
knitr::include_graphics("./results/graphs/figure.png")
```
\centering
{\footnotesize Source: Own calculations in Stata using 2004 Grade 6 Intermediate Phase Systemic Evaluation.}
\end{figure}
\normalsize

Again, you can reference the figure in-text: figure \ref{fig:1} is a figure and it displays etc. etc. etc.

This is an organic figure generated with an R chunk that is executed when the document is knitted (the image is also saved in the folder `final-article-template_files`):

```{r echo=FALSE, out.width='40%', fig.align='center'}
x <- c(1, 2, 6, 8)
plot(x)
```

Regarding R chunks: If you want a chunk's code to be printed, include set `echo = TRUE`. `message = FALSE` stops R printing package loading details and setting `warning = FALSE` should suppress most warnings.



<!-- CONCLUSION: Conclusions that can be drawn from your work, possible extensions and further work -->

\newpage

# Recommendations for Further Research \label{Recom}


\newpage

# Concluding Remarks \label{Concl}







<!-- BIBLIOGRAPHY -->

\newpage

# References


<!-- APPENDICES: Not encouraged

\newpage

# Appendix {.unnumbered}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

A table in the Appendix:

\begin{longtable}[htbp] {@{} l r r r r r @{}} \caption{
\textbf{Summary statistics of model variables}} \label{tab:Desc}  \\ \hline\hline
                \textbf{Variable } & \textbf{Mean} & \textbf{S.D.} & \textbf{Min} & \textbf{Max} & \textbf{N}\\
\hline
\bf{\emph{Educational Outcomes}} & & & & \\
   Maths Score (\%)  &   27.58797 &   17.82077 &          0 &        100 &     102048 \\
   Science Score (\%) &   41.04531 &   18.54542 &          0 &   95.83334 &     102045 \\
   English Score (\%) &   38.49276 &    24.3848 &          0 &        100 &     102048 \\
\hline \hline
\multicolumn{5}{@{}l}{Source: Own calculations in Stata using 2004 Grade 6 Intermediate Phase Systemic Evaluation.}
\end{longtable}

 -->
