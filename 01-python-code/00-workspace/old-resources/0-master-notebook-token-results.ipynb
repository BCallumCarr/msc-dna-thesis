{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOALS\n",
    "\n",
    "* Decide on viewcount threshold to eliminate views?\n",
    "* Get feature columns working\n",
    "* Build an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"Welcome to my PySpark analysis of some StackExchange Data\"\\n'\n"
     ]
    }
   ],
   "source": [
    "## testing printing output from console\n",
    "import subprocess\n",
    "cmd = [ 'echo', '\"Welcome to my PySpark analysis of some StackExchange Data\"' ]\n",
    "output = subprocess.Popen( cmd, stdout=subprocess.PIPE ).communicate()[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PySpark and Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spark UI is available at: http://192.168.0.26:4040/ and the defaultParallelism is 4\n"
     ]
    }
   ],
   "source": [
    "%run -i '1-load-pyspark.py'\n",
    "\n",
    "## array of dataset names to loop through in analysis\n",
    "data_array = [\n",
    "    \"english\",\n",
    "    \"rus_stackoverflow\",\n",
    "    \"superuser\",\n",
    "    \"stackoverflow\",\n",
    "    \"math\"\n",
    "]\n",
    "\n",
    "data_array = [\n",
    "    \"economics\",\n",
    "    \"buddhism\",\n",
    "    \"fitness\",\n",
    "    \"health\",\n",
    "    \"interpersonal\"\n",
    "]\n",
    "\n",
    "datasets = {}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:34:21.078112\n",
      "CPU times: user 93 µs, sys: 42 µs, total: 135 µs\n",
      "Wall time: 117 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "#%run -i '2-load-datasets.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spark_df(df, n=5):\n",
    "    '''\n",
    "    function to better print spark df entries\n",
    "    '''\n",
    "    display(pd.DataFrame(df.head(n), columns=df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:34:21.093467\n",
      "CPU times: user 107 µs, sys: 74 µs, total: 181 µs\n",
      "Wall time: 121 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "#%run -i '3-clean-datasets.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Target and Perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:34:21.102503\n",
      "CPU times: user 344 µs, sys: 82 µs, total: 426 µs\n",
      "Wall time: 372 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "#%run -i '4-define-target-eda.py'\n",
    "\n",
    "#NB TO DO: Find threshold to delete low views to make sure users that can vote have seen the question\n",
    "\n",
    "'''\n",
    "vc_thresh_data = {}\n",
    "\n",
    "## finding means of viewcounts across fora\n",
    "\n",
    "for i in data_array:\n",
    "    vc_thresh_data[i] = datasets[i].select(\"viewcount\").rdd.flatMap(lambda x: x).mean()\n",
    "\n",
    "vc_thresh_data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Clean Data with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:34:21.113686\n",
      "CPU times: user 184 µs, sys: 70 µs, total: 254 µs\n",
      "Wall time: 198 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "#%run -i '5-export-data.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.11 ms, sys: 2.85 ms, total: 6.96 ms\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## load clean data\n",
    "for i in data_array:\n",
    "    datasets[i] = (\n",
    "        spark\n",
    "        .read\n",
    "        .load(f'clean-data/{i}.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split MANUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:34:24.726720\n",
      "economics: 0.19688614985403827, from 1214 and 6166\n",
      "buddhism: 0.20017182130584193, from 932 and 4656\n",
      "fitness: 0.20499851234751562, from 1378 and 6722\n",
      "health: 0.18846909805634418, from 863 and 4579\n",
      "interpersonal: 0.2189300411522634, from 532 and 2430\n",
      "rus_stackoverflow: 0.21258418026653533, from 44888 and 211154\n",
      "CPU times: user 34 ms, sys: 10.4 ms, total: 44.4 ms\n",
      "Wall time: 7.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '6-train-test-split.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninteresting to see how skewed rus_stackoverflow posts are to more posts in recent years\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "interesting to see how skewed rus_stackoverflow posts are to more posts in recent years\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Results Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silly Mean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, lit, struct\n",
    "\n",
    "## create mean dictionaries\n",
    "y_ravi_means = {}\n",
    "\n",
    "## calculate the mean of each forum, using ONLY train set\n",
    "for i in data_array:\n",
    "    y_ravi_means[i] = train[i].select('y_ravi').rdd.flatMap(lambda x: x).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m mean\u001b[0m model is 0.037792\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m mean\u001b[0m model is 0.015749\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m mean\u001b[0m model is 0.020767\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m mean\u001b[0m model is 0.040167\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m mean\u001b[0m model is 0.008395\n",
      "The root-mean-square error of \u001b[94mrus_stackoverflow's\u001b[0m\u001b[92m mean\u001b[0m model is 0.026379\n"
     ]
    }
   ],
   "source": [
    "## import rmse evaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "## create baselines dictionary\n",
    "base = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "\n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## train silly mean model by assigning full mean to each row of test\n",
    "    test[i] = test[i].withColumn('mean_pred', lit(y_ravi_means[i]))\n",
    "\n",
    "    ## evaluate silly mean model, ONLY on test set\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"y_ravi\", predictionCol=\"mean_pred\")\n",
    "    base[i] = round( evaluator.evaluate(test[i]), 6)\n",
    "\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m mean\\033[0m model is {base[i]}\")\n",
    "\n",
    "    ## record time taken\n",
    "    timt = round( time.time() - t0, 2 )\n",
    "    \n",
    "    ## store as dictionary inside RESULTS dictionary, initiating dataset name entries first\n",
    "    RESULTS[i.title()]['0silly_mean.rmse'] = base[i]\n",
    "    RESULTS[i.title()]['0silly_mean.timet'] = timt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_save_results(results, filename='final-results.csv'):\n",
    "    '''\n",
    "    function to print and export modelling results\n",
    "    '''\n",
    "    display(pd.DataFrame.from_dict(results).T)\n",
    "    print(pd.DataFrame.from_dict(results).T.to_latex())\n",
    "    pd.DataFrame.from_dict(results).T.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0silly_mean.rmse</th>\n",
       "      <th>0silly_mean.timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>0.037792</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>0.040167</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rus_Stackoverflow</th>\n",
       "      <td>0.026379</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0silly_mean.rmse  0silly_mean.timet\n",
       "Economics                  0.037792               0.58\n",
       "Buddhism                   0.015749               0.29\n",
       "Fitness                    0.020767               0.32\n",
       "Health                     0.040167               0.24\n",
       "Interpersonal              0.008395               0.23\n",
       "Rus_Stackoverflow          0.026379               1.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &  0silly\\_mean.rmse &  0silly\\_mean.timet \\\\\n",
      "\\midrule\n",
      "Economics         &          0.037792 &               0.58 \\\\\n",
      "Buddhism          &          0.015749 &               0.29 \\\\\n",
      "Fitness           &          0.020767 &               0.32 \\\\\n",
      "Health            &          0.040167 &               0.24 \\\\\n",
      "Interpersonal     &          0.008395 &               0.23 \\\\\n",
      "Rus\\_Stackoverflow &          0.026379 &               1.04 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewcount Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:34:39.013098\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.037722\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.015799\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.020627\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.040102\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.008177\n",
      "The root-mean-square error of \u001b[94mrus_stackoverflow's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.026345\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0silly_mean.rmse</th>\n",
       "      <th>0silly_mean.timet</th>\n",
       "      <th>1viewcount.rmse</th>\n",
       "      <th>1viewcount.timet</th>\n",
       "      <th>1viewcount.v.imprv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>0.037792</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.037722</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>3.68</td>\n",
       "      <td>-0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>0.040167</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.040102</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rus_Stackoverflow</th>\n",
       "      <td>0.026379</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>25.04</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0silly_mean.rmse  0silly_mean.timet  1viewcount.rmse  \\\n",
       "Economics                  0.037792               0.58         0.037722   \n",
       "Buddhism                   0.015749               0.29         0.015799   \n",
       "Fitness                    0.020767               0.32         0.020627   \n",
       "Health                     0.040167               0.24         0.040102   \n",
       "Interpersonal              0.008395               0.23         0.008177   \n",
       "Rus_Stackoverflow          0.026379               1.04         0.026345   \n",
       "\n",
       "                   1viewcount.timet  1viewcount.v.imprv  \n",
       "Economics                      7.00               0.185  \n",
       "Buddhism                       3.68              -0.317  \n",
       "Fitness                        3.92               0.674  \n",
       "Health                         3.56               0.162  \n",
       "Interpersonal                  3.44               2.597  \n",
       "Rus_Stackoverflow             25.04               0.129  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  0silly\\_mean.rmse &  0silly\\_mean.timet &  1viewcount.rmse &  1viewcount.timet &  1viewcount.v.imprv \\\\\n",
      "\\midrule\n",
      "Economics         &          0.037792 &               0.58 &         0.037722 &              7.00 &               0.185 \\\\\n",
      "Buddhism          &          0.015749 &               0.29 &         0.015799 &              3.68 &              -0.317 \\\\\n",
      "Fitness           &          0.020767 &               0.32 &         0.020627 &              3.92 &               0.674 \\\\\n",
      "Health            &          0.040167 &               0.24 &         0.040102 &              3.56 &               0.162 \\\\\n",
      "Interpersonal     &          0.008395 &               0.23 &         0.008177 &              3.44 &               2.597 \\\\\n",
      "Rus\\_Stackoverflow &          0.026379 &               1.04 &         0.026345 &             25.04 &               0.129 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 1.53 s, sys: 418 ms, total: 1.95 s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "#3min 56s\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'y_ravi'\n",
    "numic_variables = ['viewcount']\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "## numerical columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## date columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression on just viewcount\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100, # this doesn't change anything\n",
    "                      #regParam=0.3, # using regularisation parameter here useless since there is one feature\n",
    "                      #elasticNetParam=0.8,\n",
    "                      featuresCol=\"features\",\n",
    "                      labelCol=target,\n",
    "                      predictionCol=\"viewcount_pred\")\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "#.addGrid(lr.regParam, [1e-3, 1.])\n",
    "#.addGrid(lr.elasticNetParam, [1e-3, 1.])\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"y_ravi\", predictionCol=\"viewcount_pred\")\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## this fits on train, then predicts on test, using 3-CV\n",
    "    rmse = round( evaluator.evaluate(crossval.fit(train[i]).transform(test[i])), 6)\n",
    "    \n",
    "    ''' CODE TO USE LATER\n",
    "    cvModel = final_pipeline.fit(datasets[i])\n",
    "    cvModel.avgMetrics\n",
    "    list(zip(cvModel.avgMetrics, paramGrid))\n",
    "    cvModel.bestModel.stages\n",
    "    cvModel.bestModel.stages[1].extractParamMap()\n",
    "    '''\n",
    "        \n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m viewcount\\033[0m model is {rmse}\")\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 3 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timt = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['1viewcount.rmse'] = rmse\n",
    "    RESULTS[i.title()]['1viewcount.v.imprv'] = impr\n",
    "    RESULTS[i.title()]['1viewcount.timet'] = timt\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInteresting that there are different improvements of viewcount over mean-only prediciton\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Interesting that there are different improvements of viewcount over mean-only prediciton\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 234 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "import gc\n",
    "collected = gc.collect()\n",
    "print('Garbage collector: collected %d objects.' % collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "## custom transformer for nltk tokenisation\n",
    "class NLTKWordPunctTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(NLTKWordPunctTokenizer, self).__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            # get tokens with separate punctuation\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            # sort out russian stopwords\n",
    "            if ('я' in tokens):\n",
    "                tokens = [t for t in tokens if t not in nltk.corpus.stopwords.words('russian')]\n",
    "            # remove english stopwords\n",
    "            tokens = [t for t in tokens if t not in stopwords]\n",
    "            # remove single letters\n",
    "            tokens = [t for t in tokens if not len(t)==1]\n",
    "            # stemming the words\n",
    "            tokens = [ps.stem(t) for t in tokens]\n",
    "            # convert to lower case\n",
    "            return [t.lower() for t in tokens]\n",
    "    \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:35:27.385587\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m tokens\u001b[0m model is 0.037769\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m tokens\u001b[0m model is 0.015749\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m tokens\u001b[0m model is 0.020767\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m tokens\u001b[0m model is 0.040048\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m tokens\u001b[0m model is 0.008395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/6-train-test-split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "# ENGLISH\n",
    "# 8min 56s for no CV and no GRIDSEARCH\n",
    "# 17min 10s for 3-CV and no GRIDSEARCH\n",
    "# SMALL DATASETS\n",
    "# 36min 55s for no CV and no GRIDSEARCH\n",
    "# 12min 35s for 3-CV and no GRIDSEARCH\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'y_ravi'\n",
    "textt_variables = ['title', 'clean_body']\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "## date columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')\n",
    "\n",
    "## textual columns\n",
    "# tokenising text cols with custom transformer\n",
    "nltk_tokenizer_body = NLTKWordPunctTokenizer(\n",
    "    inputCol='clean_body', outputCol='body_words',  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "nltk_tokenizer_title = NLTKWordPunctTokenizer(\n",
    "    inputCol='title', outputCol='titl_words',  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# count occurence of tokens\n",
    "cnt_vectrizr_body = CountVectorizer(inputCol='body_words', outputCol='body_feats', minDF=2)\n",
    "cnt_vectrizr_title = CountVectorizer(inputCol='titl_words', outputCol='titl_feats', minDF=2)\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['body_feats', 'titl_feats'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[  #inputCols=['datet_data']\n",
    "    nltk_tokenizer_body, \n",
    "    nltk_tokenizer_title,\n",
    "    cnt_vectrizr_body,\n",
    "    cnt_vectrizr_title,\n",
    "    process_assembler\n",
    "])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression on just viewcount\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100,\n",
    "                      #regParam=0.3,\n",
    "                      #elasticNetParam=0.8,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol='tokens_pred')\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "'''NEEDED, BUT IMMENSELY SLOWING DOWN'''\n",
    "# Ravi et al use L2, aka ridge, aka elasticNetParam=0\n",
    "# regParam is the value of lambda\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-3, 1.]) \\\n",
    "    .addGrid(lr.regParam, [1e-3, 1.]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='y_ravi', predictionCol='tokens_pred')\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "'''DEFINITELY SLOWING DOWN'''\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV, store model and parameters, predict on test set\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 6)\n",
    "    \n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m tokens\\033[0m model is {rmse}\")\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 3 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timt = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2tokens.rmse'] = rmse\n",
    "    RESULTS[i.title()]['2tokens.v.imprv'] = impr\n",
    "    RESULTS[i.title()]['2tokens.timet'] = timt\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0silly_mean.rmse</th>\n",
       "      <th>0silly_mean.timet</th>\n",
       "      <th>1viewcount.rmse</th>\n",
       "      <th>1viewcount.timet</th>\n",
       "      <th>1viewcount.v.imprv</th>\n",
       "      <th>2tokens.rmse</th>\n",
       "      <th>2tokens.timet</th>\n",
       "      <th>2tokens.v.imprv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>0.037792</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.037722</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.037769</td>\n",
       "      <td>525.12</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>3.68</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>329.26</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>502.22</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>0.040167</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.040102</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.040048</td>\n",
       "      <td>315.08</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.597</td>\n",
       "      <td>0.008395</td>\n",
       "      <td>393.39</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rus_Stackoverflow</th>\n",
       "      <td>0.026379</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.026345</td>\n",
       "      <td>25.04</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0silly_mean.rmse  0silly_mean.timet  1viewcount.rmse  \\\n",
       "Economics                  0.037792               0.58         0.037722   \n",
       "Buddhism                   0.015749               0.29         0.015799   \n",
       "Fitness                    0.020767               0.32         0.020627   \n",
       "Health                     0.040167               0.24         0.040102   \n",
       "Interpersonal              0.008395               0.23         0.008177   \n",
       "Rus_Stackoverflow          0.026379               1.04         0.026345   \n",
       "\n",
       "                   1viewcount.timet  1viewcount.v.imprv  2tokens.rmse  \\\n",
       "Economics                      7.00               0.185      0.037769   \n",
       "Buddhism                       3.68              -0.317      0.015749   \n",
       "Fitness                        3.92               0.674      0.020767   \n",
       "Health                         3.56               0.162      0.040048   \n",
       "Interpersonal                  3.44               2.597      0.008395   \n",
       "Rus_Stackoverflow             25.04               0.129           NaN   \n",
       "\n",
       "                   2tokens.timet  2tokens.v.imprv  \n",
       "Economics                 525.12            0.061  \n",
       "Buddhism                  329.26           -0.000  \n",
       "Fitness                   502.22           -0.000  \n",
       "Health                    315.08            0.296  \n",
       "Interpersonal             393.39           -0.000  \n",
       "Rus_Stackoverflow            NaN              NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} &  0silly\\_mean.rmse &  0silly\\_mean.timet &  1viewcount.rmse &  1viewcount.timet &  1viewcount.v.imprv &  2tokens.rmse &  2tokens.timet &  2tokens.v.imprv \\\\\n",
      "\\midrule\n",
      "Economics         &          0.037792 &               0.58 &         0.037722 &              7.00 &               0.185 &      0.037769 &         525.12 &            0.061 \\\\\n",
      "Buddhism          &          0.015749 &               0.29 &         0.015799 &              3.68 &              -0.317 &      0.015749 &         329.26 &           -0.000 \\\\\n",
      "Fitness           &          0.020767 &               0.32 &         0.020627 &              3.92 &               0.674 &      0.020767 &         502.22 &           -0.000 \\\\\n",
      "Health            &          0.040167 &               0.24 &         0.040102 &              3.56 &               0.162 &      0.040048 &         315.08 &            0.296 \\\\\n",
      "Interpersonal     &          0.008395 &               0.23 &         0.008177 &              3.44 &               2.597 &      0.008395 &         393.39 &           -0.000 \\\\\n",
      "Rus\\_Stackoverflow &          0.026379 &               1.04 &         0.026345 &             25.04 &               0.129 &           NaN &            NaN &              NaN \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/6-train-test-split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'interpersonal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'interpersonal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_dict['interpersonal'].transform(test['interpersonal']).select('tokens_pred').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NLTKWordPunctTokenizer' object has no attribute '_to_java'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/6-train-test-split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## save models DOESN'T WORK BECAUSE: 'NLTKWordPunctTokenizer' object has no attribute '_to_java'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{i}-pipeline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;34m\"\"\"Returns an MLWriter instance for this ML instance.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mJavaMLWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJavaMLWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    472\u001b[0m         _java_obj = JavaParams._new_java_obj(\"org.apache.spark.ml.tuning.CrossValidatorModel\",\n\u001b[1;32m    473\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                                              _py2java(sc, []))\n\u001b[1;32m    476\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossValidatorModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mjava_stages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mjava_stages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mjava_stages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mjava_stages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NLTKWordPunctTokenizer' object has no attribute '_to_java'"
     ]
    }
   ],
   "source": [
    "## save models DOESN'T WORK BECAUSE: 'NLTKWordPunctTokenizer' object has no attribute '_to_java'\n",
    "for i in data_array:\n",
    "    param_dict[i].save(f'{i}-pipeline') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model parameters for \u001b[94meconomics\u001b[0m are     {Param(parent='LinearRegression_25580b8caa4c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LinearRegression_25580b8caa4c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001, Param(parent='LinearRegression_25580b8caa4c', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35, Param(parent='LinearRegression_25580b8caa4c', name='featuresCol', doc='features column name'): 'features', Param(parent='LinearRegression_25580b8caa4c', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LinearRegression_25580b8caa4c', name='labelCol', doc='label column name'): 'y_ravi', Param(parent='LinearRegression_25580b8caa4c', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError', Param(parent='LinearRegression_25580b8caa4c', name='maxIter', doc='maximum number of iterations (>= 0)'): 100, Param(parent='LinearRegression_25580b8caa4c', name='predictionCol', doc='prediction column name'): 'tokens_pred', Param(parent='LinearRegression_25580b8caa4c', name='regParam', doc='regularization parameter (>= 0)'): 1.0, Param(parent='LinearRegression_25580b8caa4c', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto', Param(parent='LinearRegression_25580b8caa4c', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LinearRegression_25580b8caa4c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n",
      "The best model parameters for \u001b[94mbuddhism\u001b[0m are     {Param(parent='LinearRegression_25580b8caa4c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LinearRegression_25580b8caa4c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001, Param(parent='LinearRegression_25580b8caa4c', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35, Param(parent='LinearRegression_25580b8caa4c', name='featuresCol', doc='features column name'): 'features', Param(parent='LinearRegression_25580b8caa4c', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LinearRegression_25580b8caa4c', name='labelCol', doc='label column name'): 'y_ravi', Param(parent='LinearRegression_25580b8caa4c', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError', Param(parent='LinearRegression_25580b8caa4c', name='maxIter', doc='maximum number of iterations (>= 0)'): 100, Param(parent='LinearRegression_25580b8caa4c', name='predictionCol', doc='prediction column name'): 'tokens_pred', Param(parent='LinearRegression_25580b8caa4c', name='regParam', doc='regularization parameter (>= 0)'): 1.0, Param(parent='LinearRegression_25580b8caa4c', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto', Param(parent='LinearRegression_25580b8caa4c', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LinearRegression_25580b8caa4c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n",
      "The best model parameters for \u001b[94mfitness\u001b[0m are     {Param(parent='LinearRegression_25580b8caa4c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LinearRegression_25580b8caa4c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001, Param(parent='LinearRegression_25580b8caa4c', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35, Param(parent='LinearRegression_25580b8caa4c', name='featuresCol', doc='features column name'): 'features', Param(parent='LinearRegression_25580b8caa4c', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LinearRegression_25580b8caa4c', name='labelCol', doc='label column name'): 'y_ravi', Param(parent='LinearRegression_25580b8caa4c', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError', Param(parent='LinearRegression_25580b8caa4c', name='maxIter', doc='maximum number of iterations (>= 0)'): 100, Param(parent='LinearRegression_25580b8caa4c', name='predictionCol', doc='prediction column name'): 'tokens_pred', Param(parent='LinearRegression_25580b8caa4c', name='regParam', doc='regularization parameter (>= 0)'): 1.0, Param(parent='LinearRegression_25580b8caa4c', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto', Param(parent='LinearRegression_25580b8caa4c', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LinearRegression_25580b8caa4c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n",
      "The best model parameters for \u001b[94mhealth\u001b[0m are     {Param(parent='LinearRegression_25580b8caa4c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LinearRegression_25580b8caa4c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001, Param(parent='LinearRegression_25580b8caa4c', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35, Param(parent='LinearRegression_25580b8caa4c', name='featuresCol', doc='features column name'): 'features', Param(parent='LinearRegression_25580b8caa4c', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LinearRegression_25580b8caa4c', name='labelCol', doc='label column name'): 'y_ravi', Param(parent='LinearRegression_25580b8caa4c', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError', Param(parent='LinearRegression_25580b8caa4c', name='maxIter', doc='maximum number of iterations (>= 0)'): 100, Param(parent='LinearRegression_25580b8caa4c', name='predictionCol', doc='prediction column name'): 'tokens_pred', Param(parent='LinearRegression_25580b8caa4c', name='regParam', doc='regularization parameter (>= 0)'): 1.0, Param(parent='LinearRegression_25580b8caa4c', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto', Param(parent='LinearRegression_25580b8caa4c', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LinearRegression_25580b8caa4c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n",
      "The best model parameters for \u001b[94minterpersonal\u001b[0m are     {Param(parent='LinearRegression_25580b8caa4c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LinearRegression_25580b8caa4c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 1.0, Param(parent='LinearRegression_25580b8caa4c', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35, Param(parent='LinearRegression_25580b8caa4c', name='featuresCol', doc='features column name'): 'features', Param(parent='LinearRegression_25580b8caa4c', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LinearRegression_25580b8caa4c', name='labelCol', doc='label column name'): 'y_ravi', Param(parent='LinearRegression_25580b8caa4c', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError', Param(parent='LinearRegression_25580b8caa4c', name='maxIter', doc='maximum number of iterations (>= 0)'): 100, Param(parent='LinearRegression_25580b8caa4c', name='predictionCol', doc='prediction column name'): 'tokens_pred', Param(parent='LinearRegression_25580b8caa4c', name='regParam', doc='regularization parameter (>= 0)'): 0.001, Param(parent='LinearRegression_25580b8caa4c', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto', Param(parent='LinearRegression_25580b8caa4c', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LinearRegression_25580b8caa4c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rus_stackoverflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/6-train-test-split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     print(f'The best model parameters for \\033[94m{i}\\033[0m are \\\n\u001b[0;32m----> 4\u001b[0;31m     {param_dict[i].bestModel.stages[1].extractParamMap()}')\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'rus_stackoverflow'"
     ]
    }
   ],
   "source": [
    "## look at the best parameters\n",
    "for i in data_array:\n",
    "    print(f'The best model parameters for \\033[94m{i}\\033[0m are \\\n",
    "    {param_dict[i].bestModel.stages[1].extractParamMap()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression_25580b8caa4c__aggregationDepth 2\n",
      "LinearRegression_25580b8caa4c__elasticNetParam 0.001\n",
      "LinearRegression_25580b8caa4c__epsilon 1.35\n",
      "LinearRegression_25580b8caa4c__featuresCol features\n",
      "LinearRegression_25580b8caa4c__fitIntercept True\n",
      "LinearRegression_25580b8caa4c__labelCol y_ravi\n",
      "LinearRegression_25580b8caa4c__loss squaredError\n",
      "LinearRegression_25580b8caa4c__maxIter 100\n",
      "LinearRegression_25580b8caa4c__predictionCol tokens_pred\n",
      "LinearRegression_25580b8caa4c__regParam 1.0\n",
      "LinearRegression_25580b8caa4c__solver auto\n",
      "LinearRegression_25580b8caa4c__standardization True\n",
      "LinearRegression_25580b8caa4c__tol 1e-06\n",
      "LinearRegression_25580b8caa4c__aggregationDepth 2\n",
      "LinearRegression_25580b8caa4c__elasticNetParam 0.001\n",
      "LinearRegression_25580b8caa4c__epsilon 1.35\n",
      "LinearRegression_25580b8caa4c__featuresCol features\n",
      "LinearRegression_25580b8caa4c__fitIntercept True\n",
      "LinearRegression_25580b8caa4c__labelCol y_ravi\n",
      "LinearRegression_25580b8caa4c__loss squaredError\n",
      "LinearRegression_25580b8caa4c__maxIter 100\n",
      "LinearRegression_25580b8caa4c__predictionCol tokens_pred\n",
      "LinearRegression_25580b8caa4c__regParam 1.0\n",
      "LinearRegression_25580b8caa4c__solver auto\n",
      "LinearRegression_25580b8caa4c__standardization True\n",
      "LinearRegression_25580b8caa4c__tol 1e-06\n",
      "LinearRegression_25580b8caa4c__aggregationDepth 2\n",
      "LinearRegression_25580b8caa4c__elasticNetParam 0.001\n",
      "LinearRegression_25580b8caa4c__epsilon 1.35\n",
      "LinearRegression_25580b8caa4c__featuresCol features\n",
      "LinearRegression_25580b8caa4c__fitIntercept True\n",
      "LinearRegression_25580b8caa4c__labelCol y_ravi\n",
      "LinearRegression_25580b8caa4c__loss squaredError\n",
      "LinearRegression_25580b8caa4c__maxIter 100\n",
      "LinearRegression_25580b8caa4c__predictionCol tokens_pred\n",
      "LinearRegression_25580b8caa4c__regParam 1.0\n",
      "LinearRegression_25580b8caa4c__solver auto\n",
      "LinearRegression_25580b8caa4c__standardization True\n",
      "LinearRegression_25580b8caa4c__tol 1e-06\n",
      "LinearRegression_25580b8caa4c__aggregationDepth 2\n",
      "LinearRegression_25580b8caa4c__elasticNetParam 0.001\n",
      "LinearRegression_25580b8caa4c__epsilon 1.35\n",
      "LinearRegression_25580b8caa4c__featuresCol features\n",
      "LinearRegression_25580b8caa4c__fitIntercept True\n",
      "LinearRegression_25580b8caa4c__labelCol y_ravi\n",
      "LinearRegression_25580b8caa4c__loss squaredError\n",
      "LinearRegression_25580b8caa4c__maxIter 100\n",
      "LinearRegression_25580b8caa4c__predictionCol tokens_pred\n",
      "LinearRegression_25580b8caa4c__regParam 1.0\n",
      "LinearRegression_25580b8caa4c__solver auto\n",
      "LinearRegression_25580b8caa4c__standardization True\n",
      "LinearRegression_25580b8caa4c__tol 1e-06\n",
      "LinearRegression_25580b8caa4c__aggregationDepth 2\n",
      "LinearRegression_25580b8caa4c__elasticNetParam 1.0\n",
      "LinearRegression_25580b8caa4c__epsilon 1.35\n",
      "LinearRegression_25580b8caa4c__featuresCol features\n",
      "LinearRegression_25580b8caa4c__fitIntercept True\n",
      "LinearRegression_25580b8caa4c__labelCol y_ravi\n",
      "LinearRegression_25580b8caa4c__loss squaredError\n",
      "LinearRegression_25580b8caa4c__maxIter 100\n",
      "LinearRegression_25580b8caa4c__predictionCol tokens_pred\n",
      "LinearRegression_25580b8caa4c__regParam 0.001\n",
      "LinearRegression_25580b8caa4c__solver auto\n",
      "LinearRegression_25580b8caa4c__standardization True\n",
      "LinearRegression_25580b8caa4c__tol 1e-06\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rus_stackoverflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/6-train-test-split.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractParamMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rus_stackoverflow'"
     ]
    }
   ],
   "source": [
    "for i in data_array:\n",
    "    for key, value in param_dict[i].bestModel.stages[1].extractParamMap().items():\n",
    "        print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict['economics'].bestModel.stages[1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import elements from natural language toolkit\n",
    "import nltk\n",
    "#nltk.download('all') # uncomment after first run as admin check\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def get_tokens(line):\n",
    "    '''\n",
    "    Function to parse text features\n",
    "    '''\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stopwords\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    '''lemmatise or stem???'''\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    # remove single letters\n",
    "    words = [word for word in words if not len(word)==1]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "## custom transformer to spread sparse vectors into individual columns\n",
    "class VectorMLliber(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer which converts a column of pyspark.ml vectors to multiple pyspark.mllib vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputCol=None):\n",
    "        super(VectorMLliber, self).__init__()\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        def f(v):\n",
    "            return Vectors.sparse(v.size, v.indices, v.values)\n",
    "        \n",
    "        df = df.rdd.map(lambda r: as_mllib_vector(r[0]))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???????????\n",
    "VectorMLliber_body = VectorMLliber(inputCol='body_features')\n",
    "VectorMLliber_title = VectorMLliber(inputCol='titl_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def as_mllib_vector(v):\n",
    "    return Vectors.sparse(v.size, v.indices, v.values)\n",
    "\n",
    "features = {}\n",
    "feature_vec_list = {}\n",
    "for i in data_array:\n",
    "    features[i] = word_feat_list[i].select(\"features\")\n",
    "    feature_vec_list[i] = features[i].rdd.map(lambda r: as_mllib_vector(r[0]))\n",
    "    feature_vec_list[i].cache()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trained_pipeline\n",
    " .transform(datasets['english'])\n",
    " .select(\n",
    "    indep_text_variables + [\"prediction\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"linreg_prediction.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions = spark.read.parquet(\"linreg_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions.select(\"prediction\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(estimator_pipeline, 'pipeline.joblib') \n",
    "\n",
    "reloaded = load(\"pipeline.joblib\")\n",
    "\n",
    "Now we can predict directly!\n",
    "\n",
    "reloaded.predict(X)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert notebook to python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script 0-master-notebook-pipelines.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
