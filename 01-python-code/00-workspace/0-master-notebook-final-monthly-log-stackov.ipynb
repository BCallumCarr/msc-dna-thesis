{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"Welcome to my PySpark analysis of the StackOverflow dataset\"\\n'\n"
     ]
    }
   ],
   "source": [
    "## testing printing output from console\n",
    "import subprocess\n",
    "\n",
    "cmd = [ 'echo', '\"Welcome to my PySpark analysis of the StackOverflow dataset\"' ]\n",
    "output = subprocess.Popen( cmd, stdout=subprocess.PIPE ).communicate()[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc #garbage collection\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# set pd float display options to not have trailing 0's\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spark UI, version 2.4.3, is available at: http://192.168.0.26:4040/ and the defaultParallelism is 4\n"
     ]
    }
   ],
   "source": [
    "%run -i '1-load-pyspark.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load easyFunctions and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## easy functions\n",
    "%run -i 'load_parquet_data.py'\n",
    "%run -i 'convert_stackov_csv_to_pyarrow_parquet.py'\n",
    "%run -i 'export_parquet_data.py'\n",
    "%run -i 'count_total_questions.py'\n",
    "%run -i 'show_save_results.py'\n",
    "%run -i 'show_spark_df.py'\n",
    "%run -i 'show_date_range.py'\n",
    "%run -i 'trim_betw_dates.py'\n",
    "\n",
    "## pipeline transformers\n",
    "%run -i 'nltkWordPunctTokeniser.py'\n",
    "%run -i 'nltkSenteniser.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## these are the forum labels that will appear in graphs/tables etc.\n",
    "data_array = [\n",
    "    #'Mar-09',\n",
    "    #'Apr-09',\n",
    "    \n",
    "    'May-09',\n",
    "    'Jun-09',\n",
    "    'Jul-09',\n",
    "    'Aug-09',\n",
    "    'Sep-09',\n",
    "    \n",
    "    #'Oct-09', #breaks here\n",
    "    #'Nov-09',\n",
    "    #'Dec-09'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download XML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call download script - EDIT to feed in user data_array\n",
    "#!bash 0-dataset-download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:48:04.327083\n",
      "CPU times: user 623 µs, sys: 636 µs, total: 1.26 ms\n",
      "Wall time: 1.07 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "## convert stackoverflow .json files to .parquet\n",
    "#for i in data_array:\n",
    "#    s = i.lower()\n",
    "#    convert_stackov_csv_to_pyarrow_parquet(strDirect=f'initial-data/{s}.stackexchange.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through data_array to convert to other fora to .parquet\n",
    "#for i in data_array:\n",
    "#    s = i.lower()\n",
    "#    !python convert-xml-to-parquet.py \"$s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Initial or Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:48:04.346875\n",
      "CPU times: user 5.07 ms, sys: 2.71 ms, total: 7.79 ms\n",
      "Wall time: 4.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "datasets = load_parquet_data(dataArray=data_array, kind='clean', printSchema=False) #, gcpath='gs://bucket-brad-project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-09: 26026\n",
      "Jun-09: 28555\n",
      "Jul-09: 32752\n",
      "Aug-09: 32998\n",
      "Sep-09: 33268\n",
      "\n",
      "Total: 153599\n"
     ]
    }
   ],
   "source": [
    "## count questions before trimming\n",
    "count_total_questions(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "### Create shifted score variable\n",
    "#########################################################################\n",
    "\n",
    "# import lit function\n",
    "from pyspark.sql.functions import lit, expr\n",
    "\n",
    "# find minimum of score for shifting\n",
    "minp = 1000\n",
    "for i in data_array:\n",
    "    if (min(datasets[i].select('score').rdd.flatMap(lambda x: x).collect()) < minp):\n",
    "        # absolute value because min score is negative\n",
    "        minp = min( datasets[i].select('score').rdd.flatMap(lambda x: x).collect() )\n",
    "\n",
    "# adjust minimum to incremented absolute value\n",
    "minp = abs(minp) + 2\n",
    "\n",
    "# create new shifted score variable\n",
    "for i in data_array:\n",
    "    datasets[i] = datasets[i].withColumn('new_column', lit(minp)).\\\n",
    "    withColumn('score_shift', expr(\"new_column + score\"))\n",
    "\n",
    "#########################################################################\n",
    "### Create logged score and viewcount variables\n",
    "#########################################################################\n",
    "\n",
    "# import log function\n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "# create new logged score variable\n",
    "for i in data_array:\n",
    "    datasets[i] = datasets[i].withColumn('score_shift_log', log('score_shift'))\n",
    "    \n",
    "# create new logged viewcount variable\n",
    "for i in data_array:\n",
    "    datasets[i] = datasets[i].withColumn('viewcount_log', log('viewcount'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Trim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:48:16.283165\n",
      "CPU times: user 322 µs, sys: 76 µs, total: 398 µs\n",
      "Wall time: 360 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "#%run -i '2-clean-datasets.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-09:\n",
      "2009-05-01 00:02:42.960000\n",
      "2009-05-31 23:56:47.303000\n",
      "\n",
      "Jun-09:\n",
      "2009-06-01 00:06:57.250000\n",
      "2009-06-30 23:59:14.543000\n",
      "\n",
      "Jul-09:\n",
      "2009-07-01 00:01:35.740000\n",
      "2009-07-31 23:58:46.123000\n",
      "\n",
      "Aug-09:\n",
      "2009-08-01 00:02:10.593000\n",
      "2009-08-31 23:59:12.223000\n",
      "\n",
      "Sep-09:\n",
      "2009-09-01 00:01:17.477000\n",
      "2009-09-30 23:59:36.613000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## show range of dates for datasets before trimming\n",
    "show_date_range(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trim data between uniform date range\n",
    "#datasets = trim_betw_dates(dataArray=data_array, datasetDict=datasets, dates=('2014-06-01', '2017-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-09:\n",
      "2009-05-01 00:02:42.960000\n",
      "2009-05-31 23:56:47.303000\n",
      "\n",
      "Jun-09:\n",
      "2009-06-01 00:06:57.250000\n",
      "2009-06-30 23:59:14.543000\n",
      "\n",
      "Jul-09:\n",
      "2009-07-01 00:01:35.740000\n",
      "2009-07-31 23:58:46.123000\n",
      "\n",
      "Aug-09:\n",
      "2009-08-01 00:02:10.593000\n",
      "2009-08-31 23:59:12.223000\n",
      "\n",
      "Sep-09:\n",
      "2009-09-01 00:01:17.477000\n",
      "2009-09-30 23:59:36.613000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## show range of dates for datasets after trimming\n",
    "show_date_range(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-09: 26026\n",
      "Jun-09: 28555\n",
      "Jul-09: 32752\n",
      "Aug-09: 32998\n",
      "Sep-09: 33268\n",
      "\n",
      "Total: 153599\n"
     ]
    }
   ],
   "source": [
    "## count questions after trimming\n",
    "count_total_questions(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts and LDA Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:48:21.513020\n",
      "CPU times: user 865 µs, sys: 552 µs, total: 1.42 ms\n",
      "Wall time: 1.45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# about 16 min with 10 iterations and params for 29000 questions\n",
    "# FAILS for large datasets\n",
    "print(datetime.now().time())\n",
    "#%run -i '3-feat-engineering.py'\n",
    "\n",
    "###\n",
    "# TRY different values of alpha and rho\n",
    "# TRY different values of K\n",
    "# TRY 'em' optimizer instead of 'online'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:32:19.049283\n",
      "CPU times: user 4.15 ms, sys: 2.89 ms, total: 7.04 ms\n",
      "Wall time: 9.61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# about 12 min\n",
    "print(datetime.now().time())\n",
    "#export_parquet_data(dataArray=data_array, datasetDict=datasets, folder='cleaner-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '4-final-eda.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 304 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "collected = gc.collect()\n",
    "print(f'Garbage collector: collected {collected} objects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:48:21.633158\n",
      "\n",
      "Standard deviations are:\n",
      "\n",
      "May-09 & 0.427 & 0.425 & -0.468 \\\\\n",
      "Jun-09 & 0.434 & 0.427 & -1.613 \\\\\n",
      "Jul-09 & 0.403 & 0.4 & -0.744 \\\\\n",
      "Aug-09 & 0.362 & 0.388 & 7.182 \\\\\n",
      "Sep-09 & 0.35 & 0.365 & 4.286 \\\\\n",
      "CPU times: user 80.1 ms, sys: 22.8 ms, total: 103 ms\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '6a-rand-train-test-split-50.py'\n",
    "#%run -i '6b-time-train-test-split-50.py'\n",
    "\n",
    "## check standard deviations of variables\n",
    "print('\\nStandard deviations are:\\n')\n",
    "for i in data_array:\n",
    "    tr = round( pd.to_numeric(train[i].describe('score_shift_log').select('score_shift_log').toPandas().iloc[2][0]), 3 )\n",
    "    te = round( pd.to_numeric(test[i].describe('score_shift_log').select('score_shift_log').toPandas().iloc[2][0]), 3 )\n",
    "    perc = round( (te/tr - 1)*100, 3 )\n",
    "    s = i.title() + ' & ' + str(tr) + ' & ' + str(te) + ' & ' + str(perc) + ' \\\\\\\\'\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pyspark sql functions\n",
    "from pyspark.sql.functions import lit, avg, col\n",
    "\n",
    "## for evaluation metrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "## for pipelines\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "## for model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "## for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Target, Modeling Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## features not used\\ndatet_variables = ['clean_date']\\n\\n## assemblers not used\\ndatet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## choose target variable\n",
    "target = 'score_shift_log'\n",
    "\n",
    "## setup linear regression model on just viewcount\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      #regParam=0.3,\n",
    "                      #elasticNetParam=0.8,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target)\n",
    "                      #predictionCol='preds')\n",
    "    \n",
    "## choose evaluation metric\n",
    "ev_metric = 'rmse' #'mae'\n",
    "\n",
    "## choose number of CV folds\n",
    "num_folds = 2\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "'''NEEDED, BUT SLOWS DOWN'''\n",
    "# Ravi et al use L2, aka ridge, aka elasticNetParam=0\n",
    "# regParam is the value of lambda\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-2, 1.]) \\\n",
    "    .addGrid(lr.regParam, [1e-2, 1.]) \\\n",
    "    .build()\n",
    "\n",
    "## blank tuning grid\n",
    "noGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "## prediction column names\n",
    "mean_preds = 'mean_preds'\n",
    "viewc_preds = 'viewc_preds'\n",
    "leng_preds = 'leng_preds'\n",
    "token_preds = 'token_preds'\n",
    "topic_preds = 'topic_preds'\n",
    "final_preds = 'final_preds'\n",
    "\n",
    "'''\n",
    "## features not used\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "## assemblers not used\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silly Mean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root-mean-square error of \u001b[94mMay-09's\u001b[0m\u001b[92m mean\u001b[0m model is 0.42\n",
      "The root-mean-square error of \u001b[94mJun-09's\u001b[0m\u001b[92m mean\u001b[0m model is 0.43\n",
      "The root-mean-square error of \u001b[94mJul-09's\u001b[0m\u001b[92m mean\u001b[0m model is 0.4\n",
      "The root-mean-square error of \u001b[94mAug-09's\u001b[0m\u001b[92m mean\u001b[0m model is 0.39\n",
      "The root-mean-square error of \u001b[94mSep-09's\u001b[0m\u001b[92m mean\u001b[0m model is 0.36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0silly_mean.0tr_rmse</th>\n",
       "      <th>0silly_mean.1rmse</th>\n",
       "      <th>0silly_mean.2timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>May-09</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jun-09</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jul-09</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aug-09</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sep-09</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0silly_mean.0tr_rmse  0silly_mean.1rmse  0silly_mean.2timet\n",
       "May-09                  0.43               0.42                 1.0\n",
       "Jun-09                  0.43               0.43                 1.0\n",
       "Jul-09                  0.40               0.40                 1.0\n",
       "Aug-09                  0.36               0.39                 1.0\n",
       "Sep-09                  0.35               0.36                 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  0silly\\_mean.0tr\\_rmse &  0silly\\_mean.1rmse &  0silly\\_mean.2timet \\\\\n",
      "\\midrule\n",
      "May-09 &                  0.43 &               0.42 &                 1.0 \\\\\n",
      "Jun-09 &                  0.43 &               0.43 &                 1.0 \\\\\n",
      "Jul-09 &                  0.40 &               0.40 &                 1.0 \\\\\n",
      "Aug-09 &                  0.36 &               0.39 &                 1.0 \\\\\n",
      "Sep-09 &                  0.35 &               0.36 &                 1.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## create mean dictionaries\n",
    "tr_means = {}\n",
    "\n",
    "## calculate the mean of each forum, using ONLY training set\n",
    "for i in data_array:\n",
    "    tr_means[i] = train[i].select(target).rdd.flatMap(lambda x: x).mean() #.agg(avg(col(target))).collect()[0][0]\n",
    "\n",
    "## create dictionaries for training and testing (baseline) rmse \n",
    "base = {}\n",
    "tr_rmse = {}\n",
    "\n",
    "## setup rmse evaluator\n",
    "model_evaluator = RegressionEvaluator(metricName=ev_metric, labelCol=target, predictionCol=mean_preds)\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "\n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## train silly mean model by assigning training set mean for training and testing predictions\n",
    "    train[i] = train[i].withColumn(mean_preds, lit(tr_means[i]))\n",
    "    test[i] = test[i].withColumn(mean_preds, lit(tr_means[i]))\n",
    "\n",
    "    ## evaluate silly mean model, on both training and testing set\n",
    "    tr_rmse[i] = round( model_evaluator.evaluate(train[i]), 2 )\n",
    "    base[i] = round( model_evaluator.evaluate(test[i]), 2 )\n",
    "\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m mean\\033[0m model is {base[i]}\")\n",
    "\n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 0 )\n",
    "    \n",
    "    ## store as dictionary inside RESULTS dictionary, initiating dataset name entries first\n",
    "    RESULTS[i.title()]['0silly_mean.0tr_rmse'] = tr_rmse[i]\n",
    "    RESULTS[i.title()]['0silly_mean.1rmse'] = base[i]\n",
    "    RESULTS[i.title()]['0silly_mean.2timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewcount Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:52:19.047921\n",
      "The root-mean-square error of \u001b[94mMay-09's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.31\n",
      "The root-mean-square error of \u001b[94mJun-09's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.32\n",
      "The root-mean-square error of \u001b[94mJul-09's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.3\n",
      "The root-mean-square error of \u001b[94mAug-09's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.29\n",
      "The root-mean-square error of \u001b[94mSep-09's\u001b[0m\u001b[92m viewcount\u001b[0m model is 0.28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1viewcount.0tr_rmse</th>\n",
       "      <th>1viewcount.1rmse</th>\n",
       "      <th>1viewcount.2imprv</th>\n",
       "      <th>1viewcount.3timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>May-09</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>26.19</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jun-09</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>25.58</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jul-09</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>25.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aug-09</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>25.64</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sep-09</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.28</td>\n",
       "      <td>22.22</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1viewcount.0tr_rmse  1viewcount.1rmse  1viewcount.2imprv  \\\n",
       "May-09                 0.31              0.31              26.19   \n",
       "Jun-09                 0.32              0.32              25.58   \n",
       "Jul-09                 0.30              0.30              25.00   \n",
       "Aug-09                 0.28              0.29              25.64   \n",
       "Sep-09                 0.27              0.28              22.22   \n",
       "\n",
       "        1viewcount.3timet  \n",
       "May-09                7.0  \n",
       "Jun-09                6.0  \n",
       "Jul-09                6.0  \n",
       "Aug-09                6.0  \n",
       "Sep-09                6.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  1viewcount.0tr\\_rmse &  1viewcount.1rmse &  1viewcount.2imprv &  1viewcount.3timet \\\\\n",
      "\\midrule\n",
      "May-09 &                 0.31 &              0.31 &              26.19 &                7.0 \\\\\n",
      "Jun-09 &                 0.32 &              0.32 &              25.58 &                6.0 \\\\\n",
      "Jul-09 &                 0.30 &              0.30 &              25.00 &                6.0 \\\\\n",
      "Aug-09 &                 0.28 &              0.29 &              25.64 &                6.0 \\\\\n",
      "Sep-09 &                 0.27 &              0.28 &              22.22 &                6.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 1.03 s, sys: 268 ms, total: 1.29 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS #####\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "numic_variables = ['viewcount_log']\n",
    "\n",
    "## numerical columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "\n",
    "####################\n",
    "##### MODELING #####\n",
    "####################\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## linear regression model\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol=viewc_preds)\n",
    "\n",
    "## setup rmse evaluator\n",
    "model_evaluator = RegressionEvaluator(metricName=ev_metric, labelCol=target, predictionCol=viewc_preds)\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=noGrid,\n",
    "                          evaluator=model_evaluator,\n",
    "                          numFolds=num_folds)\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    \n",
    "    ## fitting on train and predicting on train/test\n",
    "    tr_rmse = round( model_evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( model_evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "        \n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m viewcount\\033[0m model is {rmse}\")\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 0 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['1viewcount.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['1viewcount.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['1viewcount.2imprv'] = impr\n",
    "    RESULTS[i.title()]['1viewcount.3timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:59:05.538687\n",
      "The root-mean-square error of \u001b[94mMay-09's\u001b[0m\u001b[92m counts\u001b[0m model is 0.42\n",
      "The root-mean-square error of \u001b[94mJun-09's\u001b[0m\u001b[92m counts\u001b[0m model is 0.43\n",
      "The root-mean-square error of \u001b[94mJul-09's\u001b[0m\u001b[92m counts\u001b[0m model is 0.4\n",
      "The root-mean-square error of \u001b[94mAug-09's\u001b[0m\u001b[92m counts\u001b[0m model is 0.39\n",
      "The root-mean-square error of \u001b[94mSep-09's\u001b[0m\u001b[92m counts\u001b[0m model is 0.36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1viewcount.0tr_rmse</th>\n",
       "      <th>1viewcount.1rmse</th>\n",
       "      <th>1viewcount.2imprv</th>\n",
       "      <th>1viewcount.3timet</th>\n",
       "      <th>2counts.0tr_rmse</th>\n",
       "      <th>2counts.1rmse</th>\n",
       "      <th>2counts.2imprv</th>\n",
       "      <th>2counts.3regular</th>\n",
       "      <th>2counts.4elastic</th>\n",
       "      <th>2counts.5timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>May-09</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>26.19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jun-09</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>25.58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jul-09</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>25.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aug-09</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>25.64</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sep-09</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.28</td>\n",
       "      <td>22.22</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1viewcount.0tr_rmse  1viewcount.1rmse  1viewcount.2imprv  \\\n",
       "May-09                 0.31              0.31              26.19   \n",
       "Jun-09                 0.32              0.32              25.58   \n",
       "Jul-09                 0.30              0.30              25.00   \n",
       "Aug-09                 0.28              0.29              25.64   \n",
       "Sep-09                 0.27              0.28              22.22   \n",
       "\n",
       "        1viewcount.3timet  2counts.0tr_rmse  2counts.1rmse  2counts.2imprv  \\\n",
       "May-09                7.0              0.43           0.42            -0.0   \n",
       "Jun-09                6.0              0.43           0.43            -0.0   \n",
       "Jul-09                6.0              0.40           0.40            -0.0   \n",
       "Aug-09                6.0              0.36           0.39            -0.0   \n",
       "Sep-09                6.0              0.35           0.36            -0.0   \n",
       "\n",
       "        2counts.3regular  2counts.4elastic  2counts.5timet  \n",
       "May-09               0.0               0.0            11.0  \n",
       "Jun-09               0.0               0.0            12.0  \n",
       "Jul-09               0.0               0.0            12.0  \n",
       "Aug-09               0.0               0.0            13.0  \n",
       "Sep-09               0.0               0.0            12.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &  1viewcount.0tr\\_rmse &  1viewcount.1rmse &  1viewcount.2imprv &  1viewcount.3timet &  2counts.0tr\\_rmse &  2counts.1rmse &  2counts.2imprv &  2counts.3regular &  2counts.4elastic &  2counts.5timet \\\\\n",
      "\\midrule\n",
      "May-09 &                 0.31 &              0.31 &              26.19 &                7.0 &              0.43 &           0.42 &            -0.0 &               0.0 &               0.0 &            11.0 \\\\\n",
      "Jun-09 &                 0.32 &              0.32 &              25.58 &                6.0 &              0.43 &           0.43 &            -0.0 &               0.0 &               0.0 &            12.0 \\\\\n",
      "Jul-09 &                 0.30 &              0.30 &              25.00 &                6.0 &              0.40 &           0.40 &            -0.0 &               0.0 &               0.0 &            12.0 \\\\\n",
      "Aug-09 &                 0.28 &              0.29 &              25.64 &                6.0 &              0.36 &           0.39 &            -0.0 &               0.0 &               0.0 &            13.0 \\\\\n",
      "Sep-09 &                 0.27 &              0.28 &              22.22 &                6.0 &              0.35 &           0.36 &            -0.0 &               0.0 &               0.0 &            12.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 4.3 s, sys: 1.09 s, total: 5.39 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS #####\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "numic_variables = ['body_word_cnt', 'titl_word_cnt', 'body_char_cnt', \n",
    "                   'titl_char_cnt', 'body_sent_cnt', 'titl_sent_cnt']\n",
    "\n",
    "## NUMERICAL columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## create PROCESSING pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol=leng_preds)\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName=ev_metric, labelCol=target, predictionCol=leng_preds)\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=num_folds)\n",
    "\n",
    "## create models dict for recording gridsearch params\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m counts\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 0 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2counts.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['2counts.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['2counts.2imprv'] = impr\n",
    "    RESULTS[i.title()]['2counts.3regular'] = reg_param\n",
    "    RESULTS[i.title()]['2counts.4elastic'] = ela_param    \n",
    "    RESULTS[i.title()]['2counts.5timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:00:05.792556\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS #####\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "textt_variables = ['title', 'clean_body']\n",
    "\n",
    "## textual columns\n",
    "# tokenising text cols with custom transformer\n",
    "nltk_tokeniser_body = nltkWordPunctTokeniser(\n",
    "    inputCol='clean_body', outputCol='body_words',  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "nltk_tokeniser_title = nltkWordPunctTokeniser(\n",
    "    inputCol='title', outputCol='titl_words',  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# count occurence of tokens, i.e. create dfm\n",
    "cnt_vectrizr_body = CountVectorizer(inputCol='body_words', outputCol='body_raw_feats') #!!! minDF???\n",
    "cnt_vectrizr_title = CountVectorizer(inputCol='titl_words', outputCol='titl_raw_feats')\n",
    "\n",
    "# create IDF dfm\n",
    "idf_body = IDF(inputCol=\"body_raw_feats\", outputCol=\"body_feats\")\n",
    "idf_title = IDF(inputCol=\"titl_raw_feats\", outputCol=\"titl_feats\")\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['body_feats', 'titl_feats'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[  #inputCols=['datet_data']\n",
    "    nltk_tokeniser_body, \n",
    "    nltk_tokeniser_title,\n",
    "    cnt_vectrizr_body,\n",
    "    cnt_vectrizr_title,\n",
    "    idf_body,\n",
    "    idf_title,\n",
    "    process_assembler\n",
    "])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol=token_preds)\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName=ev_metric, labelCol=target, predictionCol=token_preds)\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=num_folds)\n",
    "\n",
    "## create models dict for recording gridsearch params\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m tokens\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 0 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2tokens.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['2tokens.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['2tokens.2imprv'] = impr\n",
    "    RESULTS[i.title()]['2tokens.3regular'] = reg_param\n",
    "    RESULTS[i.title()]['2tokens.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['2tokens.5timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check predictions aren't constant\n",
    "models[data_array[0]].transform(test[data_array[0]]).select('token_preds').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS #####\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "numic_variables = ['btd[0]', 'btd[1]', 'btd[2]', 'btd[3]', 'btd[4]', \n",
    "                   'btd[5]', 'btd[6]', 'btd[7]', 'btd[8]', 'btd[9]', \n",
    "                   'std[0]', 'std[1]', 'std[2]', 'std[3]', 'std[4]', \n",
    "                   'std[5]', 'std[6]', 'std[7]', 'std[8]', 'std[9]',\n",
    "                   'ttd[0]', 'ttd[1]', 'ttd[2]', 'ttd[3]', 'ttd[4]', \n",
    "                   'ttd[5]', 'ttd[6]', 'ttd[7]', 'ttd[8]', 'ttd[9]'] \n",
    "\n",
    "## numerical columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol=topic_preds)\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName=ev_metric, labelCol=target, predictionCol=topic_preds)\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=num_folds)\n",
    "\n",
    "## create models dict for recording gridsearch params\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m topics\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 0 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2topics.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['2topics.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['2topics.2imprv'] = impr\n",
    "    RESULTS[i.title()]['2topics.3regular'] = reg_param\n",
    "    RESULTS[i.title()]['2topics.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['2topics.5timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS #####\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "numic_variables = ['btd[0]', 'btd[1]', 'btd[2]', 'btd[3]', 'btd[4]', \n",
    "                   'btd[5]', 'btd[6]', 'btd[7]', 'btd[8]', 'btd[9]', \n",
    "                   'std[0]', 'std[1]', 'std[2]', 'std[3]', 'std[4]', \n",
    "                   'std[5]', 'std[6]', 'std[7]', 'std[8]', 'std[9]',\n",
    "                   'ttd[0]', 'ttd[1]', 'ttd[2]', 'ttd[3]', 'ttd[4]', \n",
    "                   'ttd[5]', 'ttd[6]', 'ttd[7]', 'ttd[8]', 'ttd[9]',\n",
    "                   'body_word_cnt', 'titl_word_cnt', 'body_char_cnt', \n",
    "                   'titl_char_cnt', 'body_sent_cnt', 'titl_sent_cnt']\n",
    "\n",
    "### NUMERICAL columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## create PROCESSING pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[  #inputCols=['datet_data']\n",
    "    numic_pipeline,\n",
    "    process_assembler\n",
    "])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol=final_preds)\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName=ev_metric, labelCol=target, predictionCol=final_preds)\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=num_folds)\n",
    "\n",
    "## create models dict for recording gridsearch params\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m final\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 0 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['3final.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['3final.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['3final.2imprv'] = impr\n",
    "    RESULTS[i.title()]['3final.3regular'] = reg_param\n",
    "    RESULTS[i.title()]['3final.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['3final.5timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check predictions aren't constant\n",
    "models[data_array[0]].transform(test[data_array[0]]).select('final_preds').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trained_pipeline\n",
    " .transform(datasets['english'])\n",
    " .select(\n",
    "    indep_text_variables + [\"prediction\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"linreg_prediction.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions = spark.read.parquet(\"linreg_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions.select(\"prediction\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(estimator_pipeline, 'pipeline.joblib') \n",
    "\n",
    "reloaded = load(\"pipeline.joblib\")\n",
    "\n",
    "#Now we can predict directly!\n",
    "\n",
    "reloaded.predict(X)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## save models DOESN'T WORK BECAUSE: 'NLTKWordPunctTokenizer' object has no attribute '_to_java'\n",
    "for i in data_array:\n",
    "    param_dict[i].save(f'{i}-pipeline') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert notebook to python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script 0-master-notebook-pipelines.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
