{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"Welcome to my PySpark analysis of some StackExchange Data\"\\n'\n"
     ]
    }
   ],
   "source": [
    "## testing printing output from console\n",
    "import subprocess\n",
    "\n",
    "cmd = [ 'echo', '\"Welcome to my PySpark analysis of some StackExchange Data\"' ]\n",
    "output = subprocess.Popen( cmd, stdout=subprocess.PIPE ).communicate()[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc #garbage collection\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spark UI, version 2.4.3, is available at: http://192.168.0.26:4040/ and the defaultParallelism is 4\n"
     ]
    }
   ],
   "source": [
    "%run -i '1-load-pyspark.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load easyFunctions and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## easy functions\n",
    "%run -i 'load_parquet_data.py'\n",
    "%run -i 'convert_csv_to_pyarrow_parquet.py'\n",
    "%run -i 'export_parquet_data.py'\n",
    "%run -i 'count_total_questions.py'\n",
    "%run -i 'show_save_results.py'\n",
    "%run -i 'show_spark_df.py'\n",
    "%run -i 'show_date_range.py'\n",
    "%run -i 'trim_betw_dates.py'\n",
    "\n",
    "## pipeline transformers\n",
    "%run -i 'nltkWordPunctTokeniser.py'\n",
    "%run -i 'nltkSenteniser.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## these are the forum labels that will appear in graphs/tables etc.\n",
    "data_array = [\n",
    "    #'Buddhism',\n",
    "    #'Economics',\n",
    "    #'Fitness',\n",
    "    #'Health',\n",
    "    #'Interpersonal'\n",
    "    #'Stackoverflow',\n",
    "    #'Superuser',\n",
    "    'Math',\n",
    "    'Stats',\n",
    "    'English',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download XML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call download script - EDIT to feed in user data_array\n",
    "#!bash 0-dataset-download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## for stackoverflow\n",
    "#convert_csv_to_pyarrow_parquet(str_dir='initial-data/stackoverflow.stackexchange.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through data_array\n",
    "#for i in data_array:\n",
    "#    s = i.lower()\n",
    "#    !python convert-xml-to-parquet.py \"$s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Initial or Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:42:32.601525\n",
      "CPU times: user 3.18 ms, sys: 2.11 ms, total: 5.28 ms\n",
      "Wall time: 501 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "datasets = load_parquet_data(dataArray=data_array, kind='initial', printSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join Stack Overflow datasets on gcloud\n",
    "#dfUnion = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math: 1102812\n",
      "Stats: 139302\n",
      "English: 105475\n",
      "\n",
      "Total: 1347589\n"
     ]
    }
   ],
   "source": [
    "## count questions before trimming\n",
    "count_total_questions(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Trim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:42:34.135001\n",
      "\n",
      "\u001b[1m checking columns are the right types and names \u001b[0m\n",
      "\n",
      "----- Math -----\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_date: timestamp (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "----- Stats -----\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_date: timestamp (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "----- English -----\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_date: timestamp (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "\n",
      "\u001b[1m checking that there are no nans \u001b[0m\n",
      "\n",
      "----- Math -----\n",
      "+-----+---------+-----+----------+\n",
      "|title|viewcount|score|clean_body|\n",
      "+-----+---------+-----+----------+\n",
      "|    0|        0|    0|         0|\n",
      "+-----+---------+-----+----------+\n",
      "\n",
      "----- Stats -----\n",
      "+-----+---------+-----+----------+\n",
      "|title|viewcount|score|clean_body|\n",
      "+-----+---------+-----+----------+\n",
      "|    0|        0|    0|         0|\n",
      "+-----+---------+-----+----------+\n",
      "\n",
      "----- English -----\n",
      "+-----+---------+-----+----------+\n",
      "|title|viewcount|score|clean_body|\n",
      "+-----+---------+-----+----------+\n",
      "|    0|        0|    0|         0|\n",
      "+-----+---------+-----+----------+\n",
      "\n",
      "CPU times: user 74.7 ms, sys: 24.4 ms, total: 99.1 ms\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '2-clean-datasets.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math:\n",
      "2010-03-27 14:33:20.727000\n",
      "2019-06-02 04:16:34.197000\n",
      "\n",
      "Stats:\n",
      "2009-02-02 14:21:12.103000\n",
      "2019-06-02 04:19:33.143000\n",
      "\n",
      "English:\n",
      "2009-06-16 13:06:53.033000\n",
      "2019-06-02 04:02:54.070000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## show range of dates for datasets before trimming\n",
    "show_date_range(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trim data between uniform date range\n",
    "datasets = trim_betw_dates(dataArray=data_array, datasetDict=datasets, dates=('2010-09-01', '2011-09-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math:\n",
      "2010-09-01 02:55:46.223000\n",
      "2011-08-31 23:35:26.480000\n",
      "\n",
      "Stats:\n",
      "2010-09-01 05:55:30.923000\n",
      "2011-08-31 22:41:00.767000\n",
      "\n",
      "English:\n",
      "2010-09-01 03:12:19.943000\n",
      "2011-08-31 22:27:31.583000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## show range of dates for datasets after trimming\n",
    "show_date_range(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math: 18131\n",
      "Stats: 4068\n",
      "English: 8537\n",
      "\n",
      "Total: 30736\n"
     ]
    }
   ],
   "source": [
    "## count questions after trimming\n",
    "count_total_questions(data_array, temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts and LDA Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:43:38.885624\n",
      "On to Math\n",
      "On to Stats\n",
      "On to English\n",
      "On to Math\n",
      "On to Stats\n",
      "On to English\n",
      "CPU times: user 1.04 s, sys: 562 ms, total: 1.6 s\n",
      "Wall time: 23min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# about 15 min with 10 iterations and params\n",
    "# FAILS for large datasets\n",
    "print(datetime.now().time())\n",
    "%run -i '3-feat-engineering.py'\n",
    "\n",
    "###\n",
    "# TRY different values of alpha and rho\n",
    "# TRY different values of K\n",
    "# TRY 'em' optimizer instead of 'online'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:13:44.175946\n",
      "On to Superuser\n",
      "On to Math\n",
      "On to Stats\n",
      "On to English\n",
      "CPU times: user 13.6 ms, sys: 15.4 ms, total: 28.9 ms\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# about 8 min\n",
    "print(datetime.now().time())\n",
    "export_parquet_data(dataArray=data_array, datasetDict=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:55:18.897768\n",
      "\n",
      "\u001b[1mCorrelations between score and viewcount\u001b[0m\n",
      "\n",
      "Buddhism & 0.41 \\\\\n",
      "Economics & 0.67 \\\\\n",
      "Fitness & 0.26 \\\\\n",
      "Health & 0.21 \\\\\n",
      "Interpersonal & 0.87 \\\\\n",
      "\n",
      "\u001b[1mViewcount\u001b[0m descriptives\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "0 &  count &     mean &   stddev &  min &    max \\\\\n",
      "\\midrule\n",
      "Buddhism      &   3120 &   316.16 &   737.08 &   10 &  21498 \\\\\n",
      "Economics     &   3120 &   178.24 &   646.26 &    2 &  14055 \\\\\n",
      "Fitness       &   3120 &   534.35 &  2426.63 &    8 &  78829 \\\\\n",
      "Health        &   3120 &   240.40 &  1128.95 &    2 &  23098 \\\\\n",
      "Interpersonal &   3120 &  4520.74 &  7492.72 &    6 &  79049 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\u001b[1mScore\u001b[0m descriptives\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "0 &  count &   mean &  stddev &  min &  max \\\\\n",
      "\\midrule\n",
      "Buddhism      &   3120 &   2.01 &    2.19 &   -7 &   24 \\\\\n",
      "Economics     &   3120 &   1.47 &    2.70 &   -7 &   61 \\\\\n",
      "Fitness       &   3120 &   1.77 &    2.14 &   -6 &   28 \\\\\n",
      "Health        &   3120 &   2.05 &    2.06 &   -5 &   27 \\\\\n",
      "Interpersonal &   3120 &  16.38 &   23.70 &   -9 &  265 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\u001b[1mviewcount\u001b[0m\n",
      "\n",
      "\u001b[1mscore\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brad/anaconda3/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n",
      "/Users/brad/anaconda3/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fitness'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/4-final-eda.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;31m## look at certain fora best and worst questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_worst_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fitness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'viewcount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clean_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;31m## save results to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fitness'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.16 s, sys: 189 ms, total: 3.35 s\n",
      "Wall time: 9.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '4-final-eda.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 9303 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "collected = gc.collect()\n",
    "print(f'Garbage collector: collected {collected} objects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:56:09.322665\n",
      "\n",
      "Standard deviations are:\n",
      "\n",
      "Buddhism & 2.1 & 2.27 & 8.1 \\\\\n",
      "Economics & 1.86 & 3.34 & 79.57 \\\\\n",
      "Fitness & 2.18 & 2.1 & -3.67 \\\\\n",
      "Health & 2.11 & 2.01 & -4.74 \\\\\n",
      "Interpersonal & 22.37 & 24.96 & 11.58 \\\\\n",
      "CPU times: user 83.3 ms, sys: 25.5 ms, total: 109 ms\n",
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '6a-rand-train-test-split-50.py'\n",
    "#%run -i '6b-time-train-test-split-50.py'\n",
    "\n",
    "## check standard deviations of variables\n",
    "print('\\nStandard deviations are:\\n')\n",
    "for i in data_array:\n",
    "    tr = round( pd.to_numeric(train[i].describe('score').select('score').toPandas().iloc[2][0]), 2 )\n",
    "    te = round( pd.to_numeric(test[i].describe('score').select('score').toPandas().iloc[2][0]), 2 )\n",
    "    perc = round( (te/tr - 1)*100, 2 )\n",
    "    s = i.title() + ' & ' + str(tr) + ' & ' + str(te) + ' & ' + str(perc) + ' \\\\\\\\'\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 3337 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "collected = gc.collect()\n",
    "print(f'Garbage collector: collected {collected} objects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninteresting to see how skewed rus_stackoverflow posts are to more posts in recent years\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "interesting to see how skewed rus_stackoverflow posts are to more posts in recent years\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Results Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buddhism\n",
      "2016-04-11 21:50:32.343000\n",
      "2019-06-01 21:26:29.840000\n",
      "\n",
      "economics\n",
      "2017-08-25 14:46:57.953000\n",
      "2019-06-02 00:15:49.433000\n",
      "\n",
      "fitness\n",
      "2015-08-14 00:55:15.963000\n",
      "2019-06-01 17:51:10.030000\n",
      "\n",
      "health\n",
      "2016-10-29 22:01:11.947000\n",
      "2019-06-02 02:25:10.917000\n",
      "\n",
      "interpersonal\n",
      "2017-06-27 17:30:02.927000\n",
      "2019-06-02 02:35:11.207000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in data_array:\n",
    "    print(i)\n",
    "    print(datasets[i].sort(col('clean_date')).select('clean_date').take(1)[0][0])\n",
    "    print(f\"{datasets[i].sort(col('clean_date').desc()).select('clean_date').take(1)[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silly Mean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m mean\u001b[0m model is 2.27\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m mean\u001b[0m model is 3.34\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m mean\u001b[0m model is 2.1\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m mean\u001b[0m model is 2.01\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m mean\u001b[0m model is 24.96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0silly_mean.0tr_rmse</th>\n",
       "      <th>0silly_mean.1rmse</th>\n",
       "      <th>0silly_mean.2timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>2.10</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.86</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>22.36</td>\n",
       "      <td>24.96</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0silly_mean.0tr_rmse  0silly_mean.1rmse  0silly_mean.2timet\n",
       "Buddhism                       2.10               2.27                0.50\n",
       "Economics                      1.86               3.34                0.58\n",
       "Fitness                        2.18               2.10                0.52\n",
       "Health                         2.11               2.01                0.49\n",
       "Interpersonal                 22.36              24.96                0.58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  0silly\\_mean.0tr\\_rmse &  0silly\\_mean.1rmse &  0silly\\_mean.2timet \\\\\n",
      "\\midrule\n",
      "Buddhism      &                  2.10 &               2.27 &                0.50 \\\\\n",
      "Economics     &                  1.86 &               3.34 &                0.58 \\\\\n",
      "Fitness       &                  2.18 &               2.10 &                0.52 \\\\\n",
      "Health        &                  2.11 &               2.01 &                0.49 \\\\\n",
      "Interpersonal &                 22.36 &              24.96 &                0.58 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "## choose target variable\n",
    "target = 'score'\n",
    "\n",
    "## create mean dictionaries\n",
    "y_ravi_tr_means = {}\n",
    "\n",
    "## calculate the mean of each forum, using ONLY training set\n",
    "for i in data_array:\n",
    "    y_ravi_tr_means[i] = train[i].select(target).rdd.flatMap(lambda x: x).mean()\n",
    "\n",
    "## import rmse evaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "## create dictionaries for training and testing (baseline) rmse \n",
    "base = {}\n",
    "tr_rmse = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "\n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## train silly mean model by assigning training set mean for training and testing predictions\n",
    "    train[i] = train[i].withColumn('mean_pred', F.lit(y_ravi_tr_means[i]))\n",
    "    test[i] = test[i].withColumn('mean_pred', F.lit(y_ravi_tr_means[i]))\n",
    "\n",
    "    ## evaluate silly mean model, on both training and testing set\n",
    "    evaluator = RegressionEvaluator(metricName='rmse', labelCol=target, predictionCol='mean_pred')\n",
    "    tr_rmse[i] = round( evaluator.evaluate(train[i]), 2)\n",
    "    base[i] = round( evaluator.evaluate(test[i]), 2)\n",
    "\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m mean\\033[0m model is {base[i]}\")\n",
    "\n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 2 )\n",
    "    \n",
    "    ## store as dictionary inside RESULTS dictionary, initiating dataset name entries first\n",
    "    RESULTS[i.title()]['0silly_mean.0tr_rmse'] = tr_rmse[i]\n",
    "    RESULTS[i.title()]['0silly_mean.1rmse'] = base[i]\n",
    "    RESULTS[i.title()]['0silly_mean.2timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewcount Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:54:31.698853\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m viewcount\u001b[0m model is 2.03\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m viewcount\u001b[0m model is 2.55\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m viewcount\u001b[0m model is 2.13\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m viewcount\u001b[0m model is 1.98\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m viewcount\u001b[0m model is 12.72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1viewcount.0tr_rmse</th>\n",
       "      <th>1viewcount.1rmse</th>\n",
       "      <th>1viewcount.2imprv</th>\n",
       "      <th>1viewcount.3timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>1.95</td>\n",
       "      <td>2.03</td>\n",
       "      <td>10.57</td>\n",
       "      <td>7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.70</td>\n",
       "      <td>2.55</td>\n",
       "      <td>23.65</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>2.06</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>2.06</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.49</td>\n",
       "      <td>5.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>10.98</td>\n",
       "      <td>12.72</td>\n",
       "      <td>49.04</td>\n",
       "      <td>5.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1viewcount.0tr_rmse  1viewcount.1rmse  1viewcount.2imprv  \\\n",
       "Buddhism                      1.95              2.03              10.57   \n",
       "Economics                     1.70              2.55              23.65   \n",
       "Fitness                       2.06              2.13              -1.43   \n",
       "Health                        2.06              1.98               1.49   \n",
       "Interpersonal                10.98             12.72              49.04   \n",
       "\n",
       "               1viewcount.3timet  \n",
       "Buddhism                    7.07  \n",
       "Economics                   4.36  \n",
       "Fitness                     5.70  \n",
       "Health                      5.61  \n",
       "Interpersonal               5.94  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  1viewcount.0tr\\_rmse &  1viewcount.1rmse &  1viewcount.2imprv &  1viewcount.3timet \\\\\n",
      "\\midrule\n",
      "Buddhism      &                 1.95 &              2.03 &              10.57 &               7.07 \\\\\n",
      "Economics     &                 1.70 &              2.55 &              23.65 &               4.36 \\\\\n",
      "Fitness       &                 2.06 &              2.13 &              -1.43 &               5.70 \\\\\n",
      "Health        &                 2.06 &              1.98 &               1.49 &               5.61 \\\\\n",
      "Interpersonal &                10.98 &             12.72 &              49.04 &               5.94 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 1.34 s, sys: 352 ms, total: 1.7 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'score'\n",
    "numic_variables = ['viewcount']\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "## numerical columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "'''## date columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')'''\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression on just viewcount\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100, # this doesn't change anything\n",
    "                      #regParam=0.3, # using regularisation parameter here useless since there is one feature\n",
    "                      #elasticNetParam=0.8,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol='viewcount_pred')\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .build()\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol=target, predictionCol='viewcount_pred')\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    \n",
    "    ## fitting on train and predicting on train/test\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "        \n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m viewcount\\033[0m model is {rmse}\")\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['1viewcount.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['1viewcount.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['1viewcount.2imprv'] = impr\n",
    "    RESULTS[i.title()]['1viewcount.3timet'] = timet\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInteresting that there are different improvements of viewcount over mean-only prediciton\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Interesting that there are different improvements of viewcount over mean-only prediciton\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 1410 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "collected = gc.collect()\n",
    "print(f'Garbage collector: collected {collected} objects.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:55:00.667482\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m counts\u001b[0m model is 2.25\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m counts\u001b[0m model is 3.33\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m counts\u001b[0m model is 2.09\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m counts\u001b[0m model is 2.01\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m counts\u001b[0m model is 24.88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2counts.0tr_rmse</th>\n",
       "      <th>2counts.1rmse</th>\n",
       "      <th>2counts.2imprv</th>\n",
       "      <th>2counts.3timet</th>\n",
       "      <th>2counts.4elastic</th>\n",
       "      <th>2counts.5regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>2.08</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.88</td>\n",
       "      <td>14.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.86</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>2.17</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>2.09</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>7.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>22.31</td>\n",
       "      <td>24.88</td>\n",
       "      <td>0.32</td>\n",
       "      <td>14.51</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               2counts.0tr_rmse  2counts.1rmse  2counts.2imprv  \\\n",
       "Buddhism                   2.08           2.25            0.88   \n",
       "Economics                  1.86           3.33            0.30   \n",
       "Fitness                    2.17           2.09            0.48   \n",
       "Health                     2.09           2.01           -0.00   \n",
       "Interpersonal             22.31          24.88            0.32   \n",
       "\n",
       "               2counts.3timet  2counts.4elastic  2counts.5regular  \n",
       "Buddhism                14.04              0.01              0.01  \n",
       "Economics               10.34              0.01              1.00  \n",
       "Fitness                  7.60              0.01              1.00  \n",
       "Health                   7.93              1.00              0.01  \n",
       "Interpersonal           14.51              1.00              1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} &  2counts.0tr\\_rmse &  2counts.1rmse &  2counts.2imprv &  2counts.3timet &  2counts.4elastic &  2counts.5regular \\\\\n",
      "\\midrule\n",
      "Buddhism      &              2.08 &           2.25 &            0.88 &           14.04 &              0.01 &              0.01 \\\\\n",
      "Economics     &              1.86 &           3.33 &            0.30 &           10.34 &              0.01 &              1.00 \\\\\n",
      "Fitness       &              2.17 &           2.09 &            0.48 &            7.60 &              0.01 &              1.00 \\\\\n",
      "Health        &              2.09 &           2.01 &           -0.00 &            7.93 &              1.00 &              0.01 \\\\\n",
      "Interpersonal &             22.31 &          24.88 &            0.32 &           14.51 &              1.00 &              1.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 4.82 s, sys: 1.33 s, total: 6.15 s\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'score'\n",
    "numic_variables = ['body_word_cnt', 'titl_word_cnt', 'body_char_cnt', \n",
    "                   'titl_char_cnt', 'body_sent_cnt', 'titl_sent_cnt']\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "'''## date columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')'''\n",
    "\n",
    "## NUMERICAL columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## create PROCESSING pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol='counts_pred')\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "'''NEEDED, BUT IMMENSELY SLOWING DOWN'''\n",
    "# Ravi et al use L2, aka ridge, aka elasticNetParam=0\n",
    "# regParam is the value of lambda\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-2, 1.]) \\\n",
    "    .addGrid(lr.regParam, [1e-2, 1.]) \\\n",
    "    .build()\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol=target, predictionCol='counts_pred')\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "'''DEFINITELY SLOWING DOWN'''\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "## create models dict\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m counts\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2counts.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['2counts.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['2counts.2imprv'] = impr\n",
    "    RESULTS[i.title()]['2counts.3timet'] = timet\n",
    "    RESULTS[i.title()]['2counts.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['2counts.5regular'] = reg_param\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:00:48.911783\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m tokens\u001b[0m model is 2.27\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m tokens\u001b[0m model is 3.34\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m tokens\u001b[0m model is 2.1\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m tokens\u001b[0m model is 2.01\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m tokens\u001b[0m model is 25.87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2tokens.0tr_rmse</th>\n",
       "      <th>2tokens.1rmse</th>\n",
       "      <th>2tokens.2imprv</th>\n",
       "      <th>2tokens.3timet</th>\n",
       "      <th>2tokens.4elastic</th>\n",
       "      <th>2tokens.5regular</th>\n",
       "      <th>3final.0tr_rmse</th>\n",
       "      <th>3final.1rmse</th>\n",
       "      <th>3final.2imprv</th>\n",
       "      <th>3final.3timet</th>\n",
       "      <th>3final.4elastic</th>\n",
       "      <th>3final.5regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>2.10</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>221.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1.32</td>\n",
       "      <td>11.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.86</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>201.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>190.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>8.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>186.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>15.06</td>\n",
       "      <td>25.87</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>358.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.26</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.44</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               2tokens.0tr_rmse  2tokens.1rmse  2tokens.2imprv  \\\n",
       "Buddhism                   2.10           2.27           -0.00   \n",
       "Economics                  1.86           3.34           -0.00   \n",
       "Fitness                    2.18           2.10           -0.00   \n",
       "Health                     2.11           2.01           -0.00   \n",
       "Interpersonal             15.06          25.87           -3.65   \n",
       "\n",
       "               2tokens.3timet  2tokens.4elastic  2tokens.5regular  \\\n",
       "Buddhism               221.30               1.0               1.0   \n",
       "Economics              201.97               1.0               1.0   \n",
       "Fitness                190.30               1.0               1.0   \n",
       "Health                 186.69               1.0               1.0   \n",
       "Interpersonal          358.41               1.0               1.0   \n",
       "\n",
       "               3final.0tr_rmse  3final.1rmse  3final.2imprv  3final.3timet  \\\n",
       "Buddhism                  2.04          2.24           1.32          11.24   \n",
       "Economics                 1.86          3.34          -0.00           8.96   \n",
       "Fitness                   2.18          2.10          -0.00           8.26   \n",
       "Health                    2.08          2.00           0.50           8.27   \n",
       "Interpersonal            22.26         24.87           0.36           8.44   \n",
       "\n",
       "               3final.4elastic  3final.5regular  \n",
       "Buddhism                  0.01              1.0  \n",
       "Economics                 1.00              1.0  \n",
       "Fitness                   1.00              1.0  \n",
       "Health                    0.01              1.0  \n",
       "Interpersonal             1.00              1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &  2tokens.0tr\\_rmse &  2tokens.1rmse &  2tokens.2imprv &  2tokens.3timet &  2tokens.4elastic &  2tokens.5regular &  3final.0tr\\_rmse &  3final.1rmse &  3final.2imprv &  3final.3timet &  3final.4elastic &  3final.5regular \\\\\n",
      "\\midrule\n",
      "Buddhism      &              2.10 &           2.27 &           -0.00 &          221.30 &               1.0 &               1.0 &             2.04 &          2.24 &           1.32 &          11.24 &             0.01 &              1.0 \\\\\n",
      "Economics     &              1.86 &           3.34 &           -0.00 &          201.97 &               1.0 &               1.0 &             1.86 &          3.34 &          -0.00 &           8.96 &             1.00 &              1.0 \\\\\n",
      "Fitness       &              2.18 &           2.10 &           -0.00 &          190.30 &               1.0 &               1.0 &             2.18 &          2.10 &          -0.00 &           8.26 &             1.00 &              1.0 \\\\\n",
      "Health        &              2.11 &           2.01 &           -0.00 &          186.69 &               1.0 &               1.0 &             2.08 &          2.00 &           0.50 &           8.27 &             0.01 &              1.0 \\\\\n",
      "Interpersonal &             15.06 &          25.87 &           -3.65 &          358.41 &               1.0 &               1.0 &            22.26 &         24.87 &           0.36 &           8.44 &             1.00 &              1.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 9.28 s, sys: 2.53 s, total: 11.8 s\n",
      "Wall time: 19min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "# SMALL DATASETS\n",
    "# 18min 32s for 2-CV and GRIDSEARCH\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'score'\n",
    "textt_variables = ['title', 'clean_body']\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "'''## date columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')'''\n",
    "\n",
    "## textual columns\n",
    "# tokenising text cols with custom transformer\n",
    "nltk_tokeniser_body = nltkWordPunctTokeniser(\n",
    "    inputCol='clean_body', outputCol='body_words',  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "nltk_tokeniser_title = nltkWordPunctTokeniser(\n",
    "    inputCol='title', outputCol='titl_words',  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# count occurence of tokens, i.e. create dfm\n",
    "cnt_vectrizr_body = CountVectorizer(inputCol='body_words', outputCol='body_raw_feats') #!!! minDF???\n",
    "cnt_vectrizr_title = CountVectorizer(inputCol='titl_words', outputCol='titl_raw_feats')\n",
    "\n",
    "# create IDF dfm\n",
    "idf_body = IDF(inputCol=\"body_raw_feats\", outputCol=\"body_feats\")\n",
    "idf_title = IDF(inputCol=\"titl_raw_feats\", outputCol=\"titl_feats\")\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['body_feats', 'titl_feats'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[  #inputCols=['datet_data']\n",
    "    nltk_tokeniser_body, \n",
    "    nltk_tokeniser_title,\n",
    "    cnt_vectrizr_body,\n",
    "    cnt_vectrizr_title,\n",
    "    idf_body,\n",
    "    idf_title,\n",
    "    process_assembler\n",
    "])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol='tokens_pred')\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "'''NEEDED, BUT IMMENSELY SLOWING DOWN'''\n",
    "# Ravi et al use L2, aka ridge, aka elasticNetParam=0\n",
    "# regParam is the value of lambda\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-2, 1.]) \\\n",
    "    .addGrid(lr.regParam, [1e-2, 1.]) \\\n",
    "    .build()\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol=target, predictionCol='tokens_pred')\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "'''DEFINITELY SLOWING DOWN'''\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "## create models dict\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m tokens\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2tokens.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['2tokens.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['2tokens.2imprv'] = impr\n",
    "    RESULTS[i.title()]['2tokens.3timet'] = timet\n",
    "    RESULTS[i.title()]['2tokens.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['2tokens.5regular'] = reg_param\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keys\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`tokens_pred`' given input columns: [btd[0], index, btd[5], std[4], clean_date, btd[7], btd[3], ttd[3], ttd[2], ttd[1], ttd[0], btd[2], std[2], numic_data_std, std[1], std[5], ttd[8], ttd[5], ttd[7], ttd[9], ttd[6], btd[8], title, body_sent_cnt, btd[9], btd[4], std[7], mean_pred, btd[6], titl_sent_cnt, numic_data, std[3], std[8], score, std[0], titl_word_cnt, ttd[4], body_word_cnt, clean_body, std[6], titl_char_cnt, std[9], btd[1], body_char_cnt, counts_pred, features, viewcount];;\\n'Project ['tokens_pred]\\n+- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 23 more fields]\\n   +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 22 more fields]\\n      +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 21 more fields]\\n         +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 20 more fields]\\n            +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n               +- Sample 0.5, 1.0, false, 1777\\n                  +- Sort [title#246 ASC NULLS FIRST, viewcount#247L ASC NULLS FIRST, score#248L ASC NULLS FIRST, clean_date#249 ASC NULLS FIRST, clean_body#250 ASC NULLS FIRST, btd[0]#251 ASC NULLS FIRST, btd[1]#252 ASC NULLS FIRST, btd[2]#253 ASC NULLS FIRST, btd[3]#254 ASC NULLS FIRST, btd[4]#255 ASC NULLS FIRST, btd[5]#256 ASC NULLS FIRST, btd[6]#257 ASC NULLS FIRST, btd[7]#258 ASC NULLS FIRST, btd[8]#259 ASC NULLS FIRST, btd[9]#260 ASC NULLS FIRST, std[0]#261 ASC NULLS FIRST, std[1]#262 ASC NULLS FIRST, std[2]#263 ASC NULLS FIRST, std[3]#264 ASC NULLS FIRST, std[4]#265 ASC NULLS FIRST, std[5]#266 ASC NULLS FIRST, std[6]#267 ASC NULLS FIRST, std[7]#268 ASC NULLS FIRST, std[8]#269 ASC NULLS FIRST, ... 18 more fields], false\\n                     +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                        +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                           +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#102035], [clean_date#249 ASC NULLS FIRST]\\n                              +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                 +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                                    +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                                       +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#4754], [clean_date#249 ASC NULLS FIRST]\\n                                          +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                             +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                                                +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                                                   +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#3383], [clean_date#249 ASC NULLS FIRST]\\n                                                      +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                                         +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                                                            +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                                                               +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#2016], [clean_date#249 ASC NULLS FIRST]\\n                                                                  +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                                                     +- Relation[title#246,viewcount#247L,score#248L,clean_date#249,clean_body#250,btd[0]#251,btd[1]#252,btd[2]#253,btd[3]#254,btd[4]#255,btd[5]#256,btd[6]#257,btd[7]#258,btd[8]#259,btd[9]#260,std[0]#261,std[1]#262,std[2]#263,std[3]#264,std[4]#265,std[5]#266,std[6]#267,std[7]#268,std[8]#269,... 17 more fields] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o49934.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`tokens_pred`' given input columns: [btd[0], index, btd[5], std[4], clean_date, btd[7], btd[3], ttd[3], ttd[2], ttd[1], ttd[0], btd[2], std[2], numic_data_std, std[1], std[5], ttd[8], ttd[5], ttd[7], ttd[9], ttd[6], btd[8], title, body_sent_cnt, btd[9], btd[4], std[7], mean_pred, btd[6], titl_sent_cnt, numic_data, std[3], std[8], score, std[0], titl_word_cnt, ttd[4], body_word_cnt, clean_body, std[6], titl_char_cnt, std[9], btd[1], body_char_cnt, counts_pred, features, viewcount];;\n'Project ['tokens_pred]\n+- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 23 more fields]\n   +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 22 more fields]\n      +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 21 more fields]\n         +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 20 more fields]\n            +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\n               +- Sample 0.5, 1.0, false, 1777\n                  +- Sort [title#246 ASC NULLS FIRST, viewcount#247L ASC NULLS FIRST, score#248L ASC NULLS FIRST, clean_date#249 ASC NULLS FIRST, clean_body#250 ASC NULLS FIRST, btd[0]#251 ASC NULLS FIRST, btd[1]#252 ASC NULLS FIRST, btd[2]#253 ASC NULLS FIRST, btd[3]#254 ASC NULLS FIRST, btd[4]#255 ASC NULLS FIRST, btd[5]#256 ASC NULLS FIRST, btd[6]#257 ASC NULLS FIRST, btd[7]#258 ASC NULLS FIRST, btd[8]#259 ASC NULLS FIRST, btd[9]#260 ASC NULLS FIRST, std[0]#261 ASC NULLS FIRST, std[1]#262 ASC NULLS FIRST, std[2]#263 ASC NULLS FIRST, std[3]#264 ASC NULLS FIRST, std[4]#265 ASC NULLS FIRST, std[5]#266 ASC NULLS FIRST, std[6]#267 ASC NULLS FIRST, std[7]#268 ASC NULLS FIRST, std[8]#269 ASC NULLS FIRST, ... 18 more fields], false\n                     +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\n                        +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\n                           +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#102035], [clean_date#249 ASC NULLS FIRST]\n                              +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\n                                 +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\n                                    +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\n                                       +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#4754], [clean_date#249 ASC NULLS FIRST]\n                                          +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\n                                             +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\n                                                +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\n                                                   +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#3383], [clean_date#249 ASC NULLS FIRST]\n                                                      +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\n                                                         +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\n                                                            +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\n                                                               +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#2016], [clean_date#249 ASC NULLS FIRST]\n                                                                  +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\n                                                                     +- Relation[title#246,viewcount#247L,score#248L,clean_date#249,clean_body#250,btd[0]#251,btd[1]#252,btd[2]#253,btd[3]#254,btd[4]#255,btd[5]#256,btd[6]#257,btd[7]#258,btd[8]#259,btd[9]#260,std[0]#261,std[1]#262,std[2]#263,std[3]#264,std[4]#265,std[5]#266,std[6]#267,std[7]#268,std[8]#269,... 17 more fields] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/6a-rand-train-test-split-50.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## check predictions aren't constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'health'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'health'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`tokens_pred`' given input columns: [btd[0], index, btd[5], std[4], clean_date, btd[7], btd[3], ttd[3], ttd[2], ttd[1], ttd[0], btd[2], std[2], numic_data_std, std[1], std[5], ttd[8], ttd[5], ttd[7], ttd[9], ttd[6], btd[8], title, body_sent_cnt, btd[9], btd[4], std[7], mean_pred, btd[6], titl_sent_cnt, numic_data, std[3], std[8], score, std[0], titl_word_cnt, ttd[4], body_word_cnt, clean_body, std[6], titl_char_cnt, std[9], btd[1], body_char_cnt, counts_pred, features, viewcount];;\\n'Project ['tokens_pred]\\n+- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 23 more fields]\\n   +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 22 more fields]\\n      +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 21 more fields]\\n         +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 20 more fields]\\n            +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n               +- Sample 0.5, 1.0, false, 1777\\n                  +- Sort [title#246 ASC NULLS FIRST, viewcount#247L ASC NULLS FIRST, score#248L ASC NULLS FIRST, clean_date#249 ASC NULLS FIRST, clean_body#250 ASC NULLS FIRST, btd[0]#251 ASC NULLS FIRST, btd[1]#252 ASC NULLS FIRST, btd[2]#253 ASC NULLS FIRST, btd[3]#254 ASC NULLS FIRST, btd[4]#255 ASC NULLS FIRST, btd[5]#256 ASC NULLS FIRST, btd[6]#257 ASC NULLS FIRST, btd[7]#258 ASC NULLS FIRST, btd[8]#259 ASC NULLS FIRST, btd[9]#260 ASC NULLS FIRST, std[0]#261 ASC NULLS FIRST, std[1]#262 ASC NULLS FIRST, std[2]#263 ASC NULLS FIRST, std[3]#264 ASC NULLS FIRST, std[4]#265 ASC NULLS FIRST, std[5]#266 ASC NULLS FIRST, std[6]#267 ASC NULLS FIRST, std[7]#268 ASC NULLS FIRST, std[8]#269 ASC NULLS FIRST, ... 18 more fields], false\\n                     +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                        +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                           +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#102035], [clean_date#249 ASC NULLS FIRST]\\n                              +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                 +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                                    +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                                       +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#4754], [clean_date#249 ASC NULLS FIRST]\\n                                          +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                             +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                                                +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                                                   +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#3383], [clean_date#249 ASC NULLS FIRST]\\n                                                      +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                                         +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 18 more fields]\\n                                                            +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 19 more fields]\\n                                                               +- Window [row_number() windowspecdefinition(clean_date#249 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#2016], [clean_date#249 ASC NULLS FIRST]\\n                                                                  +- Project [title#246, viewcount#247L, score#248L, clean_date#249, clean_body#250, btd[0]#251, btd[1]#252, btd[2]#253, btd[3]#254, btd[4]#255, btd[5]#256, btd[6]#257, btd[7]#258, btd[8]#259, btd[9]#260, std[0]#261, std[1]#262, std[2]#263, std[3]#264, std[4]#265, std[5]#266, std[6]#267, std[7]#268, std[8]#269, ... 17 more fields]\\n                                                                     +- Relation[title#246,viewcount#247L,score#248L,clean_date#249,clean_body#250,btd[0]#251,btd[1]#252,btd[2]#253,btd[3]#254,btd[4]#255,btd[5]#256,btd[6]#257,btd[7]#258,btd[8]#259,btd[9]#260,std[0]#261,std[1]#262,std[2]#263,std[3]#264,std[4]#265,std[5]#266,std[6]#267,std[7]#268,std[8]#269,... 17 more fields] parquet\\n\""
     ]
    }
   ],
   "source": [
    "## check predictions aren't constant\n",
    "models['health'].transform(test['health']).select('tokens_pred').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"why the heck does everything besides interpersonal have constant predictions - it's not the parameters or the size of the data\"\"\"\n",
    "\"\"\"it's the number of questions in the data\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:56:37.768309\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m topics\u001b[0m model is 2.25\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m topics\u001b[0m model is 3.34\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m topics\u001b[0m model is 2.1\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m topics\u001b[0m model is 2.01\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m topics\u001b[0m model is 24.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2topics.0tr_rmse</th>\n",
       "      <th>2topics.1rmse</th>\n",
       "      <th>2topics.2imprv</th>\n",
       "      <th>2topics.3timet</th>\n",
       "      <th>2topics.4elastic</th>\n",
       "      <th>2topics.5regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>2.05</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.88</td>\n",
       "      <td>11.36</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.86</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>9.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>12.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>14.49</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>22.31</td>\n",
       "      <td>24.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>11.47</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               2topics.0tr_rmse  2topics.1rmse  2topics.2imprv  \\\n",
       "Buddhism                   2.05           2.25            0.88   \n",
       "Economics                  1.86           3.34           -0.00   \n",
       "Fitness                    2.18           2.10           -0.00   \n",
       "Health                     2.11           2.01           -0.00   \n",
       "Interpersonal             22.31          24.95            0.04   \n",
       "\n",
       "               2topics.3timet  2topics.4elastic  2topics.5regular  \n",
       "Buddhism                11.36              0.01               1.0  \n",
       "Economics                9.98              1.00               1.0  \n",
       "Fitness                 12.33              1.00               1.0  \n",
       "Health                  14.49              1.00               1.0  \n",
       "Interpersonal           11.47              1.00               1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} &  2topics.0tr\\_rmse &  2topics.1rmse &  2topics.2imprv &  2topics.3timet &  2topics.4elastic &  2topics.5regular \\\\\n",
      "\\midrule\n",
      "Buddhism      &              2.05 &           2.25 &            0.88 &           11.36 &              0.01 &               1.0 \\\\\n",
      "Economics     &              1.86 &           3.34 &           -0.00 &            9.98 &              1.00 &               1.0 \\\\\n",
      "Fitness       &              2.18 &           2.10 &           -0.00 &           12.33 &              1.00 &               1.0 \\\\\n",
      "Health        &              2.11 &           2.01 &           -0.00 &           14.49 &              1.00 &               1.0 \\\\\n",
      "Interpersonal &             22.31 &          24.95 &            0.04 &           11.47 &              1.00 &               1.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 5.88 s, sys: 1.5 s, total: 7.38 s\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'score'\n",
    "numic_variables = ['btd[0]', 'btd[1]', 'btd[2]', 'btd[3]', 'btd[4]', \n",
    "                   'btd[5]', 'btd[6]', 'btd[7]', 'btd[8]', 'btd[9]', \n",
    "                   'std[0]', 'std[1]', 'std[2]', 'std[3]', 'std[4]', \n",
    "                   'std[5]', 'std[6]', 'std[7]', 'std[8]', 'std[9]',\n",
    "                   'ttd[0]', 'ttd[1]', 'ttd[2]', 'ttd[3]', 'ttd[4]', \n",
    "                   'ttd[5]', 'ttd[6]', 'ttd[7]', 'ttd[8]', 'ttd[9]'] \n",
    "\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "'''## date columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')'''\n",
    "\n",
    "## numerical columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "## create processing pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[numic_pipeline, process_assembler])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol='topics_pred')\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "'''NEEDED, BUT IMMENSELY SLOWING DOWN'''\n",
    "# Ravi et al use L2, aka ridge, aka elasticNetParam=0\n",
    "# regParam is the value of lambda\n",
    "    \n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-2, 1.]) \\\n",
    "    .addGrid(lr.regParam, [1e-2, 1.]) \\\n",
    "    .build()\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol=target, predictionCol='topics_pred')\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "'''DEFINITELY SLOWING DOWN'''\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "## create models dict\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m topics\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['2topics.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['2topics.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['2topics.2imprv'] = impr\n",
    "    RESULTS[i.title()]['2topics.3timet'] = timet\n",
    "    RESULTS[i.title()]['2topics.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['2topics.5regular'] = reg_param\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "for i in data_array:\n",
    "    # capitalise keysa\n",
    "    RESULTS[i.title()] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:57:37.517277\n",
      "The root-mean-square error of \u001b[94mbuddhism's\u001b[0m\u001b[92m final\u001b[0m model is 2.24\n",
      "The root-mean-square error of \u001b[94meconomics's\u001b[0m\u001b[92m final\u001b[0m model is 3.34\n",
      "The root-mean-square error of \u001b[94mfitness's\u001b[0m\u001b[92m final\u001b[0m model is 2.1\n",
      "The root-mean-square error of \u001b[94mhealth's\u001b[0m\u001b[92m final\u001b[0m model is 2.0\n",
      "The root-mean-square error of \u001b[94minterpersonal's\u001b[0m\u001b[92m final\u001b[0m model is 24.87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3final.0tr_rmse</th>\n",
       "      <th>3final.1rmse</th>\n",
       "      <th>3final.2imprv</th>\n",
       "      <th>3final.3timet</th>\n",
       "      <th>3final.4elastic</th>\n",
       "      <th>3final.5regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Buddhism</th>\n",
       "      <td>2.04</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1.32</td>\n",
       "      <td>11.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.86</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>8.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>2.08</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpersonal</th>\n",
       "      <td>22.26</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.44</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               3final.0tr_rmse  3final.1rmse  3final.2imprv  3final.3timet  \\\n",
       "Buddhism                  2.04          2.24           1.32          11.24   \n",
       "Economics                 1.86          3.34          -0.00           8.96   \n",
       "Fitness                   2.18          2.10          -0.00           8.26   \n",
       "Health                    2.08          2.00           0.50           8.27   \n",
       "Interpersonal            22.26         24.87           0.36           8.44   \n",
       "\n",
       "               3final.4elastic  3final.5regular  \n",
       "Buddhism                  0.01              1.0  \n",
       "Economics                 1.00              1.0  \n",
       "Fitness                   1.00              1.0  \n",
       "Health                    0.01              1.0  \n",
       "Interpersonal             1.00              1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} &  3final.0tr\\_rmse &  3final.1rmse &  3final.2imprv &  3final.3timet &  3final.4elastic &  3final.5regular \\\\\n",
      "\\midrule\n",
      "Buddhism      &             2.04 &          2.24 &           1.32 &          11.24 &             0.01 &              1.0 \\\\\n",
      "Economics     &             1.86 &          3.34 &          -0.00 &           8.96 &             1.00 &              1.0 \\\\\n",
      "Fitness       &             2.18 &          2.10 &          -0.00 &           8.26 &             1.00 &              1.0 \\\\\n",
      "Health        &             2.08 &          2.00 &           0.50 &           8.27 &             0.01 &              1.0 \\\\\n",
      "Interpersonal &            22.26 &         24.87 &           0.36 &           8.44 &             1.00 &              1.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "CPU times: user 4.93 s, sys: 1.36 s, total: 6.29 s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StandardScaler, VectorAssembler, VectorSlicer\n",
    "\n",
    "########################\n",
    "##### CHOOSE FEATS ##### can't get date right\n",
    "########################\n",
    "\n",
    "## define features to predict on\n",
    "target = 'score'\n",
    "numic_variables = ['btd[0]', 'btd[1]', 'btd[2]', 'btd[3]', 'btd[4]', \n",
    "                   'btd[5]', 'btd[6]', 'btd[7]', 'btd[8]', 'btd[9]', \n",
    "                   'std[0]', 'std[1]', 'std[2]', 'std[3]', 'std[4]', \n",
    "                   'std[5]', 'std[6]', 'std[7]', 'std[8]', 'std[9]',\n",
    "                   'ttd[0]', 'ttd[1]', 'ttd[2]', 'ttd[3]', 'ttd[4]', \n",
    "                   'ttd[5]', 'ttd[6]', 'ttd[7]', 'ttd[8]', 'ttd[9]',\n",
    "                   'body_word_cnt', 'titl_word_cnt', 'body_char_cnt', \n",
    "                   'titl_char_cnt', 'body_sent_cnt', 'titl_sent_cnt']\n",
    "datet_variables = ['clean_date']\n",
    "\n",
    "### DATE columns\n",
    "datet_assembler = VectorAssembler(inputCols=datet_variables, outputCol='datet_data')\n",
    "\n",
    "### NUMERICAL columns\n",
    "numic_assembler = VectorAssembler(inputCols=numic_variables, outputCol='numic_data') # have to put in single col\n",
    "standardiser = StandardScaler(inputCol='numic_data', outputCol='numic_data_std')    \n",
    "numic_pipeline = Pipeline(stages=[numic_assembler, standardiser])\n",
    "\n",
    "\n",
    "## create PROCESSING pipeline\n",
    "process_assembler = VectorAssembler(inputCols=['numic_data'], #inputCols=['datet_data']\n",
    "                                    outputCol='features') \n",
    "process_pipeline = Pipeline(stages=[  #inputCols=['datet_data']\n",
    "    numic_pipeline,\n",
    "    process_assembler\n",
    "])\n",
    "\n",
    "########################\n",
    "##### CHOOSE MODEL #####\n",
    "########################\n",
    "\n",
    "## linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    \n",
    "lr = LinearRegression(#maxIter=100,\n",
    "                      #regParam=1,\n",
    "                      #elasticNetParam=1,\n",
    "                      featuresCol='features',\n",
    "                      labelCol=target,\n",
    "                      predictionCol='finalm_pred')\n",
    "\n",
    "## make final pipeline\n",
    "final_pipeline = Pipeline(stages=[process_pipeline, lr])\n",
    "\n",
    "## import methods for tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## set up grid for parameter tuning: \n",
    "'''NEEDED, BUT IMMENSELY SLOWING DOWN'''\n",
    "# Ravi et al use L2, aka ridge, aka elasticNetParam=0\n",
    "# regParam is the value of lambda\n",
    "    \n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-2, 1.]) \\\n",
    "    .addGrid(lr.regParam, [1e-2, 1.]) \\\n",
    "    .build()\n",
    "\n",
    "## set up rmse evaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol=target, predictionCol='finalm_pred')\n",
    "\n",
    "## set up cross validation for parameter tuning\n",
    "'''DEFINITELY SLOWING DOWN'''\n",
    "crossval = CrossValidator(estimator=final_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "## create models dict\n",
    "models = {}\n",
    "\n",
    "## modelling\n",
    "for i in data_array:\n",
    "    \n",
    "    ## initial variable for timing\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## fit on training set with CV\n",
    "    cvmodel = crossval.fit(train[i])\n",
    "    models[i] = cvmodel\n",
    "    \n",
    "    ## predict and evaluate\n",
    "    tr_rmse = round( evaluator.evaluate(cvmodel.transform(train[i])), 2 )\n",
    "    rmse = round( evaluator.evaluate(cvmodel.transform(test[i])), 2 )\n",
    "    print(f\"The root-mean-square error of \\033[94m{i}'s\\033[0m\\033[92m final\\033[0m model is {rmse}\")\n",
    "    \n",
    "    ## get params\n",
    "    # elasticnet\n",
    "    ela_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[1]\n",
    "    ela_param = cvmodel.bestModel.stages[-1].extractParamMap()[ela_key]\n",
    "    # reg'sation\n",
    "    reg_key = list(cvmodel.bestModel.stages[-1].extractParamMap().keys())[9]\n",
    "    reg_param = cvmodel.bestModel.stages[-1].extractParamMap()[reg_key]\n",
    "\n",
    "    ## calculate improvement over median baseline\n",
    "    impr = round( (rmse/base[i] - 1)*-100, 2 )\n",
    "    \n",
    "    ## record time taken\n",
    "    timet = round( time.time() - t0, 2 )\n",
    "\n",
    "    ## store as dictionary inside RESULTS dictionary\n",
    "    RESULTS[i.title()]['3final.0tr_rmse'] = tr_rmse\n",
    "    RESULTS[i.title()]['3final.1rmse'] = rmse\n",
    "    RESULTS[i.title()]['3final.2imprv'] = impr\n",
    "    RESULTS[i.title()]['3final.3timet'] = timet\n",
    "    RESULTS[i.title()]['3final.4elastic'] = ela_param\n",
    "    RESULTS[i.title()]['3final.5regular'] = reg_param\n",
    "    \n",
    "## record results\n",
    "show_save_results(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052),\n",
       " Row(finalm_pred=1.8051282051282052)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check predictions aren't constant\n",
    "models['fitness'].transform(test['fitness']).select('finalm_pred').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trained_pipeline\n",
    " .transform(datasets['english'])\n",
    " .select(\n",
    "    indep_text_variables + [\"prediction\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"linreg_prediction.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions = spark.read.parquet(\"linreg_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predictions.select(\"prediction\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(estimator_pipeline, 'pipeline.joblib') \n",
    "\n",
    "reloaded = load(\"pipeline.joblib\")\n",
    "\n",
    "#Now we can predict directly!\n",
    "\n",
    "reloaded.predict(X)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## save models DOESN'T WORK BECAUSE: 'NLTKWordPunctTokenizer' object has no attribute '_to_java'\n",
    "for i in data_array:\n",
    "    param_dict[i].save(f'{i}-pipeline') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert notebook to python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script 0-master-notebook-pipelines.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
