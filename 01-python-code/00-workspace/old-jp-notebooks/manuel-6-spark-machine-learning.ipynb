{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.70:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Taxis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb0cbe13470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[6]\") # we can use [X] to limit the number of cores spark uses\n",
    "    .appName(\"Taxis\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replicate the following pipeline in Spark:\n",
    "\n",
    "```\n",
    "pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]),\n",
    "            OneHotEncoder()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]),\n",
    "            StandardScaler()\n",
    "        )  \n",
    "    ),\n",
    "    SGDRegressor()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"tip_amount\"\n",
    "independent_variables = [\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\",\n",
    "               \"rate_code\", \"pickup_longitude\", \"pickup_latitude\", \n",
    "               \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = (\n",
    "    spark\n",
    "    .read\n",
    "    .parquet(\"./data/nyc_taxi_data_2014_small.parquet\")\n",
    "    .select(independent_variables + [target])\n",
    "    .withColumnRenamed(target, \"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.99476999999999, pickup_latitude=40.736828, passenger_count=1, label=1.4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has its own implementation of [Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) more or less similar to sklearns', it has 2 different kinds of objects:\n",
    "\n",
    "- **Transformers**, preprocess the dataset prior to the estimation (encoders, standardizers, etc)\n",
    "- **Estimators**, ML models that perform predictions (Linear Regression, Random Forest, etc.)\n",
    "\n",
    "Spark transformers work by taking **a single column** as an input and producing **a single column** as an output.\n",
    "\n",
    "Same way, Spark estimators work by taking **a single column** as an input (usually named `features`) and producing **a single column** as the predictions(usually named `labels`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler, VectorSlicer\n",
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = [\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = []\n",
    "encoders = []\n",
    "for column in categorical_variables:\n",
    "    indexers.append(StringIndexer(inputCol=column, outputCol=f\"{column}_idx)\", handleInvalid=\"keep\"))\n",
    "    encoders.append(OneHotEncoder(inputCol=f\"{column}_idx)\", outputCol=f\"{column}_enc\"))\n",
    "    \n",
    "categorical_ensembler = VectorAssembler(inputCols=[enc.getOutputCol() for enc in encoders], \n",
    "                                         outputCol=\"categorical_encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = Pipeline(stages=indexers +encoders + [categorical_ensembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_326236731351,\n",
       " StringIndexer_0124264c121b,\n",
       " StringIndexer_236fe3694a45,\n",
       " StringIndexer_900a6fdbd12c,\n",
       " OneHotEncoder_d2e8ab5e667f,\n",
       " OneHotEncoder_45d783d6bfc8,\n",
       " OneHotEncoder_8fc8ae062f2d,\n",
       " OneHotEncoder_5424b982fdf3,\n",
       " VectorAssembler_a99daa68b21c]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_cat = categorical_pipeline.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.99476999999999, pickup_latitude=40.736828, passenger_count=1, label=1.4, vendor_id_idx)=0.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=1.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {0: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {1: 1.0}), rate_code_enc=SparseVector(8, {0: 1.0}), categorical_encoding=SparseVector(17, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0})),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.982392, pickup_latitude=40.773382, passenger_count=1, label=1.9, vendor_id_idx)=0.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=1.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {0: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {1: 1.0}), rate_code_enc=SparseVector(8, {0: 1.0}), categorical_encoding=SparseVector(17, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_cat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables = [\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_ensembler = VectorAssembler(inputCols=numerical_variables, outputCol=\"numerical_data\")    \n",
    "\n",
    "standardizer = StandardScaler(inputCol=\"numerical_data\", outputCol=\"numerical_data_std\")    \n",
    "\n",
    "numerical_pipeline = Pipeline(stages=[numerical_ensembler, standardizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_num = numerical_pipeline.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.99476999999999, pickup_latitude=40.736828, passenger_count=1, label=1.4, numerical_data=DenseVector([-73.9948, 40.7368, 1.0]), numerical_data_std=DenseVector([-7.5283, 7.5237, 1.6172])),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.982392, pickup_latitude=40.773382, passenger_count=1, label=1.9, numerical_data=DenseVector([-73.9824, 40.7734, 1.0]), numerical_data_std=DenseVector([-7.5271, 7.5305, 1.6172]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_num.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_ensembler = VectorAssembler(inputCols=[\"categorical_encoding\", \"numerical_data\"], \n",
    "                                         outputCol=\"features\")  \n",
    "\n",
    "processing_pipeline = Pipeline(stages=[categorical_pipeline, numerical_pipeline, processing_ensembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_processed = processing_pipeline.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.99476999999999, pickup_latitude=40.736828, passenger_count=1, label=1.4, vendor_id_idx)=0.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=1.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {0: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {1: 1.0}), rate_code_enc=SparseVector(8, {0: 1.0}), categorical_encoding=SparseVector(17, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0}), numerical_data=DenseVector([-73.9948, 40.7368, 1.0]), numerical_data_std=DenseVector([-7.5283, 7.5237, 1.6172]), features=SparseVector(20, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0, 17: -73.9948, 18: 40.7368, 19: 1.0})),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.982392, pickup_latitude=40.773382, passenger_count=1, label=1.9, vendor_id_idx)=0.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=1.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {0: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {1: 1.0}), rate_code_enc=SparseVector(8, {0: 1.0}), categorical_encoding=SparseVector(17, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0}), numerical_data=DenseVector([-73.9824, 40.7734, 1.0]), numerical_data_std=DenseVector([-7.5271, 7.5305, 1.6172]), features=SparseVector(20, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0, 17: -73.9824, 18: 40.7734, 19: 1.0}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_processed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.      ,   1.      ,   1.      ,   1.      , -73.99477 ,\n",
       "        40.736828,   1.      ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_processed.head(2)[0].features.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pipeline capable of taking the input dataframe, and producing an output `features` column that will be the input for the estimator.\n",
    "\n",
    "To define a spark estimator, besides the usual model specific hyperparameters, we need to define some additiona arguments:\n",
    "\n",
    "- `featuresCol`, the name of the features column (`features`, by default)\n",
    "- `labelCol`, the name of the target variable column(`labels` by default)\n",
    "- `predictionCol`, the name for the prediction column name (`prediction` by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100,\n",
    "                      regParam=0.3,\n",
    "                      elasticNetParam=0.8,\n",
    "                      featuresCol=\"features\",\n",
    "                      labelCol=\"label\",\n",
    "                      predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[processing_pipeline, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline = pipeline.fit(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_e1a15daa7e84"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.99476999999999, pickup_latitude=40.736828, passenger_count=1, label=1.4, vendor_id_idx)=0.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=1.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {0: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {1: 1.0}), rate_code_enc=SparseVector(8, {0: 1.0}), categorical_encoding=SparseVector(17, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0}), numerical_data=DenseVector([-73.9948, 40.7368, 1.0]), numerical_data_std=DenseVector([-7.5283, 7.5237, 1.6172]), features=SparseVector(20, {0: 1.0, 2: 1.0, 5: 1.0, 9: 1.0, 17: -73.9948, 18: 40.7368, 19: 1.0}), prediction=2.083814845782015)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_pipeline.transform(taxi).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write the predictions back as another parquet file. We do this since we dont know if we could collect all of the dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trained_pipeline\n",
    " .transform(taxi)\n",
    " .select(\n",
    "    categorical_variables + numerical_variables + [\"prediction\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"taxi_2014_prediction.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_predictions = spark.read.parquet(\"taxi_2014_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CSH', rate_code=1, pickup_longitude=-73.978514, pickup_latitude=40.754396, passenger_count=1, prediction=0.1951040649097826),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CSH', rate_code=1, pickup_longitude=-73.789975, pickup_latitude=40.646617, passenger_count=1, prediction=0.1951040649097826)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_predictions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>1.0808321699952819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.9841169109290067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0.1951040649097826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>4.176174270525056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary          prediction\n",
       "0   count             1000000\n",
       "1    mean  1.0808321699952819\n",
       "2  stddev  0.9841169109290067\n",
       "3     min  0.1951040649097826\n",
       "4     max   4.176174270525056"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_predictions.select(\"prediction\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Hyperparameter Search with Spark\n",
    "\n",
    "Spark doesnt have a random search object, however it does have a grid search object,`ParamGridBuilder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the grid builder and add elements to the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [1e-3, 1.]) \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-3, 1.]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to evaluate the grid search in order to decide which hyperparameters are the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crossval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a4bd8c694c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'crossval' is not defined"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(taxi)\n",
    "\n",
    "cvModel.avgMetrics\n",
    "\n",
    "list(zip(cvModel.avgMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the pipeline stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PipelineModel_bf036130d077, LinearRegression_90134039cc6c]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can get the best hyperparameters for the estimator (`extractParamMap()` extracts a transformer/estimator parameters as a dictionary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_90134039cc6c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LinearRegression_90134039cc6c', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LinearRegression_90134039cc6c', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError',\n",
       " Param(parent='LinearRegression_90134039cc6c', name='maxIter', doc='maximum number of iterations (>= 0)'): 100,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LinearRegression_90134039cc6c', name='regParam', doc='regularization parameter (>= 0)'): 0.001,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto',\n",
       " Param(parent='LinearRegression_90134039cc6c', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LinearRegression_90134039cc6c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages[1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
