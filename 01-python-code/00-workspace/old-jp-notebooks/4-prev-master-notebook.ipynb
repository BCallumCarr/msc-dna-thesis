{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOALS FOR TOMORROW:\n",
    "\n",
    "* Build some crappy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"Welcome to my PySpark analysis of some StackExchange Data\"\\n'\n"
     ]
    }
   ],
   "source": [
    "## testing printing output from console\n",
    "import subprocess\n",
    "cmd = [ 'echo', '\"Welcome to my PySpark analysis of some StackExchange Data\"' ]\n",
    "output = subprocess.Popen( cmd, stdout=subprocess.PIPE ).communicate()[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import standard libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spark UI is available at: http://192.168.0.26:4040/ and the defaultParallelism is 4\n"
     ]
    }
   ],
   "source": [
    "%run -i '1-load-pyspark.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:14:28.164011\n",
      "------------------------\n",
      "english\n",
      "------------------------\n",
      "root\n",
      " |-- _Body: string (nullable = true)\n",
      " |-- _Title: string (nullable = true)\n",
      " |-- _ViewCount: long (nullable = true)\n",
      " |-- _Score: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+----------+------+\n",
      "|               _Body|              _Title|_ViewCount|_Score|\n",
      "+--------------------+--------------------+----------+------+\n",
      "|<p>How do I know ...|What is the diffe...|     18413|    35|\n",
      "|<p>When you want ...|Should I use a se...|    106724|    52|\n",
      "|<blockquote>\n",
      "  <p...|What does Maugham...|      1131|    11|\n",
      "+--------------------+--------------------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "------------------------\n",
      "math\n",
      "------------------------\n",
      "root\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "|                Body|               Title|ViewCount|Score|__index_level_0__|\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "|<p>Can someone ex...|What Does it Real...|     7879|  144|                0|\n",
      "|<p><a href=\"http:...|List of interesti...|    63919|  102|                1|\n",
      "|<p>I have read a ...|How can you prove...|    11354|   49|                3|\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "------------------------\n",
      "rus_stackoverflow\n",
      "------------------------\n",
      "root\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "|                Body|               Title|ViewCount|Score|__index_level_0__|\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "|<p>Нужен простейш...|Как из скрипта на...|    10786|   38|                0|\n",
      "|<p>Например, имее...|Как сохранить и в...|     3758|    9|                2|\n",
      "|<p>Какая команда ...|Как найти файл по...|    14026|    7|                4|\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "------------------------\n",
      "stackoverflow\n",
      "------------------------\n",
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- view_count: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+----------+-----+-----------------+\n",
      "|                body|               title|view_count|score|__index_level_0__|\n",
      "+--------------------+--------------------+----------+-----+-----------------+\n",
      "|<p>I'm converting...|Inherit a common ...|      6183|    6|                0|\n",
      "|<p>I have the fol...|Firefox onkeypres...|      5934|    2|                1|\n",
      "|<p>I would like t...|Bundle third part...|        31|    1|                2|\n",
      "+--------------------+--------------------+----------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "------------------------\n",
      "superuser\n",
      "------------------------\n",
      "root\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "|                Body|               Title|ViewCount|Score|__index_level_0__|\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "|<p>A Vista virtua...|Why does the /win...|   106189|  172|                0|\n",
      "|<p>I used to reco...|How can I remove ...|     6343|    6|                1|\n",
      "|<p>What is the di...|What's the differ...|    23892|   23|                5|\n",
      "+--------------------+--------------------+---------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 19.5 ms, sys: 7.93 ms, total: 27.5 ms\n",
      "Wall time: 7.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '2-load-datasets.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_Body</th>\n",
       "      <th>_Title</th>\n",
       "      <th>_ViewCount</th>\n",
       "      <th>_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;How do I know when to use &lt;em&gt;lay&lt;/em&gt; and ...</td>\n",
       "      <td>What is the difference between \"lay\" and \"lie\"?</td>\n",
       "      <td>18413</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;When you want to connect two closely relate...</td>\n",
       "      <td>Should I use a semicolon or a dash to connect ...</td>\n",
       "      <td>106724</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;&lt;strong&gt;Possible Duplicate:...</td>\n",
       "      <td>What does Maugham mean by \"his spaghetti were\"?</td>\n",
       "      <td>1131</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;How do you say it correctly?&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;...</td>\n",
       "      <td>\"Adult and children stories\" or \"Adults and ch...</td>\n",
       "      <td>959</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;\"Proven\" and \"proved\" both seem to mean the...</td>\n",
       "      <td>What is the difference between \"proven\" and \"p...</td>\n",
       "      <td>52711</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               _Body  \\\n",
       "0  <p>How do I know when to use <em>lay</em> and ...   \n",
       "1  <p>When you want to connect two closely relate...   \n",
       "2  <blockquote>\\n  <p><strong>Possible Duplicate:...   \n",
       "3  <p>How do you say it correctly?</p>\\n\\n<ul>\\n<...   \n",
       "4  <p>\"Proven\" and \"proved\" both seem to mean the...   \n",
       "\n",
       "                                              _Title  _ViewCount  _Score  \n",
       "0    What is the difference between \"lay\" and \"lie\"?       18413      35  \n",
       "1  Should I use a semicolon or a dash to connect ...      106724      52  \n",
       "2    What does Maugham mean by \"his spaghetti were\"?        1131      11  \n",
       "3  \"Adult and children stories\" or \"Adults and ch...         959       2  \n",
       "4  What is the difference between \"proven\" and \"p...       52711      50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_spark_df(df, n=5):\n",
    "    '''\n",
    "    function to better print spark df entries\n",
    "    '''\n",
    "    display(pd.DataFrame(df.head(n), columns=df.columns))\n",
    "    \n",
    "show_spark_df(datasets[\"english\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:14:36.000223\n",
      "\n",
      "\u001b[1m checking columns are the right types and names \u001b[0m\n",
      "\n",
      "----- english -----\n",
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "----- math -----\n",
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "----- rus_stackoverflow -----\n",
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "----- stackoverflow -----\n",
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "----- superuser -----\n",
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- viewcount: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- clean_body: string (nullable = true)\n",
      "\n",
      "None\n",
      "\n",
      "\u001b[1m checking that there are no nans \u001b[0m\n",
      "\n",
      "----- english -----\n",
      "+----+-----+---------+-----+----------+\n",
      "|body|title|viewcount|score|clean_body|\n",
      "+----+-----+---------+-----+----------+\n",
      "|   0|    0|        0|    0|         0|\n",
      "+----+-----+---------+-----+----------+\n",
      "\n",
      "----- math -----\n",
      "+----+-----+---------+-----+----------+\n",
      "|body|title|viewcount|score|clean_body|\n",
      "+----+-----+---------+-----+----------+\n",
      "|   0|    0|        0|    0|         0|\n",
      "+----+-----+---------+-----+----------+\n",
      "\n",
      "----- rus_stackoverflow -----\n",
      "+----+-----+---------+-----+----------+\n",
      "|body|title|viewcount|score|clean_body|\n",
      "+----+-----+---------+-----+----------+\n",
      "|   0|    0|        0|    0|         0|\n",
      "+----+-----+---------+-----+----------+\n",
      "\n",
      "----- stackoverflow -----\n",
      "+----+-----+---------+-----+----------+\n",
      "|body|title|viewcount|score|clean_body|\n",
      "+----+-----+---------+-----+----------+\n",
      "|   0|    0|        0|    0|         0|\n",
      "+----+-----+---------+-----+----------+\n",
      "\n",
      "----- superuser -----\n",
      "+----+-----+---------+-----+----------+\n",
      "|body|title|viewcount|score|clean_body|\n",
      "+----+-----+---------+-----+----------+\n",
      "|   0|    0|        0|    0|         0|\n",
      "+----+-----+---------+-----+----------+\n",
      "\n",
      "CPU times: user 110 ms, sys: 28.6 ms, total: 138 ms\n",
      "Wall time: 59.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '3-clean-datasets.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '4-eda.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''NB TO DO: Find threshold to delete low views to make sure users that can vote have seen the question'''\n",
    "\n",
    "vc_thresh_data = {}\n",
    "\n",
    "## finding means of viewcounts across fora\n",
    "\n",
    "for i in data_array:\n",
    "    vc_thresh_data[i] = datasets[i].select(\"viewcount\").rdd.flatMap(lambda x: x).mean()\n",
    "\n",
    "vc_thresh_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ravi Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:15:35.295264\n",
      "\n",
      "The average value of \u001b[1menglish\u001b[0m y_ravi is 0.0037778\n",
      "\n",
      "\n",
      "The average value of \u001b[1mmath\u001b[0m y_ravi is 0.0121854\n",
      "\n",
      "\n",
      "The average value of \u001b[1mrus_stackoverflow\u001b[0m y_ravi is 0.006857\n",
      "\n",
      "\n",
      "The average value of \u001b[1mstackoverflow\u001b[0m y_ravi is 0.002148\n",
      "\n",
      "\n",
      "The average value of \u001b[1msuperuser\u001b[0m y_ravi is 0.0027184\n",
      "\n",
      "CPU times: user 243 ms, sys: 68.5 ms, total: 311 ms\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "%run -i '5-define-target.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "\n",
    "## good/bad threshold validation\n",
    "\n",
    "## choose thresholds\n",
    "threshes = np.linspace(0.05, 0.95, num=37)\n",
    "\n",
    "## create empty list for results\n",
    "result = []\n",
    "\n",
    "## create user-defined function to add threshold columns\n",
    "def thresh_cols(a, high_threshold, low_threshold):\n",
    "    '''function to add threshold columns'''        \n",
    "    if a > high_threshold:\n",
    "        return 'good'\n",
    "    elif a <= low_threshold:\n",
    "        return 'bad'\n",
    "\n",
    "thresh_cols_udf = udf(thresh_cols, StringType())\n",
    "   \n",
    "for i in range(len(threshes)):\n",
    "    ## store thresholds, lowest represents % good questions\n",
    "    lo_thresh = np.quantile(datasets['english'].select('y_ravi').\n",
    "                            rdd.flatMap(lambda x: x).collect(), threshes[i])\n",
    "    hi_thresh = np.quantile(datasets['english'].select('y_ravi').\n",
    "                            rdd.flatMap(lambda x: x).collect(), threshes[len(threshes)-i-1])\n",
    "    \n",
    "    # if statement to change over at 50% threshold\n",
    "    if (i > (len(threshes) - 1) / 2):\n",
    "        ## label questions according to high and low thresholds AFTER 50% THRESHOLD\n",
    "        # here we only need one threshold\n",
    "        datasets['english'] = datasets['english'].withColumn(\"col.\" + str(round(threshes[i], 3)),\n",
    "                                                             thresh_cols_udf('y_ravi', lit(hi_thresh), lit(hi_thresh)))\n",
    "    else:\n",
    "        ## label questions according to high and low thresholds BEFORE 50% THRESHOLD\n",
    "        # here there are two mirrored threshold, where NAs fill the middle\n",
    "        datasets['english'] = datasets['english'].withColumn(\"col.\" + str(round(threshes[i], 3)),\n",
    "                                                             thresh_cols_udf('y_ravi', lit(hi_thresh), lit(lo_thresh)))\n",
    "\n",
    "'''\n",
    "## get tokens and remove whitespace that messes up French and Linguistics\n",
    "    toks_one <- tokens_remove(tokens(paste0(\n",
    "      sxdf_q[sxdf_q[[as.character(threshes[i])]]==\"good\",][[\"Body\"[1]]], collapse = \" \")), \n",
    "                              \"\\\\p{Z}\", valuetype = \"regex\")\n",
    "    toks_two <- tokens_remove(tokens(paste0(\n",
    "      sxdf_q[sxdf_q[[as.character(threshes[i])]]==\"bad\",][[\"Body\"[1]]], collapse = \" \")), \n",
    "                              \"\\\\p{Z}\", valuetype = \"regex\")\n",
    "    ## reassign docnames\n",
    "    docnames(toks_one) <- \"good\"\n",
    "    docnames(toks_two) <- \"bad\"\n",
    "    \n",
    "    ## we normalise the dfm by stemming, removing stopwords/punctuation and weighting proportionally\n",
    "    docfm <- dfm_weight(dfm(c(toks_one, toks_two),\n",
    "                            stem = TRUE, remove_punct = TRUE, remove = stopwords(\"english\")),\n",
    "                        \"prop\")\n",
    "    \n",
    "    ## bind results\n",
    "    result <- rbind(result, c(threshes[i], \n",
    "                              textstat_dist(docfm, method=\"euclidean\"), \n",
    "                              textstat_simil(docfm, method=\"cosine\"), \n",
    "                              textstat_simil(docfm, method=\"jaccard\"), \n",
    "                              datasets[k]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 178 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "import gc\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected %d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MUST STILL MAKE PIPELINE'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "'''MUST STILL MAKE PIPELINE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_variables = [\"title\", \"clean_body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import elements from natural language toolkit\n",
    "import nltk\n",
    "#nltk.download('all') # uncomment after first run as admin check\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def get_tokens(line):\n",
    "    '''\n",
    "    Function to parse text features\n",
    "    '''\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stopwords\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    '''lemmatise or stem???'''\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    # remove single letters\n",
    "    words = [word for word in words if not len(word)==1]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from pyspark import keyword_only  ## < 2.0 -> pyspark.ml.util.keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class NLTKWordPunctTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(NLTKWordPunctTokenizer, self).__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Hi I heard about ...|  [Hi, heard, Spark]|\n",
      "|    0|I wish Java could...|[wish, Java, coul...|\n",
      "|    1|Logistic regressi...|[Logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (0, \"I wish Java could use case classes\"),\n",
    "  (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = NLTKWordPunctTokenizer(\n",
    "    inputCol=\"sentence\", outputCol=\"words\",  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "tokenizer.transform(sentenceDataFrame).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numerical_ensembler = VectorAssembler(inputCols=numerical_variables, outputCol=\"numerical_data\")    \\n\\nstandardizer = StandardScaler(inputCol=\"numerical_data\", outputCol=\"numerical_data_std\")    \\n\\nnumerical_pipeline = Pipeline(stages=[numerical_ensembler, standardizer])\\n\\ntaxi_num = numerical_pipeline.fit(taxi).transform(taxi)\\n\\ntaxi_num.head(2)\\n\\nprocessing_ensembler = VectorAssembler(inputCols=[\"categorical_encoding\", \"numerical_data\"], \\n                                         outputCol=\"features\")  \\n\\nprocessing_pipeline = Pipeline(stages=[categorical_pipeline, numerical_pipeline, processing_ensembler])\\n\\ntaxi_processed = processing_pipeline.fit(taxi).transform(taxi)\\n\\ntaxi_processed.head(2)\\n\\ntaxi_processed.head(2)[0].features.values'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numerical_ensembler = VectorAssembler(inputCols=numerical_variables, outputCol=\"numerical_data\")    \n",
    "\n",
    "standardizer = StandardScaler(inputCol=\"numerical_data\", outputCol=\"numerical_data_std\")    \n",
    "\n",
    "numerical_pipeline = Pipeline(stages=[numerical_ensembler, standardizer])\n",
    "\n",
    "taxi_num = numerical_pipeline.fit(taxi).transform(taxi)\n",
    "\n",
    "taxi_num.head(2)\n",
    "\n",
    "processing_ensembler = VectorAssembler(inputCols=[\"categorical_encoding\", \"numerical_data\"], \n",
    "                                         outputCol=\"features\")  \n",
    "\n",
    "processing_pipeline = Pipeline(stages=[categorical_pipeline, numerical_pipeline, processing_ensembler])\n",
    "\n",
    "taxi_processed = processing_pipeline.fit(taxi).transform(taxi)\n",
    "\n",
    "taxi_processed.head(2)\n",
    "\n",
    "taxi_processed.head(2)[0].features.values'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.55 ms, sys: 3.79 ms, total: 12.3 ms\n",
      "Wall time: 470 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## tokenise and lemmatise using nltk function from import step\n",
    "body_rdd = {}\n",
    "for i in data_array:\n",
    "    body_rdd[i] = datasets[i].select('clean_body').rdd.flatMap(lambda r: r).map(lambda line: (1, get_tokens(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['know', 'use', 'lay', 'use', 'lie', 'different', 'form', 'verb', 'always', 'getting', 'confused']\n",
      "CPU times: user 21.7 ms, sys: 5.86 ms, total: 27.6 ms\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## look at first 10 terms from first post in english\n",
    "print(body_rdd['english'].take(1)[0][1][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''RUN LATER'''\n",
    "\n",
    "# get rid of stop words\n",
    "doc_stopwords = {}\n",
    "for i in data_array:\n",
    "    doc_stopwords[i] = body_rdd[i].flatMap(lambda r: r[1]).map(lambda r: (r,1)).reduceByKey(lambda a,b: a+b)\n",
    "    # here we assume that words that appear very frequently are stop words\n",
    "    doc_stopwords[i] = doc_stopwords[i].filter(lambda a: a[1]>3000).map(lambda r: r[0]).collect()\n",
    "    '''3000 threshold needs attention!'''\n",
    "    # remove stopwords and single letters\n",
    "    body_rdd[i] = body_rdd[i].map(lambda r: (r[0],[w for w in r[1] if not w in doc_stopwords[i] and not len(w)==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 25.4 ms, total: 202 ms\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "## convert tokens into sparse vectors\n",
    "body_df = {}\n",
    "for i in data_array:\n",
    "    body_df[i] = spark.createDataFrame(body_rdd[i], [\"dummy\",\"words\"])\n",
    "    body_df[i].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dummy=1, words=['know', 'use', 'lay', 'use', 'lie', 'different', 'form', 'verb', 'always', 'getting', 'confused'])]\n",
      "CPU times: user 10.4 ms, sys: 6.71 ms, total: 17.1 ms\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## check first english post vector\n",
    "print(body_df['english'].take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 160 objects.\n"
     ]
    }
   ],
   "source": [
    "## garbage collector to speed up computation\n",
    "import gc\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected %d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:18:59.206030\n",
      "\n",
      "------------------\n",
      " english \n",
      "------------------\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|dummy|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|[know, use, lay, ...|(44232,[6,15,32,5...|\n",
      "|    1|[want, connect, t...|(44232,[2,3,6,21,...|\n",
      "|    1|[possible, duplic...|(44232,[2,4,5,8,9...|\n",
      "|    1|[say, correctly, ...|(44232,[8,217,265...|\n",
      "|    1|[proven, proved, ...|(44232,[11,17,33,...|\n",
      "|    1|[reading, audienc...|(44232,[0,1,3,4,5...|\n",
      "|    1|[rule, thumb, pin...|(44232,[3,26,33,5...|\n",
      "|    1|[two, task, calle...|(44232,[1,5,8,14,...|\n",
      "|    1|[understand, term...|(44232,[6,9,23,32...|\n",
      "|    1|[nothing, correct...|(44232,[1,2,10,13...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 20.5 ms, sys: 7.21 ms, total: 27.7 ms\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "'''BOTTLENECK TIME - JUST USE english FOR NOW'''\n",
    "data_array = ['english']\n",
    "\n",
    "cntvcr = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\n",
    "\n",
    "cntvcr_models = {}\n",
    "word_feat_list = {}\n",
    "for i in data_array:\n",
    "    cntvcr_models[i] = cntvcr.fit(body_df[i])\n",
    "    word_feat_list[i] = cntvcr_models[i].transform(body_df[i])\n",
    "    word_feat_list[i].cache()\n",
    "\n",
    "## show word vectors and feature counts for fora:\n",
    "for i in data_array:\n",
    "    print(\"\\n------------------\\n\", i, \"\\n------------------\\n\")\n",
    "    word_feat_list[i].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:19:06.034033\n",
      "\n",
      "------------------\n",
      " english \n",
      "------------------\n",
      "\n",
      "[SparseVector(44232, {6: 2.0, 15: 1.0, 32: 1.0, 52: 1.0, 55: 1.0, 91: 1.0, 221: 1.0, 452: 1.0, 945: 1.0, 2008: 1.0})]\n",
      "CPU times: user 21 ms, sys: 7.92 ms, total: 28.9 ms\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "# converting pyspark.ml vectors to pyspark.mllib vectors\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "def as_mllib_vector(v):\n",
    "    return Vectors.sparse(v.size, v.indices, v.values)\n",
    "\n",
    "features = {}\n",
    "feature_vec_list = {}\n",
    "for i in data_array:\n",
    "    features[i] = word_feat_list[i].select(\"features\")\n",
    "    feature_vec_list[i] = features[i].rdd.map(lambda r: as_mllib_vector(r[0]))\n",
    "    feature_vec_list[i].cache()\n",
    "\n",
    "## print first pyspark.mllib vector across fora:\n",
    "for i in data_array:\n",
    "    print(\"\\n------------------\\n\", i, \"\\n------------------\\n\")\n",
    "    print(feature_vec_list[i].take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------\n",
      " english \n",
      "------------------\n",
      "\n",
      "Vocabulary from CountVectorizerModel is:\n",
      "\n",
      "['word', 'nt', 'sentence', 'would', 'one', 'like', 'use', 'example', 'say', 'used', 'correct', 'mean', 'english', 'question', 'something', 'know', 'phrase', 'meaning', 'could', 'time', 'way', 'two', 'people', 'term', 'following', 'think', 'also', 'want', 'someone', 'person', 'make', 'first', 'verb', 'thing', 'sound', 'find', 'looking', 'context', 'name', 'right', 'seems', 'difference', 'noun', 'usage', 'come', 'case', 'get', 'answer', 'need', 'dictionary', 'work', 'using', 'form', 'describe', 'see', 'different', 'good', 'even', 'however', 'understand', 'language', 'better', 'sure', 'many', 'may', 'help', 'please', 'said', 'go', 'another', 'book', 'part', 'found', 'expression', 'really', 'new', 'article', 'might', 'sense', 'ca', 'much', 'writing', 'wrong', 'speaker', 'thought', 'adjective', 'well', 'read', 'second', 'year', 'without', 'always', 'rule', 'day', 'feel', 'instead', 'take', 'whether', 'still', 'common']\n",
      "\n",
      "---\n",
      "\n",
      "Number of terms M =  44232\n"
     ]
    }
   ],
   "source": [
    "## look at the first 100 words of the vocabulary\n",
    "for i in data_array:\n",
    "    print(\"\\n------------------\\n\", i, \"\\n------------------\\n\")\n",
    "    print (\"Vocabulary from CountVectorizerModel is:\\n\")\n",
    "    print(cntvcr_models[i].vocabulary[:100])\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "    M = len(cntvcr_models[i].vocabulary)\n",
    "    print(\"Number of terms M = \", M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:19:14.912869\n",
      "CPU times: user 19.1 ms, sys: 7.25 ms, total: 26.4 ms\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "## fit LDA models with k=10 topics\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=10, maxIter=5)\n",
    "\n",
    "lda_model_list = {}\n",
    "for i in data_array:\n",
    "    lda_model_list[i] = lda.fit(word_feat_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "'''BOTTLENECK TIME - TAKES OVER 45 min JUST FOR ENGLISH'''\n",
    "\n",
    "# calculate the perplexity and likelihood for each forum's model\n",
    "\n",
    "loglik_list = {}\n",
    "logper_list = {}\n",
    "for i in data_array:\n",
    "    loglik_list[i] = lda_model_list[i].logLikelihood(word_feat_list[i])\n",
    "    logper_list[i] = lda_model_list[i].logPerplexity(word_feat_list[i])\n",
    "    print(\"Finished\", i, \"at\", datetime.now().time())\n",
    "\n",
    "# low perplexity value indicates the model predicts the sample well\n",
    "\n",
    "for i in data_array:\n",
    "    print(\"The lower bound on the log likelihood of the \" +'\\033[1m'+ i +'\\033[0m'+ \" corpus is: \" + str(loglik_list[i]))\n",
    "    print(\"The upper bound on the perplexity of the \" +'\\033[1m'+ i +'\\033[0m'+ \" corpus is: \" + str(logper_list[i]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:19:27.816094\n",
      "\n",
      "------------------\n",
      " english \n",
      "------------------\n",
      "\n",
      "['word' 'sentence' 'example' 'use' 'would']\n",
      "['sentence' 'mean' 'word' 'meaning' 'like']\n",
      "['word' 'sentence' 'one' 'question' 'nt']\n",
      "['nt' 'question' 'term' 'sentence' 'word']\n",
      "['word' 'like' 'nt' 'something' 'example']\n",
      "['word' 'one' 'mean' 'nt' 'someone']\n",
      "['word' 'nt' 'one' 'say' 'time']\n",
      "['word' 'meaning' 'sentence' 'example' 'one']\n",
      "['word' 'nt' 'one' 'would' 'like']\n",
      "['use' 'like' 'word' 'would' 'example']\n",
      "\n",
      "\n",
      "CPU times: user 1.1 s, sys: 98.5 ms, total: 1.2 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now().time())\n",
    "\n",
    "## first 5 words of the 10 topics from the LDA models per forum\n",
    "topic_list = {}\n",
    "for i in data_array:\n",
    "    print(\"\\n------------------\\n\", i, \"\\n------------------\\n\")\n",
    "    topic_list[i] = lda_model_list[i].describeTopics(5)\n",
    "    #print(\"The topics described by their top-weighted terms:\\n\")\n",
    "    #topic_list[i].show(truncate=False)\n",
    "    # show the results\n",
    "    topic_j = topic_list[i].select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "    for j in topic_j:\n",
    "        print(np.array(cntvcr_models[i].vocabulary)[j])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100,\n",
    "                      regParam=0.3,\n",
    "                      elasticNetParam=0.8,\n",
    "                      #featuresCol=\"features\",\n",
    "                      labelCol=\"label\",\n",
    "                      predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105475"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['english'].select('y_ravi').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105475"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_feat_list['english'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Resolved attribute(s) y_ravi#540 missing from dummy#610L,words#611,features#703 in operator !Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#864].;;\\n!Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#864]\\n+- Project [dummy#610L, words#611, UDF(words#611) AS features#703]\\n   +- LogicalRDD [dummy#610L, words#611], false\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o926.withColumn.\n: org.apache.spark.sql.AnalysisException: Resolved attribute(s) y_ravi#540 missing from dummy#610L,words#611,features#703 in operator !Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#864].;;\n!Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#864]\n+- Project [dummy#610L, words#611, UDF(words#611) AS features#703]\n   +- LogicalRDD [dummy#610L, words#611], false\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:326)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2252)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2219)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/5-define-target.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_feat_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_ravi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \"\"\"\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Resolved attribute(s) y_ravi#540 missing from dummy#610L,words#611,features#703 in operator !Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#864].;;\\n!Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#864]\\n+- Project [dummy#610L, words#611, UDF(words#611) AS features#703]\\n   +- LogicalRDD [dummy#610L, words#611], false\\n'"
     ]
    }
   ],
   "source": [
    "word_feat_list['english'].withColumn(\"label\", datasets['english']['y_ravi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Resolved attribute(s) y_ravi#540 missing from dummy#610L,words#611,features#703 in operator !Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#829].;;\\n!Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#829]\\n+- Project [dummy#610L, words#611, UDF(words#611) AS features#703]\\n   +- LogicalRDD [dummy#610L, words#611], false\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o926.withColumn.\n: org.apache.spark.sql.AnalysisException: Resolved attribute(s) y_ravi#540 missing from dummy#610L,words#611,features#703 in operator !Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#829].;;\n!Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#829]\n+- Project [dummy#610L, words#611, UDF(words#611) AS features#703]\n   +- LogicalRDD [dummy#610L, words#611], false\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:326)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2252)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2219)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/lse-msc-thesis-brad/01-python-code/00-workspace/5-define-target.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_feat_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_ravi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \"\"\"\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Resolved attribute(s) y_ravi#540 missing from dummy#610L,words#611,features#703 in operator !Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#829].;;\\n!Project [dummy#610L, words#611, features#703, y_ravi#540 AS label#829]\\n+- Project [dummy#610L, words#611, UDF(words#611) AS features#703]\\n   +- LogicalRDD [dummy#610L, words#611], false\\n'"
     ]
    }
   ],
   "source": [
    "final_data = word_feat_list['english'].withColumn(\"label\", datasets['english']['y_ravi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100,\n",
    "                      regParam=0.3,\n",
    "                      elasticNetParam=0.8,\n",
    "                      featuresCol=\"features\",\n",
    "                      labelCol=\"label\",\n",
    "                      predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[processing_pipeline, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline = pipeline.fit(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline.transform(taxi).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trained_pipeline\n",
    " .transform(taxi)\n",
    " .select(\n",
    "    categorical_variables + numerical_variables + [\"prediction\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"taxi_2014_prediction.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_predictions = spark.read.parquet(\"taxi_2014_prediction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_predictions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_predictions.select(\"prediction\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert notebook to python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script 0-master-notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
