{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Community Engagement with StackExchange Fora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Abstract\n",
    "* Introduction\n",
    "* Datasets: StackExchange\n",
    "* Distributed Computing Technology\n",
    "* Conclusion\n",
    "* References\n",
    "* Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract - EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing high-quality questions and procuring answers to these questions is not only fundamental to learning and information gathering, but to scientific and progress as a whole. Online question-answer communities and fora have given internet-users the unprecendented ability of presenting questions to the world, and while much attention has been given to finding the right answers (ask Google), relatively little attention has been dedicated to how we can improve questions. \n",
    "\n",
    "Since expert resources in technical question-answer communities are often scarce, having users produce relevant, legible and previously-researched questions is **particularly valuable to communities.** One way in which to promote this is to predict a question's quality or community engagement so that users can be nudged to improve aspects of their questions before adding demand to the expert resources in a community. With this goal in mind, this analysis critiques, **validates** and extends the methodology in previous research that has been focused on questions in question-answer communities. By harnessing the capability of distributed cloud computing, I am able to analyse the 10 largest **dedicated** question-answer communities **on the internet**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a gooood research question because...\n",
    "\n",
    "The ability to use cloud computing is cool because...\n",
    "\n",
    "The analysis will proceed in the following steps:\n",
    "\n",
    "* Fleshing out of datasets\n",
    "* Fleshing out of Big Data Tech (PySpark...\n",
    "* Elementary Exploratory Data Analysis (EDA) on 10 datasets\n",
    "* Definition of binary response variable relating to high and low quality questions.\n",
    "* Model prediction using binary response variable\n",
    "* Results and conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the project is to explore how the top 10 largest Stack Exchange communities interact with questions. Firstly, I will perform EDA on all questions, answers and comments of the 10 large datasets. The second goal is to build a classification model on various different communities to identify questions with high potential of positive community engagement.\n",
    "\n",
    "The classification model will use only the question text and title as input, deriving features using sentiment analysis and LDA. Positive community engagement will be summarised by PCA using the score of the question, the number of answers and the sentiment of the answers.\n",
    "\n",
    "I will then test the individual models across all 10 communities to compare results. In this way I should gain insight into the degree to which StackExchange communities engage with questions homogenously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discussion of more advanced distributed computing techniques necessetates at least an introduction to Structured Query Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is this and this and this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can invoke SparkContext as `sc`. `pyspark.SparkContext` is the main entry point for Spark functionality.\n",
    "\n",
    "`pyspark.RDD` is the novel concept of Resilient Distributed Dataset (RDD) which is the basic abstraction in Spark.\n",
    "\n",
    "The design for RDDs was intended to ensure fault-tolerance, distribution across **....** and in-memory computing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also able to work with streaming data with `pyspark.streaming.StreamingContext` being the main entry point for the Spark Streaming capability.\n",
    "\n",
    "`pyspark.streaming.DStream` is a Discretized Stream (DStream) which is the basic abstraction in Spark Streaming.\n",
    "\n",
    "For the use of DataFrames and SQL, `pyspark.sql.SparkSession` is used where `pyspark.sql.DataFrame` serves as a distributed collection of data that grouped into named columns.\n",
    "\n",
    "Since we will not be working with streaming data, I will mostly be using `pyspark.sql.DataFrame` objects, the user guide for which can be found [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sqlContext is also automatically created in the PySpark session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL does this, that and that other thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gcloud discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the advent of dist. compt. and big data, growing research interest in powerful tech. using state-of-art computing capabilities to derive data insight.**\n",
    "\n",
    "**Provide introduction of tech used?? Architecture, syntax, comparison to other S.Q.L.s?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List technologies used in this project, references, use Milan's slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of SparkSQL that make it so amazing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gcloud offers different types of masters/workers as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to other BigData tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a rule of thumb is that Spark is best-suited for real-time or streaming data processing where a large amount of RAM is accessible and Hadoop is more suitable for batch processing using commodity hardware, I chose to use Spark owing to the ease of use of PySpark and the SQL like API, **over some** imperative programming like Mapreduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: 3 Large StackExchange Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data I will use are community question-answering fora from the family of websites, [StackExchange](https://stackexchange.com/sites#traffic). StackExchange is a group of over 170 dedicated question-answer websites with topics ranging from vegetarianism to robotics to science-fiction world building - the computer coding/debugging forum StackOverflow being the **first** and largest with 10 million registered users and 18 million questions at the time of writing.\n",
    "\n",
    "The data across all StackExchange sites are publicly available in **7z compressed** XML format at [archive.org](https://archive.org/download/stackexchange) and are regularly updated (the last update being **March 2019**. For each site, the following files are available:\n",
    "\n",
    "* `PostHistory.xml`: A history of the versions of each question and answer posted (questions can be edited)\n",
    "* `Posts.xml`: The final, up to date version of each question and answer posted\n",
    "* `Users.xml`: Data on registered users \n",
    "* `Votes.xml`: Data on **votes cast??**\n",
    "* `Comments.xml`: The final, up to date version of each comment posted\n",
    "* `Badges.xml`: Log of when badges (an incentive mechanism) were awarded to users for specific achievements\n",
    "* `PostLinks.xml`: **URL links in posts??**\n",
    "* `Tags.xml`: Data on the tags **in tandem** with each question posted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is an initial analysis, I will only be using the `Posts.xml` data for each forum. In this file, the following variables are recorded per question posted:\n",
    "\n",
    "* `Id`: A chronological post identity variable\n",
    "* `PostTypeId`: An indicator of whether the post is an question (==1) or an answer (==2) **as well as other values**\n",
    "* `ParentId`: An indicator of which question and answer belongs to (only answers)\n",
    "* `AcceptedAnswerId`: An indicator of which answer the original question poster selected as accepted (only questions)\n",
    "* `CreationDate`: A date variable relating to when the post was made\n",
    "* `Score`: The difference between up-votes and down-votes for posts\n",
    "* `ViewCount`: The number of times that a post has been viewed by registered and non-registered users alike\n",
    "* `Body`: The main post content\n",
    "* `OwnerUserId`: A identity variable for the post owner\n",
    "* `LastEditorUserId`: A identity number for the last registered user that edited the post\n",
    "* `LastEditDate`: A date variable relating to when the post was last edited\n",
    "* `LastActivityDate`: A date variable relating to when last there was activity on the post\n",
    "* `Title`: The post title for questions only\n",
    "* `Tags`: The collection of tags linked to a question\n",
    "* `AnswerCount`: The number of answers that a question receives (questions only)\n",
    "* `CommentCount`: The number of comments that a post receives\n",
    "* `FavoriteCount`: The number of times that users have favourited a question (questions only)\n",
    "* `ClosedDate`: A boolean relating to whether a question has been closed or not (questions only)\n",
    "\n",
    "Note that post above refers to both question and answer.\n",
    "\n",
    "Users can upvote posts and can have reputation.\n",
    "\n",
    "Score distribution is highly skewed - users can upvote at 15, downvote at 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables that are important to this analysis are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets were downloaded with the following bash script, run when the gcloud cluster is created:\n",
    "\n",
    "```bash\n",
    "## install unzipper\n",
    "sudo apt-get install --yes p7zip-full\n",
    "\n",
    "## create folder and datasets array (separate approach for stackoverflow.com-Posts.7z)\n",
    "mkdir datasets && cd datasets\n",
    "\n",
    "declare -a arr=(\n",
    "\"askubuntu.com\"\n",
    "\"codereview.stackexchange.com\"\n",
    "\"math.stackexchange.com\"\n",
    "\"physics.stackexchange.com\"\n",
    "\"ru.stackoverflow.com\"\n",
    "\"serverfault.com\"\n",
    "\"stackoverflow.com-Posts\"\n",
    "\"superuser.com\"\n",
    "\"tex.stackexchange.com\"\n",
    "\"unix.stackexchange.com\"\n",
    ")\n",
    "\n",
    "## loop downloading, unzipping and extracting all XML data\n",
    "for i in \"${arr[@]}\"\n",
    "do\n",
    "   mkdir \"$i\" && cd \"$i\"\n",
    "   wget https://archive.org/download/stackexchange/\"$i\".7z\n",
    "   p7zip -d \"$i\".7z\n",
    "   \n",
    "   cd ..\n",
    "done\n",
    "\n",
    "## rename folders for ease of import later\n",
    "mv stackoverflow.com-Posts stackoverflow.stackexchange.com\n",
    "mv superuser.com superuser.stackexchange.com\n",
    "mv askubuntu.com askubuntu.stackexchange.com\n",
    "mv serverfault.com serverfault.stackexchange.com\n",
    "mv ru.stackoverflow.com rus_stackoverflow.stackexchange.com\n",
    "\n",
    "cd\n",
    "\n",
    "## copy datasets folder over to bucket\n",
    "gsutil cp -r datasets gs://bucket-brad/datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following code enter into my terminal to spin up a custom cluster on Gcloud:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "## set project beforehand\n",
    "gcloud config list\n",
    "export PROJECT=\"stack-exchange-project\"\n",
    "gcloud config set project ${PROJECT}\n",
    "\n",
    "## set variables beforehand\n",
    "export NUM_WORKERS=\"2\"\n",
    "export BUCKET=\"bucket-brad-project\"\n",
    "export CLUSTER=\"cluster-brad\"\n",
    "\n",
    "## spin up cluster\n",
    "gcloud dataproc clusters create ${CLUSTER} \\\n",
    "--properties=^#^spark:spark.jars.packages=com.databricks:spark-xml_2.11:0.4.1 \\\n",
    " --subnet default --zone europe-west2-a --master-machine-type n1-highmem-96 \\\n",
    " --master-boot-disk-size 500 --num-workers=${NUM_WORKERS} --worker-machine-type n1-highmem-16 \\\n",
    " --worker-boot-disk-size 500 --image-version 1.3-deb9 --project=${PROJECT} --bucket=${BUCKET} \\\n",
    " --initialization-actions 'gs://dataproc-initialization-actions/jupyter/jupyter.sh',\\\n",
    "'gs://dataproc-initialization-actions/python/pip-install.sh','gs://bucket-brad-project/my-actions.sh' \\\n",
    "--metadata 'PIP_PACKAGES=sklearn nltk pandas'\n",
    "\n",
    "## connect to jupyter notebook\n",
    "export PORT=8123\n",
    "export ZONE=\"europe-west2-a\"\n",
    "export HOSTNAME=cluster-brad-m\n",
    "\n",
    "gcloud compute ssh ${HOSTNAME} \\\n",
    "    --project=${PROJECT} --zone=${ZONE}  -- \\\n",
    "    -D ${PORT} -N &\n",
    "\n",
    "\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\" \\\n",
    "      --proxy-server=\"socks5://localhost:${PORT}\" \\\n",
    "      --user-data-dir=/tmp/${HOSTNAME}\n",
    "```\n",
    "```bash\n",
    "## delete clusters after use\n",
    "gcloud dataproc clusters delete cluster-brad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I used a high-memory master node with 96 CPUs and two high-memory worker nodes with 16 CPUs each. In order to have access to this level of computing, I had to request an increase in my quotas from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"Welcome to my notebook\"\\n'\n"
     ]
    }
   ],
   "source": [
    "## testing printing output from console\n",
    "import subprocess\n",
    "cmd = [ 'echo', '\"Welcome to my notebook\"' ]\n",
    "output = subprocess.Popen( cmd, stdout=subprocess.PIPE ).communicate()[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use com.databricks package\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.4.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restart kernel\n",
    "#os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import standard libraries\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "\n",
    "## import pyspark elements\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql import Row\n",
    "\n",
    "## import natural language processing elements\n",
    "import nltk\n",
    "#nltk.download('all') # uncomment first run as admin check\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "## function to tokenise and lemmatise\n",
    "def get_tokens(line):\n",
    "    ###\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    return (words)\n",
    "\n",
    "## set checkpoint directory\n",
    "sc.setCheckpointDir(\"gs://bucket-brad-project/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check default number of partitions\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose datasets to compare\n",
    "data_array = [\n",
    "#\"askubuntu\",\n",
    "\"codereview\",\n",
    "#\"math\",\n",
    "\"physics\",\n",
    "#\"rus_stackoverflow\",\n",
    "#\"serverfault\",\n",
    "#\"stackoverflow\",\n",
    "#\"superuser\",\n",
    "#\"tex\",\n",
    "\"unix\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## point to right directory\n",
    "fpath = 'gs://bucket-brad-project/datasets/large-datasets/'\n",
    "\n",
    "## function to extract data from xml files\n",
    "def load_all_xml(table_name):\n",
    "    # create dataframe list skeleton\n",
    "    dfs = {}\n",
    "    xml_name = table_name.capitalize()\n",
    "    for i in data_array:\n",
    "        table = sqlContext.read.format('com.databricks.spark.xml').options(rowTag=table_name).load(fpath+i+'.stackexchange.com/'+xml_name+'.xml')\n",
    "        rdd_table = table.rdd.flatMap(lambda x: x).flatMap(lambda x: x)\n",
    "        # assign dataframe to list entry\n",
    "        dfs[i] = sqlContext.createDataFrame(rdd_table, table.schema.fields[0].dataType.elementType)\n",
    "        dfs[i].createOrReplaceTempView(table_name)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs master node: 96\n"
     ]
    }
   ],
   "source": [
    "## display architecture (must find workers as well)\n",
    "import multiprocessing\n",
    "print(\"Number of CPUs master node:\", multiprocessing.cpu_count())\n",
    "\n",
    "#import sigar\n",
    "#sg = sigar.open()\n",
    "#mem = sg.mem()\n",
    "#sg.close() \n",
    "#print(mem.total()/1024, mem.free()/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A substantial amount of time was spent testing out cluster architectures since just the loading in of the **smaller** datasets were taking a substantial amount of time. \n",
    "\n",
    "At the end of the day, I decided to scale mostly vertically, as well as horizontally, resulting in a high-memory master node with **96 CPUs and two high-memory worker nodes with 16 CPUs each**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of the times taken to read in only the respective forums are displayed in the comments below, where this is how the architecture is described:\n",
    "\n",
    "`# CPUs master; number of workers x CPUs workers`\n",
    "\n",
    "where `hc` stands for high cpu and `hm` stands for high memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:16:52.041164\n",
      "Time taken: 11.866666666666667 minutes.\n"
     ]
    }
   ],
   "source": [
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## extract all datasets and store in list of dataframes\n",
    "posts_dfs = load_all_xml(\"posts\")\n",
    "\n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") \n",
    "# hc32;4xhc8 - more than 7 minutes codereview\n",
    "# hc32;7x4 - 6.57 minutes codereview\n",
    "# hm32;2x8 - 4.18 minutes codereview\n",
    "# 32;2x4 - 3.83 minutes codereview\n",
    "# hc32;2x4 - 3.78 minutes codereview\n",
    "# 64;2x4 - 3.73 minutes codereview\n",
    "# hm32;2x4 - 3.68 minutes codereview\n",
    "# hm96;2xhm16 - 3.85 minutes codereview\n",
    "# hm96;2xhm16 - 11.86 minutes codereview, physics and unix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that **memory** was the resource needed in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the data is initially in raw XML format which is considered **semi-structured** since it contains tags/markers to enforce hierarchy of entries. This then gets stored in a `pyspark.sql.DataFrame` which has a schema and is thus **structured**, but the final `Body` variable I will work with is completely **unstructured** since as text it is not organised in any manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "\n",
      "root\n",
      " |-- _AcceptedAnswerId: long (nullable = true)\n",
      " |-- _AnswerCount: long (nullable = true)\n",
      " |-- _Body: string (nullable = true)\n",
      " |-- _ClosedDate: string (nullable = true)\n",
      " |-- _CommentCount: long (nullable = true)\n",
      " |-- _CommunityOwnedDate: string (nullable = true)\n",
      " |-- _CreationDate: string (nullable = true)\n",
      " |-- _FavoriteCount: long (nullable = true)\n",
      " |-- _Id: long (nullable = true)\n",
      " |-- _LastActivityDate: string (nullable = true)\n",
      " |-- _LastEditDate: string (nullable = true)\n",
      " |-- _LastEditorDisplayName: string (nullable = true)\n",
      " |-- _LastEditorUserId: long (nullable = true)\n",
      " |-- _OwnerDisplayName: string (nullable = true)\n",
      " |-- _OwnerUserId: long (nullable = true)\n",
      " |-- _ParentId: long (nullable = true)\n",
      " |-- _PostTypeId: long (nullable = true)\n",
      " |-- _Score: long (nullable = true)\n",
      " |-- _Tags: string (nullable = true)\n",
      " |-- _Title: string (nullable = true)\n",
      " |-- _VALUE: string (nullable = true)\n",
      " |-- _ViewCount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print type of posts_dfs\n",
    "print(type(posts_dfs))\n",
    "print('\\n')\n",
    "\n",
    "# print type of first dataframe\n",
    "print(type(posts_dfs[\"codereview\"]))\n",
    "print('\\n')\n",
    "\n",
    "# schema of first dataframe\n",
    "posts_dfs[\"codereview\"].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:45.174618\n",
      "\n",
      "------------------\n",
      " codereview \n",
      "------------------\n",
      "\n",
      "DataFrame[summary: string, _AcceptedAnswerId: string, _AnswerCount: string, _Body: string, _ClosedDate: string, _CommentCount: string, _CommunityOwnedDate: string, _CreationDate: string, _FavoriteCount: string, _Id: string, _LastActivityDate: string, _LastEditDate: string, _LastEditorDisplayName: string, _LastEditorUserId: string, _OwnerDisplayName: string, _OwnerUserId: string, _ParentId: string, _PostTypeId: string, _Score: string, _Tags: string, _Title: string, _VALUE: string, _ViewCount: string]\n",
      "\n",
      "------------------\n",
      " physics \n",
      "------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c04ba2773a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n------------------\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n------------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# takes flippen long, must select certain things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m## end time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## FIX THIS CELL\n",
    "\n",
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "for i in data_array:\n",
    "    print(\"\\n------------------\\n\", i, \"\\n------------------\\n\")\n",
    "    print(posts_dfs[i].describe())\n",
    "    \n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") #hm96;2xhm16 - 5.6 minutes codereview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## empty dictionary of df skeleton\n",
    "plot_data = {}\n",
    "\n",
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## total number of posts of fora\n",
    "for i in data_array:\n",
    "    plot_data[i] = posts_dfs[i].count()\n",
    "\n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") # hm96;2xhm16 - 5.85 minutes codereview\n",
    "    \n",
    "## bar plot of post counts in descending order\n",
    "import operator\n",
    "plot_list = sorted(plot_data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "x, y = zip(*plot_list) # unpack a list of pairs into two tuples\n",
    "plt.bar(x, y, align='center', alpha=.8)\n",
    "plt.xticks(range(len(plot_list)), list([i[0] for i in plot_list]), rotation=90)\n",
    "plt.title('The total number of posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:38:36.974593\n",
      "Time taken: 24 minutes.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAE4CAYAAACgzrNHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHOJJREFUeJzt3Xu0XHV99/H3hwCKiKIktYUQgi1Y0aWiEWylFRE1okK1XoioWNGsPj5KaWtbbK1Q1OfxUpe9SMXUIhUriPfIgwZb5aKAElRoAbERlKSIhPvFCwa/zx97HxkP55yZkzOTOey8X2udldl7/2bv756ZfOY3vz2zd6oKSVK3bDPuAiRJw2e4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnumreSnJPkNWPa9g5JPpfktiQfH0cN0lwY7iPWBtQtSR4w7lo0Ky8CHgHsUlUvHncxml6SVyX5yrjrmG8M9xFKshT4HaCAQ0e0jW1Hsd4uSWO2r/U9gO9U1aZR1LSlbOa+D2vbvjbHyHAfrVcCFwGnAEdOzEzylCTXJ1nQM+8FSS5rb2+T5Ngk301yU5Izkjy8XbY0SSU5Ksm1wJfa+R9v13lbkvOSPKZn3bu0Qwy3J7k4ydt6ezpJfjPJF5PcnOSqJC+ZbofaTyJvTfLVJHckOTvJwnbZgUk2TGr/vSQHt7ePb+v8SHvf/0yyd5I3Jbkhyfokz5q0yV9P8vV2vz478Tj0PI4XJLk1yaVJDpxU59uTfBX4EfDIKfbl0W27W5NcnuTQdv7fAG8BXprkziRHTXHf/ZJc2N73B0nel2T7nuWV5A+T/Hf7ye3EJGmX/UaSc9t9ujHJxya2m+Qf29vbJbkrybva6R2S/CTJwzZn39ve7dXt435NkiOmeX6PT/KJJB9r234jyeN7lu+a5JNJNrbrOXqK+34kye3Aq6ZY/ylJTmpfb3e0j8MePct/u32N3tb++9s9y+6zD0keDZwE/Fb7XN061X5tlarKvxH9AeuA1wFPAn4GPKJn2XeBZ/ZMfxw4tr19DM2bwmLgAcAHgNPaZUtpPgl8GNgR2KGd/2pgp7b93wHf6ln36e3fg4B9gPXAV9plO7bTfwBsCzwRuBF4zDT7dE5b+97ADu30O9plBwIbJrX/HnBwe/t44CfAs9ttfRi4BvgrYDvgtcA1k7b1P8Bj2zo/CXykXbYbcBNwCE0n5Znt9KKe+14LPKbd1naT6tqufX7+EtgeOAi4A3hUT60fmeG5fRLwlHbdS4ErgWN6lhdwJrAzsATYCCxvl53W7vM2wAOBA9r5BwH/2d7+7fZx/lrPsks3c98fCtzes2+/NsPzezzNa/VF7WP0xvY52q7d1iU0b3zb07xhXg08e9J9f69tu8MU6z+lfZx/l+a1+vfc+1p8OHAL8Iq27hXt9C7t8z/lPtC8iXxl3P/f59vf2Avo6h9wQPtCX9hOfxv4457lbwNObm/vBNwF7NFOXwk8o6ftr7XrmgiSAh45w7Z3bts8FFjQ3vdRk7Y98R/qpcD5k+7/AeC4adZ9DvDmnunXAV9obx9I/3D/Ys+y5wN3Agt6HocCdu7Z1jt62u8D3N3u018Ap07a1hrgyJ77njDDY/Q7wPXANj3zTgOO76l12nCfYn3HAJ/umS7a0G6nz+DeN+8PA6uAxZPWsQPNm98uwLE0bzwbgAcDfwP8Q9tuVvtOE4y3Ar/PFIE7aT3HAxf1TG8D/KB9vPYHrp3U/k3Ah3rue16f9Z8CnN4z/WDgHmB3mlD/+qT2F9KE97T7gOE+5Z/DMqNzJHB2Vd3YTn+UnqGZdvqFaQ60vhD4RlV9v122B/Dp9iP3rTRhfw/NAb4J6yduJFmQ5B1phnFupwlUgIXAIpo3hfVT3bfd1v4T22q3dwTwqzPs2/U9t39E8x90UD/suf1j4Maquqdnmknr6631+zQ9yIVt3S+eVPcBNG+EU913sl2B9VX180nr322QnWiHk85MMxR2O/B/2rp6Tfc4/TkQ4OvtcNCrAarqx8Ba4Gk0PdtzgQuAp7bzzm3vP6t9r6q7aN7E/xD4QZL/l+Q3Z9i93vv+nOYNZtd2u7tO2u5fMs3rcsD13wnc3K5/V5rnoNf3gd02Yx+2eh7wGIEkOwAvARYkmfgP/gBg5ySPr6pLq+qKJN8HngO8jCbsJ6wHXl1VX51i3Uvbm72n83wZcBhwME2wP5Tm42xohgM20QzxfKdtv/ukbZ1bVc/crJ39ZXfRDP1M1LqA5s1lLnprXULzKeRGmrpPrarXznDfmU55eh2we5JtegJ+Cfc+Rv28H/gmsKKq7khyDM1QRl9VdT3NEBRJDgD+Pcl5VbWOJsAPAvYFLm6nnw3sB5zXrmLW+15Va4A17WvzbcA/0/TGp/KLxzzNwdjFNI/XJpphs70G3e4A638wzXDMde3fHpPaLgG+0GcfPLXtFOy5j8bv0fS09wGe0P49Gjif5iDrhI8CR9P00nq/S30S8PaJA01JFiU5bIbt7QT8lGbc9UE0vUgA2l7xp4Djkzyo7e301nAmsHeSV7QH8bZL8uT2QNVsfQd4YJLnJtkOeDPNm9pcvDzJPkkeBJwAfKLdp48Az0/y7PaTywPTHNBdPOB6v0bzZvTn7T4fSDNMdPqA99+JZgz4zvYx/V+D7lCSF/fUeQtNOE18ejmX5vm5oqruphlieQ1NqG5s28xq35M8IsmhSXakeZ3c2bO9qTwpyQvTfNvlmPY+FwFfB25P8hdpDvAuSPLYJE8edN9bhyQ5IM0B6LfSHFdYD5xF81p8WZJtk7yU5v/QmX324YfA4vQc0JbhPipH0oxDXltV10/8Ae8Djsi9XxE7jWac+ks9wzfQHGRaDZyd5A6a/1j7z7C9D9N8fP0f4Iq2fa/X0/TmrwdObbf7U4CqugN4FnA4Tc/peuCdbEYoV9VtNGPwH2xruYvmI/1cnEozTns9zcHHo9ttraf5tPKXNJ9O1gN/xoCv6TY4D6X55HQj8E/AK6vq2wPW9UaaT0x30PQgPzbg/QCeDHwtyZ00z/MfVdU17bILaMbeJ3rpV9CMw09Mb86+bwP8Kc3zezPNEM/rZqjvszRDIBMHN19YVT9r31SfT9NZuYbmcfsgzWtrNj4KHNfW8iSaYUCq6ibgeW2tN9EMXz2v/b8x0z58CbgcuD5J7/+jrVraAxLaiiR5J/CrVXVk38baqiQ5HviNqnr5iNZ/Cs1B9zePYv26lz33rUCa77E/Lo39gKOAT4+7Lkmj4wHVrcNONEMxuwI3AO+h+egtqaMclpGkDnJYRpI6yHCXpA4a25j7woULa+nSpePavCTdL11yySU3VlXfHweOLdyXLl3K2rVrx7V5Sbpfan/Z3pfDMpLUQYa7JHVQ33BPcnKaCyn81wxtDkzyrfYMd+dO106StGUM0nM/BVg+3cIkO9Ocl+PQqnoM4PUmJWnM+oZ7VZ1Hc6Ke6bwM+FRVXdu2v2FItUmSNtMwxtz3Bh6W5rqNlyR5Zd97SJJGahhfhdyW5rSdz6A5VemFSS6qqvtc9CDJSmAlwJIlS4awaUnSVIbRc99Acw3Nu9rzLp8HPH6qhlW1qqqWVdWyRYvmeoEeSdJ0htFz/yzwvvYCFNvTXFTivUNY77Se/49fGeXqt2qfe8MB4y5B0hD0DfckE1cLWphkA80VVLYDqKqTqurKJF8ALgN+Dnywqqb92qQkafT6hntVrRigzbuBdw+lIknSnPkLVUnqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4a22X2tHXxV8Wj46+KNRV77pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EF+W0bSlPyG0+hsiW842XOXpA4y3CWpgwx3Seogw12SOqhvuCc5OckNSWa8LmqSJye5J8mLhleeJGlzDNJzPwVYPlODJAuAdwJrhlCTJGmO+oZ7VZ0H3Nyn2RuATwI3DKMoSdLczHnMPcluwAuAk+ZejiRpGIZxQPXvgL+oqnv6NUyyMsnaJGs3btw4hE1LkqYyjF+oLgNOTwKwEDgkyaaq+szkhlW1ClgFsGzZshrCtiVJU5hzuFfVnhO3k5wCnDlVsEuStpy+4Z7kNOBAYGGSDcBxwHYAVeU4uyTNQ33DvapWDLqyqnrVnKqRJA2Fv1CVpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QO6hvuSU5OckOS/5pm+RFJLmv/Lkjy+OGXKUmajUF67qcAy2dYfg3wtKp6HPBWYNUQ6pIkzcEg11A9L8nSGZZf0DN5EbB47mVJkuZi2GPuRwGfH/I6JUmz1LfnPqgkT6cJ9wNmaLMSWAmwZMmSYW1akjTJUHruSR4HfBA4rKpumq5dVa2qqmVVtWzRokXD2LQkaQpzDvckS4BPAa+oqu/MvSRJ0lz1HZZJchpwILAwyQbgOGA7gKo6CXgLsAvwT0kANlXVslEVLEnqb5Bvy6zos/w1wGuGVpEkac78haokdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHVQ33BPcnKSG5L81zTLk+QfkqxLclmSJw6/TEnSbAzScz8FWD7D8ucAe7V/K4H3z70sSdJc9A33qjoPuHmGJocBH67GRcDOSX5tWAVKkmZvGGPuuwHre6Y3tPMkSWMyjHDPFPNqyobJyiRrk6zduHHjEDYtSZrKMMJ9A7B7z/Ri4LqpGlbVqqpaVlXLFi1aNIRNS5KmMoxwXw28sv3WzFOA26rqB0NYryRpM23br0GS04ADgYVJNgDHAdsBVNVJwFnAIcA64EfAH4yqWEnSYPqGe1Wt6LO8gP89tIokSXPmL1QlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDBgr3JMuTXJVkXZJjp1i+JMmXk3wzyWVJDhl+qZKkQfUN9yQLgBOB5wD7ACuS7DOp2ZuBM6pqX+Bw4J+GXagkaXCD9Nz3A9ZV1dVVdTdwOnDYpDYFPKS9/VDguuGVKEmarb4XyAZ2A9b3TG8A9p/U5njg7CRvAHYEDh5KdZKkzTJIzz1TzKtJ0yuAU6pqMXAIcGqS+6w7ycoka5Os3bhx4+yrlSQNZJBw3wDs3jO9mPsOuxwFnAFQVRcCDwQWTl5RVa2qqmVVtWzRokWbV7Ekqa9Bwv1iYK8keybZnuaA6epJba4FngGQ5NE04W7XXJLGpG+4V9Um4PXAGuBKmm/FXJ7khCSHts3+FHhtkkuB04BXVdXkoRtJ0hYyyAFVquos4KxJ897Sc/sK4KnDLU2StLn8haokdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHXQQOGeZHmSq5KsS3LsNG1ekuSKJJcn+ehwy5QkzUbfy+wlWQCcCDwT2ABcnGR1e2m9iTZ7AW8CnlpVtyT5lVEVLEnqb5Ce+37Auqq6uqruBk4HDpvU5rXAiVV1C0BV3TDcMiVJszFIuO8GrO+Z3tDO67U3sHeSrya5KMnyYRUoSZq9vsMyQKaYV1OsZy/gQGAxcH6Sx1bVrb+0omQlsBJgyZIlsy5WkjSYQXruG4Dde6YXA9dN0eazVfWzqroGuIom7H9JVa2qqmVVtWzRokWbW7MkqY9Bwv1iYK8keybZHjgcWD2pzWeApwMkWUgzTHP1MAuVJA2ub7hX1Sbg9cAa4ErgjKq6PMkJSQ5tm60BbkpyBfBl4M+q6qZRFS1JmtkgY+5U1VnAWZPmvaXndgF/0v5JksbMX6hKUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHDRTuSZYnuSrJuiTHztDuRUkqybLhlShJmq2+4Z5kAXAi8BxgH2BFkn2maLcTcDTwtWEXKUmanUF67vsB66rq6qq6GzgdOGyKdm8F3gX8ZIj1SZI2wyDhvhuwvmd6QzvvF5LsC+xeVWcOsTZJ0mYaJNwzxbz6xcJkG+C9wJ/2XVGyMsnaJGs3btw4eJWSpFkZJNw3ALv3TC8GruuZ3gl4LHBOku8BTwFWT3VQtapWVdWyqlq2aNGiza9akjSjQcL9YmCvJHsm2R44HFg9sbCqbquqhVW1tKqWAhcBh1bV2pFULEnqq2+4V9Um4PXAGuBK4IyqujzJCUkOHXWBkqTZ23aQRlV1FnDWpHlvmabtgXMvS5I0F/5CVZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOmigcE+yPMlVSdYlOXaK5X+S5IoklyX5jyR7DL9USdKg+oZ7kgXAicBzgH2AFUn2mdTsm8Cyqnoc8AngXcMuVJI0uEF67vsB66rq6qq6GzgdOKy3QVV9uap+1E5eBCwebpmSpNkYJNx3A9b3TG9o503nKODzcylKkjQ32w7QJlPMqykbJi8HlgFPm2b5SmAlwJIlSwYsUZI0W4P03DcAu/dMLwaum9woycHAXwGHVtVPp1pRVa2qqmVVtWzRokWbU68kaQCDhPvFwF5J9kyyPXA4sLq3QZJ9gQ/QBPsNwy9TkjQbfcO9qjYBrwfWAFcCZ1TV5UlOSHJo2+zdwIOBjyf5VpLV06xOkrQFDDLmTlWdBZw1ad5bem4fPOS6JElz4C9UJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3SeqggcI9yfIkVyVZl+TYKZY/IMnH2uVfS7J02IVKkgbXN9yTLABOBJ4D7AOsSLLPpGZHAbdU1W8A7wXeOexCJUmDG6Tnvh+wrqqurqq7gdOBwya1OQz41/b2J4BnJMnwypQkzcYg4b4bsL5nekM7b8o2VbUJuA3YZRgFSpJmb9sB2kzVA6/NaEOSlcDKdvLOJFcNsP0uWAjcOO4iBpGjx13BvOFzdv9yv3m+YM7P2R6DNBok3DcAu/dMLwaum6bNhiTbAg8Fbp68oqpaBawapLAuSbK2qpaNuw4Nzufs/sXn674GGZa5GNgryZ5JtgcOB1ZParMaOLK9/SLgS1V1n567JGnL6Ntzr6pNSV4PrAEWACdX1eVJTgDWVtVq4F+AU5Oso+mxHz7KoiVJM4sd7NFLsrIdktL9hM/Z/YvP130Z7pLUQZ5+QJI6yHCXpA4y3KVptN8Ok+6XDPcRSHJ+kre3J1zbadz1qL8k5/Se8C7JfjRfA9Y8leSPkjwkjX9J8o0kzxp3XfOF4T4aRwJXAb8PXJBkbZL3jrkmzez/Al9I8rokbwdOAv5gzDVpZq+uqtuBZwGLaJ6vd4y3pPljkF+oapaq6uokPwbubv+eDjx6vFVpJlW1JskfAl+k+Rn7vlV1/ZjL0swmTntyCPChqrrUExbey577CCT5LvAZ4BE0P/B6bFUtH29VmkmSvwb+Efhd4HjgnCTPHWtR6ueSJGfThPuadgj052Ouad7we+4jkOSPgANozrfzbeBc4Lyq+u5YC9O0kvw9cGxV/bid3gP4YFU9c7yVaTpJtgGeAFxdVbcm2QXYraouG3Np84LhPkJJHkwzDvhGYHFVLRhzSVJnJHkBzXmsbmundwYOrKrPjLey+cFwH4Ek76HpuT8YuBA4Hzi/qq4ea2G6jyR/V1XHJPkcU5ymuqoOHUNZGkCSb1XVEybN+2ZV7TuumuYTD6iOxkXAu6rqh+MuRH2d2v77t2OtQptjqmOGZlrLnvsItGOBLwP2rKq3JlkC/GpVfX3MpUmdkeRk4FaaazwX8AbgYVX1qnHWNV8Y7iOQ5P00R+0PqqpHJ3kYcHZVPXnMpWkaSZ5K8y2ZPWh6fwGqqh45zro0vSQ7An8NHEzzfJ0NvK2q7hprYfOE4T4CSb5RVU/sHf9LcmlVPX7ctWlqSb4N/DFwCXDPxPyqumlsRUlz4PjUaPwsyQLaA3RJFuH3b+e726rq8+MuQv15EHwwhvto/APwaeBX2p+yvwh483hLUh9fTvJu4FPATydmVtU3xleSpuFB8AE4LDMiSX4TeAbNWOB/VNWVYy5JM0jy5fbmxH+IiTH3g8ZUkmahPa61uz9gupc99yFK8pCquj3Jw4EbgNN6lj28qm4eX3Xq45wp5tnzmceSnAMcSpNj3wI2Jjm3qv5krIXNE4b7cH0UeB7NQbneYEg77Tcv5q87e24/kOZ59NPW/PbQtjP1GpoThx2XxJ57y2EZaQpJHgCsrqpnj7sWTS3Jf9Kc7vdfgb+qqouTXFZVjxtzafOCZ4UcgSSfTbIiyYPGXYs224Pwk9Z8dwKwBljXBvsjgf8ec03zhj33EUjyNOClwHOBrwMfA86sqp+MtTBNq+0FTvxnWEBz8YcTqup946tKM/E41swM9xFqv+t+EPBaYHlVPWTMJWka7Sl+J2wCflhVm8ZVj/pL8t80B1I/BHy+DLNfYriPSJIdgOfT9OCfSNNzf8N4q5K6o73q0sHAq4H9aD4hn1JV3xlrYfOE4T4CST4G7A98ATgDOKeq/IWqNCJJng58BNgRuJTmwisXjreq8TLcRyDJcuCLVXVP38aSNkt75aWXA68AfkhzScvVNFdn+nhV7TnG8sbOb8uMxnnAm5KsAkiyV5LnjbkmqWsuBB4C/F5VPbeqPlVVm6pqLXDSmGsbO3vuI9AOy1wCvLKqHtuOv184+aoxkjZfkngQdXr+QnU0fr2qXppkBUBV/bg9+CNpePZK8kZgKT1Z5vmAGob7aNzd9tYnTvn76/ScaVDSUHycZvjlg/Scg18Nw300jqP5pszuSf4NeCrwqrFWJHXPpqp6/7iLmK8ccx+ydvhlMfAj4Ck0Jw27qKpuHGthUke0Z10FOBrYyH3Pwe+vVjHcRyLJJVX1pHHXIXVRkmtohjwnjmP9Uoh53duGwzKjcVGSJ1fVxeMuROqaie+vt8e1XgccQBPw5+NXIH/BnvsIJLkCeBTwPeAu7r2qj6cilYYkyRnA7cC/tbNWADtX1UvGV9X8YbiPwKSTUP1CVX1/S9cidVWSS6vq8f3mba38heoItCG+O3BQe/tH+FhLw/bNJE+ZmEiyP/DVMdYzr9hzH4EkxwHLgEdV1d5JdqU518VTx1ya1BlJrqQZ/ry2nbWE5tKIP8dhUA+ojsgLgH2BbwBU1XVJdhpvSVLnLB93AfOZ4T4ad1dVJZn4heqO4y5I6hqPYc3MceDROCPJB4Cdk7wW+Hfgn8dck6StiGPuI5LkmTRXZg+wpqq+OOaSJG1FDHdJ6iDH3IcoyR1M+il0Ly+QLWlLMdyHqKp2AkhyAnA9cCrNsMwRgN+WkbTFOCwzAkm+VlX795snSaPit2VG454kRyRZkGSbJEfgxQQkbUGG+2i8DHgJzRXZbwBe3M6TpC3CYRlJ6iB77iOQZHGSTye5IckPk3wyyeJx1yVp62G4j8aHgNXArsBuwOfaeZK0RTgsMwJJvlVVT+g3T5JGxZ77aNyY5OXtt2UWJHk5cNO4i5K09bDnPgJJlgDvA36L5herFwBHV9W1M95RkobEcB+BJP8KHFNVt7TTDwf+tqpePd7KJG0tHJYZjcdNBDtAVd1Mc/EOSdoiDPfR2CbJwyYm2p675/GRtMUYOKPxHuCCJJ+gGXN/CfD28ZYkaWvimPuIJNkHOIjmrJD/UVVXjLkkSVsRw12SOsgxd0nqIMNdkjrIcJekDjLcJamDDHdJ6qD/DyFaoJO3c7keAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## empty dictionary of df skeleton\n",
    "plot_data = {}\n",
    "\n",
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## total number of answers per post of fora\n",
    "for i in data_array:\n",
    "    plot_data[i] = posts_dfs[i].groupBy().avg('_AnswerCount').collect()[0][0]\n",
    "\n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") # hm96;2xhm16 - 24 minutes codereview, physics, unix\n",
    "    \n",
    "## bar plot of answers per post in descending order\n",
    "import operator\n",
    "plot_list = sorted(plot_data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "x, y = zip(*plot_list) # unpack a list of pairs into two tuples\n",
    "plt.bar(x, y, align='center', alpha=.8)\n",
    "plt.xticks(range(len(plot_list)), list([i[0] for i in plot_list]), rotation=90)\n",
    "plt.title('Average number of answers per post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:02:21.688652\n"
     ]
    }
   ],
   "source": [
    "## empty dictionary of df skeleton\n",
    "plot_data = {}\n",
    "\n",
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## total number of comments per post of fora\n",
    "for i in data_array:\n",
    "    plot_data[i] = posts_dfs[i].groupBy().avg('_CommentCount').collect()[0][0]\n",
    "\n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") #5 workers - 73 minutesseconds\n",
    "    \n",
    "## bar plot of comments per post in descending order\n",
    "import operator\n",
    "plot_list = sorted(plot_data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "x, y = zip(*plot_list) # unpack a list of pairs into two tuples\n",
    "plt.bar(x, y, align='center', alpha=.8)\n",
    "plt.xticks(range(len(plot_list)), list([i[0] for i in plot_list]), rotation=90)\n",
    "plt.title('Average number of comments per post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## empty dictionary of df skeleton\n",
    "plot_data = {}\n",
    "\n",
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## average score per post across fora\n",
    "for i in data_array:\n",
    "    plot_data[i] = posts_dfs[i].groupBy().avg('_Score').collect()[0][0]\n",
    "\n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") #5 workers - 74 minutes\n",
    "    \n",
    "## bar plot of score per post in descending order\n",
    "import operator\n",
    "plot_list = sorted(plot_data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "x, y = zip(*plot_list) # unpack a list of pairs into two tuples\n",
    "plt.bar(x, y, align='center', alpha=.8)\n",
    "plt.xticks(range(len(plot_list)), list([i[0] for i in plot_list]), rotation=90)\n",
    "plt.title('Average score per post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## empty dictionary of df skeleton\n",
    "plot_data = {}\n",
    "\n",
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## average viewcount per post across fora\n",
    "for i in data_array:\n",
    "    plot_data[i] = posts_dfs[i].groupBy().avg('_ViewCount').collect()[0][0]\n",
    "\n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") #5 workers - 75 minutesseconds\n",
    "    \n",
    "## bar plot of viewcount per post in descending order\n",
    "import operator\n",
    "plot_list = sorted(plot_data.items(), key=operator.itemgetter(1), reverse=True)\n",
    "x, y = zip(*plot_list) # unpack a list of pairs into two tuples\n",
    "plt.bar(x, y, align='center', alpha=.8)\n",
    "plt.xticks(range(len(plot_list)), list([i[0] for i in plot_list]), rotation=90)\n",
    "plt.title('Viewcount per post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start time\n",
    "t0 = time.time()\n",
    "print(datetime.now().time())\n",
    "\n",
    "## create plot data for cumulative viewcount\n",
    "plot_data = {}\n",
    "for i in data_array:\n",
    "    plot_data[i] = posts_dfs[i].select('_ViewCount').rdd.flatMap(lambda x: x).collect()\n",
    "    plot_data[i] = [x for x in plot_data[i] if x is not None]\n",
    "    \n",
    "## end time\n",
    "T = time.time() - t0;\n",
    "print(\"Time taken:\", round(T/60), \"minutes.\") #5 workers - 73 minutesseconds\n",
    "\n",
    "## plot cumulative distribution of viewcount across fora\n",
    "n_bins = 500000\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "for i in data_array:\n",
    "    n, bins, patches = ax.hist(plot_data[i], n_bins, density=True, histtype='step',\n",
    "                               cumulative=True, label=i)\n",
    "ax.grid(True)\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='right')\n",
    "ax.set_title('Cumulative step histograms')\n",
    "ax.set_xlabel('ViewCount')\n",
    "ax.set_ylabel('Cumulative percentage of question posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_data = {}\n",
    "for i in data_array:\n",
    "    body_data[i] = posts_dfs[i].select(\"_Body\").rdd.flatMap(lambda r: r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_data['expatriates'].take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise and lemmatise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_rdd = {}\n",
    "for i in data_array:\n",
    "    body_rdd[i] = body_data[i].map(lambda line: (1, get_tokens(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_rdd['expatriates'].take(1)[0][1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WHAT IF DIFFERENT THRESHOLDS ??? NEED number of tokens per fora\n",
    "\n",
    "doc_stop_words = {}\n",
    "for i in data_array:\n",
    "    doc_stop_words[i] = body_rdd[i].flatMap(lambda r: r[1]).map(lambda r: (r,1)).reduceByKey(lambda a,b: a+b)\n",
    "    # here we assume that words that appear very frequently are stop words. We use 3000 as a threshold.\n",
    "    doc_stop_words[i] = doc_stop_words[i].filter(lambda a: a[1]>3000).map(lambda r: r[0]).collect()\n",
    "    body_rdd[i] = body_rdd[i].map(lambda r: (r[0],[w for w in r[1] if not w in doc_stop_words[i] and not len(w)==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body_rdd['expatriates'].collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens into sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "body_df = {}\n",
    "for i in data_array:\n",
    "    body_df[i] = spark.createDataFrame(body_rdd[i], [\"dummy\",\"words\"])\n",
    "    body_df[i].cache()\n",
    "\n",
    "body_df['expatriates'].take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we generate vectors that are histograms of word counts associated with each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvcr = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\n",
    "\n",
    "cntvcr_models = {}\n",
    "word_feat_list = {}\n",
    "for i in data_array:\n",
    "    cntvcr_models[i] = cntvcr.fit(body_df[i])\n",
    "    word_feat_list[i] = cntvcr_models[i].transform(body_df[i])\n",
    "    word_feat_list[i].cache()\n",
    "    \n",
    "word_feat_list['expatriates'].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pyspark.ml vectors to pyspark.mllib vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml import linalg as ml_linalg\n",
    "def as_mllib_vector(v):\n",
    "    return Vectors.sparse(v.size, v.indices, v.values)\n",
    "\n",
    "features = {}\n",
    "feature_vec_list = {}\n",
    "for i in data_array:\n",
    "    features[i] = word_feat_list[i].select(\"features\")\n",
    "    feature_vec_list[i] = features[i].rdd.map(lambda r: as_mllib_vector(r[0]))\n",
    "    feature_vec_list[i].cache()\n",
    "\n",
    "feature_vec_list['expatriates'].take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Vocabulary from CountVectorizerModel is:\\n\")\n",
    "print(cntvcr_models['expatriates'].vocabulary[:100])\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "M = len(cntvcr_models['expatriates'].vocabulary)\n",
    "print(\"Number of terms M = \", M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "feature_mat_list = {}\n",
    "for i in data_array:\n",
    "    feature_mat_list[i] = RowMatrix(feature_vec_list[i])\n",
    "\n",
    "# Here we use the dimension of latent factors to be k\n",
    "k = 5\n",
    "feature_svd_list = {}\n",
    "U_list = {}\n",
    "s_list = {}\n",
    "V_list = {}\n",
    "for i in data_array:\n",
    "    feature_svd_list[i] = feature_mat_list[i].computeSVD(5, computeU=True)\n",
    "    U_list[i] = feature_svd_list[i].U       # The U factor is a RowMatrix.\n",
    "    s_list[i] = feature_svd_list[i].s       # The singular values are stored in a local dense vector.\n",
    "    V_list[i] = feature_svd_list[i].V       # The V factor is a local dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of rows of U\", U_list['expatriates'].numRows()) # this is the number of documents (N)\n",
    "print(\"number of rows of V\", V_list['expatriates'].numRows ) # this is the number of terms (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = U_list['expatriates'].rows.take(10)\n",
    "\n",
    "print(\"U factors for expatriate forum are:\")\n",
    "for vector in collected:\n",
    "    print(vector)\n",
    "    \n",
    "print(\"Singular values are: \\n\", s_list['expatriates'])\n",
    "\n",
    "print(\"V factors are: \\n\", V_list['expatriates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "collected_list = {}\n",
    "Uarray_list = {}\n",
    "Uembedded_list = {}\n",
    "\n",
    "for i in data_array:\n",
    "    collected_list[i] = U_list[i].rows.collect()\n",
    "    Uarray_list[i] = np.array([[x[i] for i in range(k)] for x in collected_list[i]]) # unpack strange spark vectors\n",
    "    t_sne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='random')\n",
    "    Uembedded_list[i] = t_sne_model.fit_transform(Uarray_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=6, figsize=(16, 8))\n",
    "\n",
    "j = 1\n",
    "for i in data_array:\n",
    "    plt.subplot(2, 6, j)\n",
    "    plt.scatter(Uembedded_list[i][:,0],Uembedded_list[i][:,1], color=\"bgrcmykbgrcmyk\"[j], alpha=0.1)\n",
    "    j = j+1\n",
    "#    np.shape(Uembedded_list[i])\n",
    "    #plt.scatter(Uembedded_list[i][:,0],Uembedded_list[i][:,1],alpha=.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=6, figsize=(16, 8))\n",
    "\n",
    "j = 1\n",
    "for i in data_array:\n",
    "    plt.subplot(2, 6, j)\n",
    "    plt.scatter(Uarray_list[i][:,0],Uarray_list[i][:,1],alpha=.1)\n",
    "    plt.scatter(Uarray_list[i][:,0],Uarray_list[i][:,2],alpha=.1)\n",
    "    plt.scatter(Uarray_list[i][:,0],Uarray_list[i][:,3],alpha=.1)\n",
    "    j = j+1\n",
    "#    np.shape(Uembedded_list[i])\n",
    "    #plt.scatter(Uembedded_list[i][:,0],Uembedded_list[i][:,1],alpha=.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=6, figsize=(16, 8))\n",
    "\n",
    "j = 1\n",
    "for i in data_array:\n",
    "    plt.subplot(2, 6, j)\n",
    "    plt.scatter(Uarray_list[i][:,1],Uarray_list[i][:,2],alpha=.1)\n",
    "    plt.scatter(Uarray_list[i][:,1],Uarray_list[i][:,3],alpha=.1)\n",
    "    j = j+1\n",
    "#    np.shape(Uembedded_list[i])\n",
    "    #plt.scatter(Uembedded_list[i][:,0],Uembedded_list[i][:,1],alpha=.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not true that few messages account for a large fraction of variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we gotta choose a viewcount threshold to address possibility of users that can vote not seeing question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we gotta choose the threshold for the binary labelling of question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=5, maxIter=5)\n",
    "\n",
    "lda_model_list = {}\n",
    "for i in data_array:\n",
    "    lda_model_list[i] = lda.fit(word_feat_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglik_list = {}\n",
    "logper_list = {}\n",
    "for i in data_array:\n",
    "    loglik_list[i] = lda_model_list[i].logLikelihood(word_feat_list[i])\n",
    "    logper_list[i] = lda_model_list[i].logPerplexity(word_feat_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_array:\n",
    "    print(\"The lower bound on the log likelihood of the \" +'\\033[1m'+ i +'\\033[0m'+ \" corpus is: \" + str(loglik_list[i]))\n",
    "    print(\"The upper bound on the perplexity of the \" +'\\033[1m'+ i +'\\033[0m'+ \" corpus is: \" + str(logper_list[i]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE HTML STUFF!\n",
    "\n",
    "# describe topics\n",
    "topic_list = {}\n",
    "for i in data_array:\n",
    "    print(i)\n",
    "    topic_list[i] = lda_model_list[i].describeTopics(5)\n",
    "    #print(\"The topics described by their top-weighted terms:\\n\")\n",
    "    #topic_list[i].show(truncate=False)\n",
    "    # show the results\n",
    "    topic_j = topic_list[i].select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "    for j in topic_j:\n",
    "        print(np.array(cntvcr_models[i].vocabulary)[j])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
