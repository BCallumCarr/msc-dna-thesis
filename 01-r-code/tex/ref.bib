@misc{Gervais2015,
abstract = {ABSTRACTWith the advances in interpersonal communication of the ``Web 2.0'' era, questions about the importance of civility are perhaps more important than ever. Mass digital interaction between strangers has become an everyday occurrence, bound by few behavioral norms. I argue that the widespread presence of incivility in online political communication limits the deliberative potential of online interactions. To test this hypothesis, I manipulate exposure to uncivil political discourse in an online discussion forum. I find that exposure to disagreeable uncivil political talk induces feelings of anger and aversion, which in turn reduces satisfaction with the message board discourse. On the other hand, exposure to like-minded incivility increases the use of uncivil behavior in political comments by message board posters. Notably, these effects mainly occur when histrionic, emotional incivility is present. I discuss why like-minded and disagreeable incivility have different effects, and reflect on what the presence of incivility means for online political discourse.},
author = {Gervais, Bryan T.},
booktitle = {Journal of Information Technology and Politics},
doi = {10.1080/19331681.2014.997416},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/Incivility Online - Affective and Behavioral Reactions to Uncivil Political Posts in a Web based Experiment.pdf:pdf},
issn = {1933169X},
keywords = {Affective intelligence,incivility,online discourse,political deliberation,political discourse},
number = {2},
pages = {167--185},
title = {{Incivility Online: Affective and Behavioral Reactions to Uncivil Political Posts in a Web-based Experiment}},
volume = {12},
year = {2015}
}
@inproceedings{Sung2013,
abstract = {Community-based question answering(CQA) services such asYahoo! Answers have been widely used by Internet users to get the answers for their inquiries. The CQA services totally rely on the contributions by the users. However, it is known that newcomers are prone to lose their interests and leave the communities. Thus, finding expert users in an early phase when they are still active is essential to improve the chances of motivating them to contribute to the communities further. In this paper, we propose a novel approach to discovering “potentially” contributive users from recently-joined users in CQAservices. The likelihood of becoming a contributive user is defined by the user's expertise as well as availability, which we call the answer affordance. The main technical difficulty lies in the fact that such recently-joined users do not have abundant information accumulated for many years. We uti- lize a user's productive vocabulary to mitigate the lack of available information since the vocabulary is the most fun- damental element that reveals his/her knowledge. Extensive experiments were conducted with a huge data set of Naver Knowledge-In (KiN), which is the dominating CQA service in Korea.We demonstrate that the top rankers selected by the answer affordance outperformed those by KiN in terms of the amount of answering activity. Introduction},
author = {Sung, Juyup and Lee, Jae-gil and Lee, Uichin},
booktitle = {Seventh International AAAI Conference on Weblogs and Social Media},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/sung-contributive-users.pdf:pdf},
keywords = {community based question answering,heavy users,light users,social services},
pages = {602--610},
title = {{Booming Up the Long Tails: Discovering Potentially Contributive Users in Community-Based Question Answering Services}},
year = {2013}
}
@inproceedings{Qu2009,
abstract = {User-InteractiveQuestion Answering (QA) communities such as Yahoo! Answers are growing in popularity. However, as these QA sites always have thousands of new questions posted daily, it is difficult for users to find the questions that are of interest to them. Consequently, this may delay the an- swering of the new questions. This gives rise to question rec- ommendation techniques that help users locate interesting questions. In this paper, we adopt the Probabilistic Latent Semantic Analysis (PLSA) model for question recommenda- tion and propose a novel metric to evaluate the performance of our approach. The experimental results show our recom- mendation approach is effective.},
author = {Qu, Mingcheng and Qiu, Guang and He, Xiaofei and Zhang, Cheng and Wu, Hao and Bu, Jiajun and Chen, Chun},
booktitle = {Proceedings of the 18th international conference on World wide web},
doi = {10.1145/1526709.1526942},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Qu-recommendation.pdf:pdf},
isbn = {9781605584874},
keywords = {plsa,question answering,question recommendation},
number = {2},
pages = {1229--1230},
publisher = {ACM},
title = {{Probabilistic question recommendation for question answering communities}},
year = {2009}
}
@inproceedings{Li2011,
abstract = {This paper investigates a ground-breaking incorporation of question category to Question Routing (QR) in Commu- nity Question Answering (CQA) services. The incorpora- tion of question category was designed to estimate answerer expertise for routing questions to potential answerers. Two category-sensitive Language Models (LMs) were developed with large-scale real world data sets being experimented. Results demonstrated that higher accuracies of routing ques- tions with lower computational costs were achieved, relative to traditional Query Likelihood LM (QLLM), state-of-the- art Cluster-Based LM (CBLM) and the mixture of Latent Dirichlet Allocation and QLLM (LDALM).},
author = {Li, Baichuan and King, Irwin and Lyu, Michael R.},
booktitle = {Proceedings of the 20th ACM international conference on Information and knowledge management},
doi = {10.1145/2063576.2063885},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Li-King-Lyu-Routing.pdf:pdf},
isbn = {9781450307178},
keywords = {category,category-sensitive language model,community question answering,question,question routing},
pages = {2041--2044},
publisher = {ACM},
title = {{Question routing in community question answering}},
year = {2011}
}
@inproceedings{Li2010,
abstract = {Community Question Answering (CQA) service provides a platform for increasing number of users to ask and answer for their own needs but unanswered questions still exist within a fixed period. To address this, the paper aims to route questions to the right answerers who have a top rank in accordance of their previous answering performance. In order to rank the answerers, we propose a framework called Question Routing (QR) which consists of four phases: (1) performance profiling, (2) expertise estimation, (3) availability estimation, and (4) answerer ranking. Applying the framework, we conduct experiments with Yahoo! Answers dataset and the results demonstrate that on average each of 1,713 testing questions obtains at least one answer if it is routed to the top 20 ranked answerers.},
author = {Li, Baichuan and King, Irwin},
booktitle = {Proceedings of the 19th ACM international conference on Information and knowledge management},
doi = {10.1145/1871437.1871678},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Li-King-Routing.pdf:pdf},
isbn = {9781450300995},
keywords = {community question answering,question routing},
pages = {1585--1588},
publisher = {ACM},
title = {{Routing questions to appropriate answerers in community question answering services}},
year = {2010}
}
@inproceedings{Zhou2012,
author = {Zhou, Tom Chao and Lyu, Michael R and King, Irwin},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Zhou-Routing.pdf:pdf},
isbn = {9781450312301},
keywords = {classi-,community question answering,question routing},
pages = {783--790},
publisher = {ACM},
title = {{A classification-based approach to question routing in community question answering}},
url = {http://www2012.wwwconference.org/proceedings/companion/p783.pdf},
year = {2012}
}
@inproceedings{Szpektor2013,
author = {Szpektor, Idan and Maarek, Yoelle and Pelleg, Dan},
booktitle = {Proceedings of the 22nd international conference on World Wide Web},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Szpektor-recommendation.pdf:pdf},
isbn = {9781450320351},
pages = {1249--1260},
publisher = {ACM},
title = {{When relevance is not enough: promoting diversity and freshness in personalized question recommendation}},
year = {2013}
}
@inproceedings{Wu2008,
abstract = {With the fast development of web 2.0, user-centric publishing and knowledge management platforms, such as Wiki, Blogs, and Q {\&} A systems attract a large number of users. Given the availability of the huge amount of meaningful user generated content, incremental model based recommendation techniques can be employed to improve users' experience using automatic recommendations. In this paper, we propose an incremental recommendation algorithm based on Probabilistic Latent Semantic Analysis (PLSA). The proposed algorithm can consider not only the users' long-term and short-term interests, but also users' negative and positive feedback. We compare the proposed method with several baseline methods using a real-world Question {\&} Answer website called Wenda. Experiments demonstrate both the effectiveness and the efficiency of the proposed methods.},
author = {Wu, Hu and Wang, Yongji and Cheng, Xiang},
booktitle = {Proceedings of the 2008 ACM conference on Recommender systems},
doi = {10.1145/1454008.1454026},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Wu-recommendation.pdf:pdf},
isbn = {9781605580937},
keywords = {incremental learning,plsa,recommendation system},
pages = {99},
publisher = {ACM},
title = {{Incremental probabilistic latent semantic analysis for automatic question recommendation}},
year = {2008}
}
@inproceedings{Shah2010,
abstract = {Question answering (QA) helps one go beyond traditional keywords-based querying and retrieve information in more precise form than given by a document or a list of documents. Several community-based QA (CQA) services have emerged allowing information seekers pose their information need as questions and receive answers from their fellow users. A question may receive multiple answers from multiple users and the asker or the community can choose the best answer. While the asker can thus indicate if he was satisfied with the information he received, there is no clear way of evaluating the quality of that information. We present a study to evaluate and predict the quality of an answer in a CQA setting. We chose Yahoo! Answers as such CQA service and selected a small set of questions, each with at least five answers. We asked Amazon Mechanical Turk workers to rate the quality of each answer for a given question based on 13 different criteria. Each answer was rated by five different workers. We then matched their assessments with the actual asker's rating of a given answer. We show that the quality criteria we used faithfully match with asker's perception of a quality answer. We furthered our investigation by extracting various features from questions, answers, and the users who posted them, and training a number of classifiers to select the best answer using those features. We demonstrate a high predictability of our trained models along with the relative merits of each of the features for such prediction. These models support our argument that in case of CQA, contextual information such as a user's profile, can be critical in evaluating and predicting content quality.},
author = {Shah, Chirag and Pomerantz, Jefferey},
booktitle = {Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/1835449.1835518},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Shah-Yahoo-Answer-Quality.pdf:pdf},
isbn = {9781605588964},
keywords = {-based querying and retrieve,a list of documents,allowing information seekers pose,by a document or,cqa,information in more,precise form than given,services have emerged,several community-based qa,their information need as},
number = {March 2008},
pages = {411--418},
publisher = {ACM},
title = {{Evaluating and predicting answer quality in community QA}},
year = {2010}
}
@inproceedings{Chiang2010,
abstract = {We describe a Bayesian inference algorithm that can be used to train any cascade of weighted finite-state transducers on end-to-end data. We also investigate the problem of automatically selecting from among mul-tiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian ap-proaches.},
author = {Chiang, D and Graehl, Jonathan and Knight, Kevin and Pauls, Adam and Ravi, Sujith},
booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/chiang-finite-bayesian.pdf:pdf},
isbn = {1932432655},
number = {June},
pages = {447--455},
publisher = {Association for Computational Linguistics},
title = {{Bayesian Inference for Finite-State Transducers}},
url = {http://www.isi.edu/{~}sravi/pubs/naacl2010{\_}bayes-fst.pdf},
year = {2010}
}
@inproceedings{Jeon2006,
abstract = {New types of document collections are being developed by various web services. The service providers keep track of non-textual features such as click counts. In this paper, we present a framework to use non-textual features to predict the quality of documents. We also show our quality measure can be successfully incorporated into the language modeling-based retrieval model. We test our approach on a collection of question and answer pairs gathered from a community based question answering service where people ask and answer questions. Experimental results using our quality measure show a significant improvement over our baseline.},
author = {Jeon, Jiwoon and Croft, W. Bruce and Lee, Joon Ho and Park, Soyeon},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/1148170.1148212},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Jeon-Predict-Answer-Quality.pdf:pdf},
isbn = {1595933697},
keywords = {document qual-,information retrieval,ity,language models,maximum entropy},
pages = {228--235},
publisher = {ACM},
title = {{A framework to predict the quality of answers with non-textual features}},
year = {2006}
}
@inproceedings{Liu2008,
abstract = {Question answering communities such as Naver and Yahoo! An- swers have emerged as popular, and often effective, means of infor- mation seeking on the web. By posting questions for other partic- ipants to answer, information seekers can obtain specific answers to their questions. Users of popular portals such as Yahoo! An- swers already have submitted millions of questions and received hundreds of millions of answers from other participants. However, it may also take hours –and sometime days– until a satisfactory an- swer is posted. In this paper we introduce the problemof predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question au- thor will be satisfied with the answers submitted by the community participants. We present a general prediction model, and de- velop a variety of content, structure, and community-focused fea- tures for this task. Our experimental results, obtained from a large- scale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satis- faction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question an- swering communities that correlate with information seeker satis- faction. Our models and predictions could be useful for a variety of applications such as user intent inference, answer ranking, interface design, and query suggestion and routing.},
author = {Liu, Yandong and Bian, Jiang and Agichtein, Eugene},
booktitle = {Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/1390334.1390417},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Liu-Predicting Information Seeker Satisfaction in Community Question Answering.pdf:pdf},
isbn = {9781605581644},
keywords = {community question answering,information seeker satisfaction},
number = {Section 2},
pages = {483--490},
publisher = {ACM},
title = {{Predicting information seeker satisfaction in community question answering}},
url = {http://portal.acm.org/citation.cfm?doid=1390334.1390417},
year = {2008}
}
@inproceedings{Riahi2012,
abstract = {Community Question Answering (CQA) websites provide a rapidly growing source of information in many areas. This rapid growth, while offering new opportunities, puts forward new challenges. In most CQA implementations there is little effort in directing new questions to the right group of experts. This means that experts are not provided with questions matching their expertise, and therefore new matching questions may be missed and not receive a proper answer. We focus on finding experts for a newly posted question. We investigate the suitability of two statistical topic models for solving this issue and compare these methods against more traditional Information Retrieval approaches. We show that for a dataset constructed from the Stackoverflow website, these topic models outperform other methods in retrieving a candidate set of best experts for a question. We also show that the Segmented Topic Model gives consistently better performance compared to the Latent Dirichlet Allocation Model.},
author = {Riahi, Fatemeh and Zolaktaf, Zainab and Shafiei, Mahdi and Milios, Evangelos},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
doi = {10.1145/2187980.2188202},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/riahi-expert-users.pdf:pdf},
isbn = {9781450312301},
keywords = {community question answering,expert recommendation,language,model,tf-idf,topic modeling},
pages = {791--798},
publisher = {ACM},
title = {{Finding expert users in community question answering}},
year = {2012}
}
@inproceedings{Shah2018,
abstract = {A matching in a two-sided market often incurs an externality: a matched resource may become unavailable to the other side of the market, at least for a while. This is especially an issue in online platforms involving human experts as the expert resources are often scarce. The efficient utilization of experts in these platforms is made challenging by the fact that the information available about the parties involved is usually limited. To address this challenge, we develop a model of a task-expert matching system where a task is matched to an expert using not only the prior information about the task but also the feedback obtained from the past matches. In our model the tasks arrive online while the experts are fixed and constrained by a finite service capacity. For this model, we characterize the maximum task resolution throughput a platform can achieve. We show that the natural greedy approaches where each expert is assigned a task most suitable to her skill is suboptimal, as it does not internalize the above externality. We develop a throughput optimal backpressure algorithm which does so by accounting for the `congestion' among different task types. Finally, we validate our model and confirm our theoretical findings with data-driven simulations via logs of Math.StackExchange, a StackOverflow forum dedicated to mathematics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00674v3},
author = {Shah, Virag and Gulikers, Lennart and Massouli{\'{e}}, Laurent and Vojnovi{\'{c}}, Milan},
booktitle = {2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
doi = {10.1109/ALLERTON.2017.8262814},
eprint = {arXiv:1703.00674v3},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Shah-Milan.pdf:pdf},
isbn = {9781538632666},
pages = {753--760},
publisher = {IEEE},
title = {{Adaptive matching for expert systems with uncertain task types}},
year = {2018}
}
@inproceedings{Tian2013,
abstract = {Community-based question-answering (CQA) services contribute to solving many difficult questions we have. For each question in such services, one best answer can be designated, among all answers, often by the asker. However, many questions on typical CQA sites are left without a best answer even if when good candidates are available. In this paper, we attempt to address the prob-lem of predicting if an answer may be selected as the best answer, based on learning from labeled data. The key tasks include designing features measuring impor-tant aspects of an answer and identifying the most im-portance features. Experiments with a Stack Overflow dataset show that the contextual information among the answers should be the most important factor to consider.},
author = {Tian, Qiongjie and Zhang, Peng and Li, Baoxin},
booktitle = {Seventh International AAAI Conference on Weblogs and Social Media},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Tian-Predicting-Best-Answers-SO.pdf:pdf},
keywords = {Short Paper},
pages = {725--728},
title = {{Towards Predicting the Best Answers in Community-Based Question-Answering Services}},
year = {2013}
}
@inproceedings{Agichtein2008,
abstract = {The quality of user-generated content varies drastically from excellent to abuse and spam. As the availability of such content increases, the task of identifying high-quality content in sites based on user contributions—social media sites— becomes increasingly important. Social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. In this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. As a test case, we focus on Yahoo! Answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. We introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. In particular, for the community question/answering domain, we show that our system is able to separate high-quality items from the rest with an accuracy close to that of humans.},
author = {Agichtein, E. and Castillo, C. and Donato, D. and Gionis, A. and Mishne, G.},
booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
doi = {10.1145/1341531.1341557},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/agichtein.pdf:pdf},
isbn = {978-1-59593-927-2},
issn = {15435008},
keywords = {Community Question Answering,Social media,User I},
pages = {183--194},
pmid = {11},
publisher = {ACM},
title = {{Finding high-quality content in social media}},
url = {http://dl.acm.org/citation.cfm?id=1341557{\&}CFID=258789117{\&}CFTOKEN=24893236},
year = {2008}
}
@inproceedings{Allamanis2013,
author = {Allamanis, Miltiadis and Sutton, Charles},
booktitle = {2013 10th Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2013.6624004},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/00-why-when-what-allamanis.pdf:pdf},
isbn = {9781467329361},
issn = {21601852},
pages = {53--56},
publisher = {IEEE},
title = {{Why, when, and what: Analyzing stack overflow questions by topic, type, and code}},
year = {2013}
}
@inproceedings{Bian2009,
abstract = {Community Question Answering (CQA) has emerged as a popular forum for users to pose questions for other users to answer. Over the last few years, CQA portals such as Naver and Yahoo! Answers have exploded in popularity, and now provide a viable alternative to general purpose Web search. At the same time, the answers to past questions submit- ted in CQA sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering. Unfortunately, the quality of the submitted questions and answers varies widely - in- creasingly so that a large fraction of the content is not usable for answering queries. Previous approaches for retrieving relevant and high quality content have been proposed, but they require large amounts of manually labeled data – which limits the applicability of the supervised approaches to new sites and domains. In this paper we address this problem by developing a semi-supervised coupled mutual reinforce- ment framework for simultaneously calculating content qual- ity and user reputation, that requires relatively few labeled examples to initialize the training process. Results of a large scale evaluation demonstrate that our methods are more ef- fective than previous approaches for finding high-quality an- swers, questions, and users. More importantly, our quality estimation significantly improves the accuracy of search over CQA archives over the state-of-the-art methods.},
author = {Bian, Jiang and Liu, Yandong and Zhou, Ding and Agichtein, Eugene and Zha, Hongyuan},
booktitle = {Proceedings of the 18th international conference on World wide web},
doi = {10.1145/1526709.1526717},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/bian.pdf:pdf},
isbn = {9781605584874},
keywords = {authority and expertise,community question answering,copyright is held by,in online communities,the international world wide,web conference com-},
pages = {51--60},
publisher = {ACM},
title = {{Learning to recognize reliable users and content in social media with coupled mutual reinforcement}},
year = {2009}
}
@inproceedings{Anderson2012,
abstract = {Question answering (Q{\&}A) websites are now large repositories of valuable knowledge. While most Q{\&}A sites were initially aimed at providing useful answers to the question asker, there has been a marked shift towards question answering as a community-driven knowledge creation process whose end product can be of enduring value to a broad audience. As part of this shift, specific expertise and deep knowledge of the subject at hand have become increasingly important, and many Q{\&}A sites employ voting and reputation mechanisms as centerpieces of their design to help users identify the trustworthiness and accuracy of the content. To better understand this shift in focus from one-off answers to a group knowledge-creation process, we consider a question together with its entire set of corresponding answers as our fundamental unit of analysis, in contrast with the focus on individual questionanswer pairs that characterized previous work. Our investigation considers the dynamics of the community activity that shapes the set of answers, both how answers and voters arrive over time and how this influences the eventual outcome. For example, we observe significant assortativity in the reputations of co-answerers, relationships between reputation and answer speed, and that the probability of an answer being chosen as the best one strongly depends on temporal characteristics of answer arrivals. We then show that our understanding of such properties is naturally applicable to predicting several important quantities, including the long-term value of the question and its answers, as well as whether a question requires a better answer. Finally, we discuss the implications of these results for the design of Q{\&}A sites.},
author = {Anderson, A. and Huttenlocher, D. and Kleinberg, J. and Leskovec, J.},
booktitle = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/anderson.pdf:pdf},
isbn = {9781450314626},
keywords = {question-answering,reputation,value prediction},
pages = {850--858},
publisher = {ACM},
title = {{Discovering value from community activity on focused question answering sites: a case study of stack overflow}},
url = {http://dl.acm.org/citation.cfm?id=2339665},
year = {2012}
}
@inproceedings{Li2012,
abstract = {Users tend to ask and answer questions in community question answering (CQA) services to seek information and share knowledge. A corollary is that myriad of questions and answers appear in CQA service. Accordingly, volumes of studies have been taken to explore the answer quality so as to provide a preliminary screening for better answers. However, to our knowledge, less attention has so far been paid to question quality in CQA. Knowing question quality provides us with finding and recommending good questions together with identifying bad ones which hinder the CQA service. In this paper, we are conducting two studies to investigate the question quality issue. The first study analyzes the factors of question quality and finds that the interaction between askers and topics results in the differences of question quality. Based on this finding, in the second study we propose a Mutual Reinforcement-based Label Propagation (MRLP) algorithm to predict question quality. We experiment with Yahoo!{\~{}}Answers data and the results demonstrate the effectiveness of our algorithm in distinguishing high-quality questions from low-quality ones.},
author = {Li, Baichuan and Jin, Tan and Lyu, Michael R. and King, Irwin and Mak, Barley},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
doi = {10.1145/2187980.2188200},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Analyzing and Predicting Question Quality in Community Question Answering Services.pdf:pdf},
isbn = {9781450312301},
keywords = {analy-,community question answering,question quality},
pages = {775--782},
publisher = {ACM},
title = {{Analyzing and predicting question quality in community question answering services}},
year = {2012}
}
@article{Liu2017,
abstract = {Community question answering services (CQAS) (e.g., Yahoo! Answers)
provides a platform where people post questions and answer questions
posed by others. Previous works analyzed the answer quality (AQ) based
on answer-related features, but neglect the question-related features on
AQ. Previous work analyzed how asker-and question-related features
affect the question quality (QQ) regarding the amount of attention from
users, the number of answers and the question solving latency, but
neglect the correlation between QQ and AQ (measured by the rating of the
best answer), which is critical to quality of service (QoS). We handle
this problem from two aspects. First, we additionally use QQ in
measuring AQ, and analyze the correlation between a comprehensive list
of features (including answer-related features) and QQ. Second, we
propose the first method that estimates the probability for a given
question to obtain high AQ. Our analysis on the Yahoo! Answers trace
confirmed that the list of our identified features exert influence on
AQ, which determines QQ. For the correlation analysis, the previous
classification algorithms cannot consider the mutual interactions
between multiple ({\textgreater}2) classes of features. We then propose a novel
Coupled Semi-Supervised Mutual Reinforcement-based Label Propagation
(CSMRLP) algorithm for this purpose. Our extensive experiments show that
CSMRLP outperforms the Mutual Reinforcement-based Label Propagation
(MRLP) and five other traditional classification algorithms in the
accuracy of AQ classification, and the effectiveness of our proposed
method in AQ prediction. Finally, we provide suggestions on how to
create a question that will receive high AQ, which can be exploited to
improve the QoS of CQAS.},
author = {Liu, Jinwei and Shen, Haiying and Yu, Lei},
doi = {10.1109/TSC.2015.2446991},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Question Quality Analysis and Prediction in Community Question Answering Services with Coupled Mutual Reinforcement.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Answer quality,Classification,Community question answering,Prediction,Question quality},
number = {2},
pages = {286--301},
publisher = {IEEE},
title = {{Question quality analysis and prediction in community question answering services with coupled mutual reinforcement}},
volume = {10},
year = {2017}
}
@article{Baltadzhieva2015,
abstract = {Community Question Answering websites (CQA) offer a new opportunity for users to provide, search and share knowl-edge. Although the idea of receiving a direct, targeted re-sponse to a question sounds very attractive, the quality of the question itself can have an important effect on the like-lihood of getting useful answers. High quality questions im-prove the CQA experience and therefore it is essential for CQA forums to better understand what characterizes ques-tions that are more appealing for the forum community. In this survey, we review existing research on question quality in CQA websites. We discuss the possible measures of ques-tion quality and the question features that have been shown to influence question quality.},
author = {Baltadzhieva, Antoaneta and Chrupa{\l}a, Grzegorz},
doi = {10.1145/2830544.2830547},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Question Quality in Community Question Answering Forums.pdf:pdf},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
keywords = {community question answering,question,question quality},
number = {1},
pages = {8--13},
title = {{Question Quality in Community Question Answering Forums}},
volume = {17},
year = {2015}
}
@article{King1994,
abstract = {SOCIAL SCIENCE RESEARCH, whether quantitative or qualitative, in-volves the dual goals of describing and explaining. Some scholars set out to describe the world; others to explain. Each is essential. We can-not construct meaningful causal explanations without good descrip-tion; description, in turn, loses most of its interest unless linked to some causal relationships. Description often comes first; it is hard to develop explanations before we know something about the world and what needs to be explained on the basis of what characteristics. But the relationship between description and explanation is interactive. Some-times our explanations lead us to look for descriptions of different parts of the world; conversely, our descriptions may lead to new causal explanations. Description and explanation both depend upon rules of scientific inference. In this chapter we focus on description and descriptive in-ference. Description is far from mechanical or unproblematic since it involves selection from the infinite number of facts that could be re-corded. There are several fundamental aspects of scientific description. One is that it involves inference: part of the descriptive task is to infer information about unobserved facts from the facts we have observed. Another aspect involves distinguishing between that which is system-atic about the observed facts and that which is nonsystematic. As should be clear, we disagree with those who denigrate " mere " description. Even if explanation—connecting causes and effects—is the ultimate goal, description has a central role in all explanation, and it is fundamentally important in and of itself. It is not description ver-sus explanation that distinguishes scientific research from other re-search; it is whether systematic inference is conducted according to valid procedures. Inference, whether descriptive or causal, quantita-tive or qualitative, is the ultimate goal of all good social science. Sys-tematically collecting facts is a very important endeavor without which science would not be possible but which does not by itself con-stitute science. Good archival work or well-done summaries of histori-cal facts may make good descriptive history, but neither are sufficient to constitute social science. In this chapter, we distinguish description—the collection of facts— from descriptive inference. In section 2.1 we discuss the relationship General Knowledge and Particular Facts {\textperiodcentered} 35 between the seemingly contradictory goals of scholarship: discovering general knowledge and learning about particular facts. We are then able to explain in more detail the concept of inference in section 2.2. Our approach in the remainder of the book is to present ideas both verbally and through very simple algebraic models of research. In section 2.3 we consider the nature of these models. We then discuss models for data collection, for summarizing historical detail, and for descriptive inference in sections 2.4, 2.5, and 2.6, respectively. Finally, we provide some specific criteria for judging descriptive inferences in section 2.7. 2.1 GENERAL KNOWLEDGE AND PARTICULAR FACTS},
author = {King, Gary and Keohane, Robert O. and Verba, Sidney},
doi = {10.1007/978-3-531-92076-4},
file = {:Users/brad/Library/Application Support/Mendeley Desktop/Downloaded/King, Keohane, Verba - 1994 - Descriptive Inference.pdf:pdf},
isbn = {0691034702},
issn = {9780691034713},
journal = {Designing Social Inquiry Scientific Inference in Qualitative Research},
pages = {260},
title = {{Descriptive Inference}},
year = {1994}
}
@article{Darley2010,
abstract = {We investigate lay intuitions about the appropriate compensatory and retributive consequences of a wrongdoer putting another in harm's way when harm either does or does not result. Compensation tracked whether the harm actually occurred, though when harm has not yet occurred but might, participants prefer an escrow-like solution in which money will be available to the victim only if the risk matures into actual harm. Retributive sanctions (punitive damages, fines, prison terms) were largely unaffected by whether the harm materialized but were instead sensitive to whether the wrongdoer exhibited negligent or reckless conduct. Thus, subjects clearly differentiated between the retributive nature of punitive sanctions and the compensatory nature of restorative damages. Finally, subjects often assigned liability to the actor even when the risk-causing actions were not negligent—and in this way preferred a strict liability stance more than does the current legal doctrine.},
author = {Darley, John M. and Solan, Lawrence M. and Kugler, Matthew B. and Sanders, Joseph},
doi = {10.1111/j.1740-1461.2009.01169.x},
file = {:Users/brad/Library/Application Support/Mendeley Desktop/Downloaded/Darley et al. - 2010 - Doing Wrong Without Creating Harm.pdf:pdf},
issn = {17401453},
journal = {Journal of Empirical Legal Studies},
number = {1},
pages = {30--63},
title = {{Doing Wrong Without Creating Harm}},
url = {http://doi.wiley.com/10.1111/j.1740-1461.2009.01169.x},
volume = {7},
year = {2010}
}
@article{Inference1984,
author = {Inference, Descriptive},
file = {:Users/brad/Library/Application Support/Mendeley Desktop/Downloaded/Inference - 1984 - Causality and Causal Inference (2.6).pdf:pdf},
isbn = {9781400821211},
title = {{Causality and Causal Inference (2.6)}},
year = {1984}
}
@article{Brinton2014,
abstract = {We study user behavior in the courses offered by a major Massive Online Open Course (MOOC) provider during the summer of 2013. Since social learning is a key element of scalable education in MOOCs and is done via online discussion forums, our main focus is in understanding forum activities. Two salient features of MOOC forum activities drive our research: 1. High decline rate: for all courses studied, the volume of discussions in the forum declines continuously throughout the duration of the course. 2. High-volume, noisy discussions: at least 30{\%} of the courses produce new discussion threads at rates that are infeasible for students or teaching staff to read through. Furthermore, a substantial portion of the discussions are not directly course-related. We investigate factors that correlate with the decline of activity in the online discussion forums and find effective strategies to classify threads and rank their relevance. Specifically, we use linear regression models to analyze the time series of the count data for the forum activities and make a number of observations, e.g., the teaching staff's active participation in the discussion increases the discussion volume but does not slow down the decline rate. We then propose a unified generative model for the discussion threads, which allows us both to choose efficient thread classifiers and design an effective algorithm for ranking thread relevance. Our ranking algorithm is further compared against two baseline algorithms, using human evaluation from Amazon Mechanical Turk. The authors on this paper are listed in alphabetical order. For media and press coverage, please refer to us collectively, as "researchers from the EDGE Lab at Princeton University, together with collaborators at Boston University and Microsoft Corporation."},
author = {Brinton, Christopher G. and Chiang, Mung and Jain, Shaili and Lam, Henry and Liu, Zhenming and Wong, Felix Ming Fai},
doi = {10.1109/TLT.2014.2337900},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/Learning about social learning in MOOCs From Statistical Analysis to Generative Model.pdf:pdf},
issn = {19391382},
journal = {IEEE Transactions on Learning Technologies},
keywords = {MOOC,concept learning,data mining,regression,social learning networks},
number = {4},
pages = {346--359},
publisher = {IEEE},
title = {{Learning about social learning in MOOCs: From statistical analysis to generative model}},
volume = {7},
year = {2014}
}
@article{Berry2017,
abstract = {Studies of online social influence have demonstrated that friends have important effects on many types of behavior in a wide variety of settings. However, we know much less about how influence works among relative strangers in digital public squares, despite important conversations happening in such spaces. We present the results of a study on large public Facebook pages where we randomly used two different methods--most recent and social feedback--to order comments on posts. We find that the social feedback condition results in higher quality viewed comments and response comments. After measuring the average quality of comments written by users before the study, we find that social feedback has a positive effect on response quality for both low and high quality commenters. We draw on a theoretical framework of social norms to explain this empirical result. In order to examine the influence mechanism further, we measure the similarity between comments viewed and written during the study, finding that similarity increases for the highest quality contributors under the social feedback condition. This suggests that, in addition to norms, some individuals may respond with increased relevance to high-quality comments.},
archivePrefix = {arXiv},
arxivId = {1702.06677},
author = {Berry, George and Taylor, Sean J.},
eprint = {1702.06677},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/Discussion quality diffuses in the digital public square.pdf:pdf},
isbn = {9781450349130},
keywords = {comment ranking,online discussions,social,social influence},
title = {{Discussion quality diffuses in the digital public square}},
url = {http://arxiv.org/abs/1702.06677},
year = {2017}
}
@article{Fligner1986,
author = {Fligner, M. and Verducci, J. S},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/00-distance-based-ranking-fligner.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {analysis of covariance,kernel smoothing,partial linear models},
number = {3},
pages = {359--369},
title = {{Distance based ranking models}},
volume = {48},
year = {1986}
}
@inproceedings{Ravi2014,
abstract = {Asking the right question in the right way is an art (and a science). In a community question-answering setting, a good question is not just one that is found to be useful by other people—a question is good if it is also pre-sented clearly and shows prior research. Using a com-munity question-answering site that allows voting over the questions, we show that there is a notion of question quality that goes beyond mere popularity. We present techniques using latent topical models to automatically predict the quality of questions based on their content. Our best system achieves a prediction accuracy of 72{\%}, beating out strong baselines by a significant amount. We also examine the effect of question quality on the dy-namics of user behavior and the longevity of questions.},
author = {Ravi, Sujith and Pang, Bo and Rastogi, V and Kumar, Ravi and Rastagori, VIbhor and Kumar, Ravi},
booktitle = {Eighth International AAAI Conference on Weblogs and Social Media},
file = {:Users/brad/Dropbox/lse-msc-thesis-brad/articles/00-final-nb-articles/question-quality/01-great-question-quality-ravi.pdf:pdf},
isbn = {978-1-57735-657-8},
keywords = {Full Papers},
number = {1},
pages = {426--435},
title = {{Great Question! Question Quality in Community Q{\&}A.}},
year = {2014}
}
@article{Blei2003,
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
file = {:Users/brad/Dropbox/lse-msc/01-dissertation-lit/articles-info-retrieval/Latent Dirichlet Allocation.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
